import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from src.utils import (get_all_datasets, get_dataset, 
                       save_dataset_analysis, get_dataset_analysis, 
                       delete_dataset_analysis, is_analysis_outdated)
from src.models.llms import load_llm
import time

st.set_page_config(page_title="Chi Ti·∫øt B·ªô D·ªØ Li·ªáu", layout="wide")
st.title("üìÇ Chi Ti·∫øt B·ªô D·ªØ Li·ªáu")

# Add custom CSS for better styling
st.markdown("""
<style>
    .data-description-box {
        background: linear-gradient(135deg, #667eea15 0%, #764ba215 100%);
        border: 2px solid #667eea30;
        padding: 1.5rem;
        border-radius: 12px;
        margin: 1rem 0;
    }
    .column-analysis-card {
        color: black;
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #667eea;
        margin: 0.5rem 0;
    }
    .insight-badge {
        background: linear-gradient(135deg, #28a745 0%, #20c997 100%);
        color: white;
        padding: 0.25rem 0.5rem;
        border-radius: 12px;
        font-size: 0.8rem;
        display: inline-block;
        margin: 0.25rem 0;
    }
    .warning-badge {
        background: linear-gradient(135deg, #ffc107 0%, #fd7e14 100%);
        color: white;
        padding: 0.25rem 0.5rem;
        border-radius: 12px;
        font-size: 0.8rem;
        display: inline-block;
        margin: 0.25rem 0;
    }
    .error-badge {
        background: linear-gradient(135deg, #dc3545 0%, #e55353 100%);
        color: white;
        padding: 0.25rem 0.5rem;
        border-radius: 12px;
        font-size: 0.8rem;
        display: inline-block;
        margin: 0.25rem 0;
    }
    .cache-info {
        color: black;
        background: #e3f2fd;
        border: 1px solid #2196f3;
        padding: 0.75rem;
        border-radius: 6px;
        margin: 0.5rem 0;
        font-size: 0.9rem;
    }
</style>
""", unsafe_allow_html=True)

llm = load_llm("gpt-3.5-turbo")

# ---------- Helper functions with enhanced error handling ----------
def safe_read_csv(file_path):
    """Safely read CSV with multiple encoding attempts"""
    for enc in ['utf-8', 'ISO-8859-1', 'utf-16', 'cp1252']:
        try:
            return pd.read_csv(file_path, encoding=enc)
        except UnicodeDecodeError:
            continue
        except Exception as e:
            st.error(f"Error reading file with {enc}: {str(e)}")
            continue
    raise UnicodeDecodeError("utf-8", b"", 0, 1, "Unable to decode file with common encodings.")

def extract_llm_content(response):
    """Tr√≠ch xu·∫•t n·ªôi dung t·ª´ LLM response object"""
    try:
        # N·∫øu response c√≥ thu·ªôc t√≠nh content
        if hasattr(response, 'content'):
            return response.content
        
        # N·∫øu response l√† string
        elif isinstance(response, str):
            return response
        
        # N·∫øu response c√≥ thu·ªôc t√≠nh text
        elif hasattr(response, 'text'):
            return response.text
        
        # N·∫øu response c√≥ thu·ªôc t√≠nh message v√† content
        elif hasattr(response, 'message') and hasattr(response.message, 'content'):
            return response.message.content
        
        # Fallback: convert to string
        else:
            return str(response)
            
    except Exception as e:
        st.warning(f"Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung LLM: {str(e)}")
        return "Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c √Ω nghƒ©a"

def analyze_column(col_name, series):
    """Enhanced column analysis with better error handling"""
    try:
        info = {
            'name': col_name, 
            'dtype': str(series.dtype), 
            'missing_pct': series.isna().mean() * 100, 
            'unique': series.nunique(),
            'total_count': len(series)
        }
        
        if pd.api.types.is_numeric_dtype(series):
            desc = series.describe()
            info.update({
                'min': desc['min'], 
                'max': desc['max'], 
                'mean': desc['mean'],
                'median': series.median(), 
                'std': desc['std'],
                'outliers': ((series < (desc['25%'] - 1.5*(desc['75%'] - desc['25%']))) | 
                           (series > (desc['75%'] + 1.5*(desc['75%'] - desc['25%'])))).sum(),
                'type': 'Numeric',
                'skewness': series.skew(),
                'kurtosis': series.kurtosis()
            })
        elif series.nunique() == 2:
            info['type'] = 'Boolean'
            info['value_counts'] = series.value_counts().to_dict()
        elif info['unique'] == len(series):
            info['type'] = 'ID'
        elif info['unique'] <= 20:
            info['type'] = 'Category'
            info['value_counts'] = series.value_counts().head(10).to_dict()
        else:
            info['type'] = 'Text'
            info['avg_length'] = series.astype(str).str.len().mean()
            info['max_length'] = series.astype(str).str.len().max()
        
        return info
    except Exception as e:
        return {
            'name': col_name,
            'dtype': 'Error',
            'missing_pct': 100,
            'unique': 0,
            'type': 'Error',
            'error': str(e)
        }

def guess_column_semantic_llm(col_name, sample_values=None):
    """Enhanced semantic analysis with sample values"""
    try:
        sample_text = ""
        if sample_values is not None and len(sample_values) > 0:
            # Convert sample values to string and take first few
            sample_str = [str(v) for v in sample_values[:5] if pd.notna(v)]
            if sample_str:
                sample_text = f" Gi√° tr·ªã m·∫´u: {', '.join(sample_str)}"
        
        prompt = f"Lo·∫°i ng·ªØ nghƒ©a c·ªßa c·ªôt '{col_name}'{sample_text} l√† g√¨? Tr·∫£ l·ªùi b·∫±ng 3-5 t·ª´ ti·∫øng Vi·ªát m√¥ t·∫£ √Ω nghƒ©a (v√≠ d·ª•: 'ID kh√°ch h√†ng', 'Ng√†y sinh', 'T√™n s·∫£n ph·∫©m')."
        
        response = llm.invoke(prompt)
        
        # S·ª≠ d·ª•ng h√†m extract_llm_content ƒë·ªÉ l·∫•y n·ªôi dung
        result = extract_llm_content(response)
        return result.strip()
        
    except Exception as e:
        return f"Kh√¥ng x√°c ƒë·ªãnh ({str(e)[:50]}...)"

@st.cache_data(show_spinner=False)
def get_cleaning_suggestions(col_stats, user_description=""):
    """Enhanced cleaning suggestions with user context"""
    try:
        cols_description = "\n".join([
            f"C·ªôt: {col['name']} | Lo·∫°i: {col['dtype']} | Thi·∫øu: {col['missing_pct']:.2f}% | Duy nh·∫•t: {col['unique']}" 
            for col in col_stats if 'error' not in col
        ])
        
        context_text = f"\nM√¥ t·∫£ ng∆∞·ªùi d√πng: {user_description}" if user_description else ""
        
        prompt = f"""
D·ª±a tr√™n t√≥m t·∫Øt sau v·ªÅ c√°c c·ªôt trong b·ªô d·ªØ li·ªáu:
{cols_description}{context_text}

H√£y ƒë·ªÅ xu·∫•t k·∫ø ho·∫°ch l√†m s·∫°ch v·ªõi c√°c quy t·∫Øc sau:
- Ch·ªâ x√≥a c√°c c·ªôt n·∫øu t·ª∑ l·ªá thi·∫øu > 70% ho·∫∑c to√†n b·ªô l√† ID kh√¥ng c·∫ßn thi·∫øt.
- ƒê·ªëi v·ªõi c√°c c·ªôt c√≥ gi√° tr·ªã thi·∫øu ‚â§ 70%:
    - N·∫øu l√† s·ªë: ƒëi·ªÅn b·∫±ng trung v·ªã ho·∫∑c trung b√¨nh.
    - N·∫øu l√† ph√¢n lo·∫°i: ƒëi·ªÅn b·∫±ng mode ho·∫∑c 'Unknown'.
- Ch·ªâ lo·∫°i b·ªè ngo·∫°i l·ªá t·ª´ c√°c c·ªôt s·ªë c√≥ outliers > 5% t·ªïng d·ªØ li·ªáu.
- Chu·∫©n h√≥a c√°c c·ªôt s·ªë ch·ªâ khi c·∫ßn thi·∫øt cho ph√¢n t√≠ch.
- ∆Øu ti√™n gi·ªØ nguy√™n d·ªØ li·ªáu n·∫øu c√≥ th·ªÉ.
- ƒê·ªÅ xu·∫•t chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu n·∫øu ph√π h·ª£p.

Tr·∫£ v·ªÅ k·∫ø ho·∫°ch d∆∞·ªõi d·∫°ng danh s√°ch c√≥ c·∫•u tr√∫c r√µ r√†ng v·ªõi l√Ω do.
"""
        response = llm.invoke(prompt)
        return extract_llm_content(response)
    except Exception as e:
        return f"L·ªói t·∫°o ƒë·ªÅ xu·∫•t l√†m s·∫°ch: {str(e)}"

@st.cache_data(show_spinner=False)
def refine_cleaning_strategy(user_input, _base_plan):
    """Refine cleaning strategy based on user input"""
    try:
        base_plan_text = extract_llm_content(_base_plan)
        
        prompt = f"""
K·∫ø ho·∫°ch l√†m s·∫°ch hi·ªán t·∫°i:
{base_plan_text}

Ng∆∞·ªùi d√πng mu·ªën ƒëi·ªÅu ch·ªânh: {user_input}

C·∫≠p nh·∫≠t k·∫ø ho·∫°ch l√†m s·∫°ch ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng. Gi·ªØ nguy√™n c√°c ph·∫ßn t·ªët v√† ch·ªâ thay ƒë·ªïi theo y√™u c·∫ßu.
"""
        response = llm.invoke(prompt)
        return extract_llm_content(response)
    except Exception as e:
        return f"L·ªói c·∫≠p nh·∫≠t k·∫ø ho·∫°ch: {str(e)}"

@st.cache_data(show_spinner=False)
def generate_cleaning_code_from_plan(_plan):
    """Enhanced code generation with better error handling"""
    try:
        plan_text = extract_llm_content(_plan)
        
        prompt = f"""
            T·∫°o m√£ Python an to√†n ƒë·ªÉ l√†m s·∫°ch d·ªØ li·ªáu:
            
            ```python
            import pandas as pd
            import numpy as np
            
            print("üîß B·∫Øt ƒë·∫ßu l√†m s·∫°ch d·ªØ li·ªáu...")
            
            # X·ª≠ l√Ω missing values an to√†n
            for col in df.columns:
                missing_count = df[col].isnull().sum()
                if missing_count > 0:
                    try:
                        if pd.api.types.is_numeric_dtype(df[col]):
                            df[col] = df[col].fillna(df[col].median())
                            print(f"ƒêi·ªÅn missing cho c·ªôt s·ªë: {{col}}")
                        else:
                            mode_val = df[col].mode()
                            if len(mode_val) > 0:
                                df[col] = df[col].fillna(mode_val[0])
                            else:
                                df[col] = df[col].fillna('Unknown')
                            print(f"ƒêi·ªÅn missing cho c·ªôt text: {{col}}")
                    except Exception as e:
                        print(f"B·ªè qua c·ªôt {{col}}: {{e}}")
            
            print("‚úÖ L√†m s·∫°ch ho√†n th√†nh!")

            K·∫ø ho·∫°ch l√†m s·∫°ch g·ªëc:
            {plan_text}
            
            CH·ªà tr·∫£ v·ªÅ Python code.
        """
        
        response = llm.invoke(prompt)
        return extract_llm_content(response)
    except Exception as e:
        return f"# L·ªói t·∫°o m√£: {str(e)}\nprint('Kh√¥ng th·ªÉ t·∫°o m√£ l√†m s·∫°ch')"

def safe_execute_cleaning_code(code: str, df: pd.DataFrame):
    """Safely execute cleaning code with better error handling"""
    try:
        # Create safe execution environment
        safe_globals = {
            'df': df.copy(),  # Work on copy to avoid modifying original
            'pd': pd, 
            'np': np,
            'print': st.write  # Redirect print to streamlit
        }
        
        # Add safety functions
        safe_globals['fix_numeric_strings'] = fix_numeric_strings
        
        # Execute the code
        exec(code, safe_globals)
        
        cleaned_df = safe_globals['df']
        
        return cleaned_df, True
        
    except Exception as e:
        # Return original dataframe if cleaning fails
        return df, False

def generate_insight(info):
    """Generate insights for column analysis"""
    try:
        if 'error' in info:
            return f"‚ùå L·ªói ph√¢n t√≠ch: {info['error']}"
        
        if info['type'] == 'ID':
            return "üîπ ƒê√¢y l√† c·ªôt ƒë·ªãnh danh duy nh·∫•t."
        
        if info['missing_pct'] > 50:
            return f"‚ö†Ô∏è {info['missing_pct']:.1f}% gi√° tr·ªã thi·∫øu - c·∫ßn xem x√©t lo·∫°i b·ªè."
        elif info['missing_pct'] > 10:
            return f"‚ö†Ô∏è {info['missing_pct']:.1f}% gi√° tr·ªã thi·∫øu - c·∫ßn ƒëi·ªÅn b·ªï sung."
        
        if info['type'] == 'Numeric':
            if 'std' in info and info['std'] < 1e-3:
                return "‚ö†Ô∏è ƒê·ªô bi·∫øn thi√™n r·∫•t th·∫•p - c√≥ th·ªÉ l√† h·∫±ng s·ªë."
            if 'outliers' in info and info['outliers'] > 0:
                outlier_pct = (info['outliers'] / info['total_count']) * 100
                if outlier_pct > 5:
                    return f"‚ö†Ô∏è {info['outliers']} ngo·∫°i l·ªá ({outlier_pct:.1f}%) - c·∫ßn ki·ªÉm tra."
        
        if info['unique'] < 5 and info['type'] == 'Category':
            return "‚ÑπÔ∏è Ph√¢n lo·∫°i v·ªõi √≠t gi√° tr·ªã - ph√π h·ª£p cho grouping."
        
        if info['type'] == 'Text' and 'avg_length' in info:
            if info['avg_length'] > 100:
                return f"üìù VƒÉn b·∫£n d√†i (TB: {info['avg_length']:.0f} k√Ω t·ª±) - c√≥ th·ªÉ c·∫ßn x·ª≠ l√Ω NLP."
        
        return "‚úÖ Kh√¥ng ph√°t hi·ªán v·∫•n ƒë·ªÅ l·ªõn."
    except Exception as e:
        return f"‚ùå L·ªói t·∫°o insight: {str(e)}"

def plot_distribution(col_name, series):
    """Enhanced distribution plotting with error handling"""
    try:
        fig, ax = plt.subplots(figsize=(8, 5))
        
        if pd.api.types.is_numeric_dtype(series):
            # Numeric distribution
            clean_series = series.dropna()
            if len(clean_series) > 0:
                ax.hist(clean_series, bins=min(30, len(clean_series.unique())), 
                       color='#69b3a2', alpha=0.7, edgecolor='black')
                ax.axvline(clean_series.mean(), color='red', linestyle='--', 
                          label=f'Trung b√¨nh: {clean_series.mean():.2f}')
                ax.axvline(clean_series.median(), color='orange', linestyle='--', 
                          label=f'Trung v·ªã: {clean_series.median():.2f}')
                ax.legend()
                ax.set_xlabel(col_name)
                ax.set_ylabel('T·∫ßn su·∫•t')
        else:
            # Categorical distribution
            vc = series.fillna("NaN").value_counts().head(15)  # Show top 15
            if len(vc) > 0:
                bars = ax.bar(range(len(vc)), vc.values, color='#8c54ff', alpha=0.7)
                ax.set_xticks(range(len(vc)))
                ax.set_xticklabels([str(x) for x in vc.index], rotation=45, ha='right')
                ax.set_ylabel('S·ªë l∆∞·ª£ng')
                
                # Add value labels on bars
                for bar, value in zip(bars, vc.values):
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(vc.values)*0.01,
                           str(value), ha='center', va='bottom', fontsize=9)
        
        ax.set_title(f"Ph√¢n ph·ªëi: {col_name}", fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        st.pyplot(fig)
        plt.close()
        
    except Exception as e:
        st.error(f"L·ªói v·∫Ω bi·ªÉu ƒë·ªì cho {col_name}: {str(e)}")

def perform_column_analysis(df, dataset_id, progress_container):
    """Th·ª±c hi·ªán ph√¢n t√≠ch c·ªôt v·ªõi progress bar chuy√™n nghi·ªáp"""
    col_analyses = []
    total_cols = len(df.columns)
    
    try:
        # Create progress elements within the container
        with progress_container:
            progress_header = st.empty()
            progress_bar = st.progress(0)
            status_text = st.empty()
            detail_text = st.empty()
        
        # Header with animation
        progress_header.markdown("""
        <div style="text-align: center; padding: 1rem; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 10px; color: white; margin-bottom: 1rem;">
            <h4 style="margin: 0;">üî¨ ƒêang Ph√¢n T√≠ch D·ªØ Li·ªáu</h4>
            <p style="margin: 0.5rem 0 0 0; opacity: 0.9;">S·ª≠ d·ª•ng AI ƒë·ªÉ ph√¢n t√≠ch t·ª´ng c·ªôt...</p>
        </div>
        """, unsafe_allow_html=True)
        
        for i, col in enumerate(df.columns):
            # Update progress
            progress_percent = (i + 1) / total_cols
            progress_bar.progress(progress_percent)
            
            # Update status with professional styling
            status_text.markdown(f"""
            <div style="background: #f8f9fa; padding: 0.8rem; border-radius: 8px; border-left: 4px solid #667eea;">
                <strong>üîç ƒêang ph√¢n t√≠ch:</strong> <code>{col}</code><br>
                <small>B∆∞·ªõc {i + 1}/{total_cols} - {progress_percent:.1%} ho√†n th√†nh</small>
            </div>
            """, unsafe_allow_html=True)
            
            # Show current analysis details
            detail_text.info(f"üß† AI ƒëang ph√¢n t√≠ch ng·ªØ nghƒ©a v√† th·ªëng k√™ cho c·ªôt '{col}'...")
            
            # Column analysis
            stats = analyze_column(col, df[col])
            # Get sample values for semantic analysis
            sample_vals = df[col].dropna().head(5).tolist()
            try:
                semantic = guess_column_semantic_llm(col, sample_vals)
            except Exception as semantic_error:
                semantic = "Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c"
            
            stats['semantic'] = semantic
            col_analyses.append(stats)
            
            # Brief pause for smooth animation
            time.sleep(0.1)  # Reduced from 0.2 for faster testing
        
        
        # Complete progress
        progress_bar.progress(1.0)
        status_text.success("‚úÖ Ph√¢n t√≠ch ho√†n th√†nh th√†nh c√¥ng!")
        detail_text.info("üíæ ƒêang l∆∞u k·∫øt qu·∫£ v√†o c∆° s·ªü d·ªØ li·ªáu...")
        
        # Save results to database v·ªõi debug
        save_success = save_dataset_analysis(dataset_id, col_analyses)
        
        if save_success:
            detail_text.success("üéâ ƒê√£ l∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch!")
        else:
            detail_text.error("‚ùå L·ªói khi l∆∞u v√†o database!")
        
        # Show completion with animation
        time.sleep(0.8)
        time.sleep(0.5)
        
        # Clear all progress elements completely
        progress_header.empty()
        progress_bar.empty() 
        status_text.empty()
        detail_text.empty()
        
        # Show final success message briefly then clear it
        success_msg = st.empty()
        with progress_container:
            if save_success:
                success_msg.success("‚úÖ Ph√¢n t√≠ch d·ªØ li·ªáu ho√†n t·∫•t! K·∫øt qu·∫£ hi·ªÉn th·ªã b√™n d∆∞·ªõi.")
            else:
                success_msg.warning("‚ö†Ô∏è Ph√¢n t√≠ch ho√†n t·∫•t nh∆∞ng c√≥ l·ªói khi l∆∞u cache!")
            time.sleep(1.2)
            success_msg.empty()
        
        return col_analyses
        
    except Exception as e:
        # Clear progress on error
        try:
            progress_header.empty()
            progress_bar.empty()
            status_text.empty() 
            detail_text.empty()
        except:
            pass
        
        # Show error in container
        with progress_container:
            st.error(f"‚ùå L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch: {str(e)}")
        
        return []

def extract_valid_code(llm_response):
    """Extract valid Python code from LLM response"""
    try:
        # Try to extract code between ```python and ```
        match = re.search(r"```(?:python)?\n(.*?)```", llm_response.strip(), re.DOTALL)
        if match:
            return match.group(1)
        
        # If no code blocks, try to extract lines that look like Python code
        lines = llm_response.splitlines()
        code_lines = []
        for line in lines:
            stripped = line.strip()
            if stripped and not stripped.startswith("#") and not stripped.startswith("K·∫ø ho·∫°ch"):
                # Basic check if it looks like Python code
                if any(keyword in stripped for keyword in ['df[', 'df.', 'pd.', 'np.', '=', 'print(', 'try:', 'except:']):
                    code_lines.append(line)
        
        return "\n".join(code_lines) if code_lines else llm_response
    except Exception as e:
        return f"# Error extracting code: {str(e)}\n{llm_response}"

def fix_numeric_strings(df):
    """Enhanced numeric string fixing"""
    fixed_cols = []
    for col in df.select_dtypes(include='object').columns:
        try:
            if df[col].dropna().apply(lambda x: isinstance(x, str)).all():
                # Try to convert numeric strings
                original_type = df[col].dtype
                test_series = df[col].str.replace(',', '', regex=False)
                test_series = pd.to_numeric(test_series, errors='coerce')
                
                # If more than 80% can be converted, do the conversion
                valid_ratio = test_series.notna().sum() / len(test_series)
                if valid_ratio > 0.8:
                    df[col] = test_series
                    fixed_cols.append(col)
        except Exception as e:
            continue
    
    if fixed_cols:
        st.info(f"‚úÖ ƒê√£ chuy·ªÉn ƒë·ªïi c√°c c·ªôt s·ªë: {', '.join(fixed_cols)}")
    
    return df

def show_skew_kurtosis(df, cleaned_df):
    """Enhanced skewness and kurtosis analysis"""
    try:
        raw_cols = df.select_dtypes(include='number').columns
        clean_cols = cleaned_df.select_dtypes(include='number').columns
        numeric_cols = list(set(raw_cols).intersection(set(clean_cols)))

        if not numeric_cols:
            st.info("Kh√¥ng c√≥ c·ªôt s·ªë chung n√†o kh·∫£ d·ª•ng cho b√°o c√°o ƒë·ªô l·ªách/ƒë·ªô nh·ªçn.")
            return

        # Create comprehensive report
        report = pd.DataFrame(index=numeric_cols)
        report['ƒê·ªô l·ªách (Tr∆∞·ªõc)'] = df[numeric_cols].skew()
        report['ƒê·ªô nh·ªçn (Tr∆∞·ªõc)'] = df[numeric_cols].kurtosis()
        report['ƒê·ªô l·ªách (Sau)'] = cleaned_df[numeric_cols].skew()
        report['ƒê·ªô nh·ªçn (Sau)'] = cleaned_df[numeric_cols].kurtosis()
        
        # Calculate improvements
        report['C·∫£i thi·ªán ƒê·ªô l·ªách'] = abs(report['ƒê·ªô l·ªách (Tr∆∞·ªõc)']) - abs(report['ƒê·ªô l·ªách (Sau)'])
        report['C·∫£i thi·ªán ƒê·ªô nh·ªçn'] = abs(report['ƒê·ªô nh·ªçn (Tr∆∞·ªõc)']) - abs(report['ƒê·ªô nh·ªçn (Sau)'])
        
        st.dataframe(report.round(3), use_container_width=True)

        # Visualization with subplots
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Skewness comparison
        x_pos = np.arange(len(numeric_cols))
        width = 0.35
        
        ax1.bar(x_pos - width/2, report['ƒê·ªô l·ªách (Tr∆∞·ªõc)'], width, 
               label='Tr∆∞·ªõc', alpha=0.8, color='#ff7f7f')
        ax1.bar(x_pos + width/2, report['ƒê·ªô l·ªách (Sau)'], width,
               label='Sau', alpha=0.8, color='#7fbf7f')
        
        ax1.set_xlabel('ƒê·∫∑c tr∆∞ng')
        ax1.set_ylabel('ƒê·ªô l·ªách')
        ax1.set_title('So s√°nh ƒê·ªô l·ªách Tr∆∞·ªõc vs Sau L√†m s·∫°ch')
        ax1.set_xticks(x_pos)
        ax1.set_xticklabels(numeric_cols, rotation=45, ha='right')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        
        # Kurtosis comparison
        ax2.bar(x_pos - width/2, report['ƒê·ªô nh·ªçn (Tr∆∞·ªõc)'], width,
               label='Tr∆∞·ªõc', alpha=0.8, color='#ff7f7f')
        ax2.bar(x_pos + width/2, report['ƒê·ªô nh·ªçn (Sau)'], width,
               label='Sau', alpha=0.8, color='#7fbf7f')
        
        ax2.set_xlabel('ƒê·∫∑c tr∆∞ng')
        ax2.set_ylabel('ƒê·ªô nh·ªçn')
        ax2.set_title('So s√°nh ƒê·ªô nh·ªçn Tr∆∞·ªõc vs Sau L√†m s·∫°ch')
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels(numeric_cols, rotation=45, ha='right')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)
        plt.close()

        # Generate AI insights
        try:
            interpretation_prompt = f"""
Ph√¢n t√≠ch b√°o c√°o ƒë·ªô l·ªách v√† ƒë·ªô nh·ªçn sau:

{report.to_string()}

H√£y ƒë∆∞a ra nh·∫≠n x√©t v·ªÅ:
1. Nh·ªØng c·∫£i thi·ªán ƒë√°ng k·ªÉ trong ph√¢n ph·ªëi d·ªØ li·ªáu
2. C√°c c·ªôt c√≤n c·∫ßn x·ª≠ l√Ω th√™m
3. T√°c ƒë·ªông ƒë·∫øn ch·∫•t l∆∞·ª£ng ph√¢n t√≠ch
4. ƒê·ªÅ xu·∫•t b∆∞·ªõc ti·∫øp theo

Tr·∫£ l·ªùi b·∫±ng markdown v·ªõi format ƒë·∫πp v√† d·ªÖ hi·ªÉu.
"""
            
            response = llm.invoke(interpretation_prompt)
            interpretation = extract_llm_content(response)
            st.markdown("### ü§ñ Ph√¢n t√≠ch AI")
            st.markdown(interpretation)
            
        except Exception as e:
            st.warning(f"Kh√¥ng th·ªÉ t·∫°o ph√¢n t√≠ch AI: {str(e)}")

    except Exception as e:
        st.error(f"L·ªói trong ph√¢n t√≠ch ƒë·ªô l·ªách/ƒë·ªô nh·ªçn: {str(e)}")

# Main application
def main():
    # Load datasets
    datasets = get_all_datasets()
    if not datasets:
        st.warning("Kh√¥ng t√¨m th·∫•y b·ªô d·ªØ li·ªáu n√†o. Vui l√≤ng t·∫£i l√™n m·ªôt b·ªô d·ªØ li·ªáu trong B·∫£ng ƒëi·ªÅu khi·ªÉn.")
        st.stop()

    # Dataset selection
    selected = st.selectbox("Ch·ªçn b·ªô d·ªØ li·ªáu:", [f"{d[0]} - {d[1]}" for d in datasets])
    dataset_id = int(selected.split(" - ")[0])
    dataset = get_dataset(dataset_id)
    
    try:
        df = safe_read_csv(dataset[2])
    except Exception as e:
        st.error(f"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc file: {str(e)}")
        st.stop()

    st.markdown(f"### B·ªô d·ªØ li·ªáu: `{dataset[1]}` ‚Äî {df.shape[0]:,} h√†ng √ó {df.shape[1]} c·ªôt")

    # Add data description section
    st.markdown("### üìù M√¥ t·∫£ D·ªØ li·ªáu")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # User data description input
        user_description = st.text_area(
            "‚úçÔ∏è M√¥ t·∫£ b·ªô d·ªØ li·ªáu c·ªßa b·∫°n:",
            height=100,
            placeholder="V√≠ d·ª•: D·ªØ li·ªáu b√°n h√†ng t·ª´ 2023-2024, bao g·ªìm th√¥ng tin kh√°ch h√†ng, s·∫£n ph·∫©m v√† doanh thu. ƒê∆∞·ª£c thu th·∫≠p t·ª´ h·ªá th·ªëng POS...",
            help="M√¥ t·∫£ chi ti·∫øt gi√∫p AI hi·ªÉu r√µ h∆°n v·ªÅ ng·ªØ c·∫£nh v√† ƒë∆∞a ra g·ª£i √Ω ch√≠nh x√°c h∆°n"
        )
        
        if user_description:
            st.session_state.user_data_description = user_description
    
    with col2:
        st.markdown("**üí° M·∫πo vi·∫øt m√¥ t·∫£ t·ªët:**")
        st.markdown("""
        - Ngu·ªìn g·ªëc d·ªØ li·ªáu
        - M·ª•c ƒë√≠ch thu th·∫≠p  
        - Kho·∫£ng th·ªùi gian
        - √ù nghƒ©a c√°c c·ªôt ch√≠nh
        - ƒê∆°n v·ªã ƒëo l∆∞·ªùng
        - L∆∞u √Ω ƒë·∫∑c bi·ªát
        """)

    # Create tabs for different analysis sections
    tab1, tab2, tab3 = st.tabs(["üìä T·ªïng quan Chi ti·∫øt", "üßº L√†m s·∫°ch Th√¥ng minh", "üìà Ph√¢n t√≠ch Ph√¢n ph·ªëi"])

    with tab1:
        st.markdown("### üîç Ph√¢n t√≠ch Chi ti·∫øt t·ª´ng C·ªôt")
        
        # Ki·ªÉm tra xem ƒë√£ c√≥ ph√¢n t√≠ch cached kh√¥ng
        cached_analysis = get_dataset_analysis(dataset_id)
        
        # AUTO-RESTORE with force option
        auto_loaded = False
        force_load_cache = st.checkbox("üîß Force Load Cache (ignore outdated)", value=False, help="Load cache even if considered outdated")
        
        if cached_analysis and not hasattr(st.session_state, 'col_analyses'):
            is_outdated = is_analysis_outdated(cached_analysis, dataset[4])
            
            # Load cache if not outdated OR if force load is enabled
            if not is_outdated or force_load_cache:
                try:
                    # T·ª± ƒë·ªông restore cache v√†o session state
                    st.session_state.col_analyses = cached_analysis['analysis']
                    st.session_state.analysis_auto_loaded = True
                    auto_loaded = True
                    
                    if force_load_cache:
                        st.success("üîß Force-loaded cache into session state (ignoring outdated status)!")
                    else:
                        st.success("üîÑ Auto-loaded cache into session state!")
                        
                except Exception as e:
                    st.error(f"Failed to auto-load cache: {e}")
        
        # Check session state
        st.write("**üìä Session State Check:**")
        if hasattr(st.session_state, 'col_analyses'):
            st.success(f"‚úÖ Session state has col_analyses with {len(st.session_state.col_analyses)} items")
            st.write(f"Auto-loaded flag: {st.session_state.get('analysis_auto_loaded', False)}")
        else:
            st.warning("‚ùå No col_analyses in session state")
        
        # Hi·ªÉn th·ªã th√¥ng tin cache
        if cached_analysis:
            is_outdated = is_analysis_outdated(cached_analysis, dataset[4])
            
            if is_outdated and not force_load_cache:
                st.markdown("""
                <div class="cache-info">
                    ‚ö†Ô∏è <strong>Ph√¢n t√≠ch c≈© ƒë∆∞·ª£c t√¨m th·∫•y</strong> - Dataset ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t sau l·∫ßn ph√¢n t√≠ch cu·ªëi. 
                    N√™n ch·∫°y ph√¢n t√≠ch l·∫°i ƒë·ªÉ c√≥ k·∫øt qu·∫£ ch√≠nh x√°c nh·∫•t.
                </div>
                """, unsafe_allow_html=True)
            else:
                # Hi·ªÉn th·ªã th√¥ng tin cache v·ªõi status
                cache_status = "ƒë√£ ƒë∆∞·ª£c t·ª± ƒë·ªông t·∫£i" if auto_loaded or st.session_state.get('analysis_auto_loaded', False) else "c√≥ s·∫µn"
                if force_load_cache:
                    cache_status += " (force loaded)"
                    
                st.markdown(f"""
                <div class="cache-info">
                    ‚úÖ <strong>Ph√¢n t√≠ch c√≥ s·∫µn</strong> - ƒê√£ ph√¢n t√≠ch l√∫c {cached_analysis['updated_at']} v√† {cache_status}. 
                    B·∫°n c√≥ th·ªÉ ch·∫°y ph√¢n t√≠ch l·∫°i n·∫øu c·∫ßn.
                </div>
                """, unsafe_allow_html=True)
        else:
            st.info("‚ÑπÔ∏è Ch∆∞a c√≥ ph√¢n t√≠ch n√†o ƒë∆∞·ª£c l∆∞u cho dataset n√†y.")
        
        # Create buttons with better layout
        st.markdown("#### ‚ö° T√πy ch·ªçn Ph√¢n t√≠ch")
        col_btn1, col_btn2, col_btn3 = st.columns(3)
        
        with col_btn1:
            use_cached = False
            if cached_analysis:
                use_cached = st.button("üîÑ T·∫£i l·∫°i t·ª´ Cache", type="secondary", use_container_width=True)
        
        with col_btn2:
            run_analysis = st.button("üöÄ Ch·∫°y Ph√¢n t√≠ch M·ªõi", type="primary", use_container_width=True)
        
        with col_btn3:
            delete_cache = False
            if cached_analysis:
                delete_cache = st.button("üóëÔ∏è X√≥a Cache", type="secondary", use_container_width=True)
        
        # Handle button actions
        if use_cached:
            try:
                st.session_state.col_analyses = cached_analysis['analysis']
                st.session_state.analysis_auto_loaded = False  # Reset auto-load flag
                st.success("üîÑ ƒê√£ t·∫£i l·∫°i ph√¢n t√≠ch t·ª´ cache!")
                st.rerun()
            except Exception as e:
                st.error(f"L·ªói khi t·∫£i cache: {e}")
        
        if delete_cache:
            try:
                delete_dataset_analysis(dataset_id)
                # Clear session state as well
                if 'col_analyses' in st.session_state:
                    del st.session_state['col_analyses']
                if 'analysis_auto_loaded' in st.session_state:
                    del st.session_state['analysis_auto_loaded']
                st.success("üóëÔ∏è ƒê√£ x√≥a cache ph√¢n t√≠ch!")
                st.rerun()
            except Exception as e:
                st.error(f"L·ªói khi x√≥a cache: {e}")
        
        # Create dedicated container for progress (will be managed by analysis function)
        progress_container = st.empty()
        
        # Handle analysis execution
        if run_analysis:
            # Clear any existing analysis first
            if 'col_analyses' in st.session_state:
                del st.session_state['col_analyses']
            if 'analysis_auto_loaded' in st.session_state:
                del st.session_state['analysis_auto_loaded']
            
            # Run the analysis with progress tracking
            col_analyses = perform_column_analysis(df, dataset_id, progress_container)
            
            if col_analyses:
                st.session_state.col_analyses = col_analyses
                st.session_state.analysis_auto_loaded = False
                st.rerun()
            else:
                st.error("‚ùå Ph√¢n t√≠ch th·∫•t b·∫°i. Vui l√≤ng th·ª≠ l·∫°i.")
        
        # Display analysis results - IMPROVED LOGIC
        current_analyses = None
        
        # Try to get analyses from session state first
        if hasattr(st.session_state, 'col_analyses'):
            current_analyses = st.session_state.col_analyses
            st.write("üéØ **Using data from session state**")
        # Fallback: try to get from cache if session state is empty
        elif cached_analysis and (not is_analysis_outdated(cached_analysis, dataset[4]) or force_load_cache):
            try:
                current_analyses = cached_analysis['analysis']
                # Auto-load into session state for next time
                st.session_state.col_analyses = current_analyses
                st.session_state.analysis_auto_loaded = True
                st.write("üéØ **Using data from cache (fallback)**")
            except Exception as e:
                st.error(f"L·ªói khi ƒë·ªçc t·ª´ cache: {e}")
        
        if current_analyses:
            st.markdown("---")
            
            # Show data source indicator
            if st.session_state.get('analysis_auto_loaded', False):
                st.info("üìä **Hi·ªÉn th·ªã k·∫øt qu·∫£ t·ª´ cache** - D·ªØ li·ªáu ƒë∆∞·ª£c t·ª± ƒë·ªông kh√¥i ph·ª•c t·ª´ ph√¢n t√≠ch tr∆∞·ªõc ƒë√≥.")
            
            st.markdown("### üìä K·∫øt qu·∫£ Ph√¢n t√≠ch")
            
            # Validate analysis data
            if not isinstance(current_analyses, list):
                st.error(f"‚ùå D·ªØ li·ªáu ph√¢n t√≠ch kh√¥ng h·ª£p l·ªá. Ki·ªÉu: {type(current_analyses)}")
                st.write("Raw data:", current_analyses)
            elif len(current_analyses) == 0:
                st.warning("‚ö†Ô∏è D·ªØ li·ªáu ph√¢n t√≠ch tr·ªëng")
            else:
                # Add summary statistics at the top
                analyses = current_analyses
                total_cols = len(analyses)
                numeric_cols = len([a for a in analyses if a.get('type') == 'Numeric'])
                categorical_cols = len([a for a in analyses if a.get('type') == 'Category']) 
                missing_issues = len([a for a in analyses if a.get('missing_pct', 0) > 10])
                
                # Summary metrics with professional styling
                st.markdown("#### üìà T·ªïng quan K·∫øt qu·∫£")
                summary_col1, summary_col2, summary_col3, summary_col4 = st.columns(4)
                
                with summary_col1:
                    st.metric("T·ªïng C·ªôt", total_cols)
                
                with summary_col2:
                    st.metric("C·ªôt S·ªë", numeric_cols)
                
                with summary_col3:
                    st.metric("C·ªôt Ph√¢n lo·∫°i", categorical_cols)
                
                with summary_col4:
                    delta_text = "C·∫ßn ch√∫ √Ω" if missing_issues > 0 else "T·ªët"
                    st.metric("V·∫•n ƒë·ªÅ Thi·∫øu d·ªØ li·ªáu", missing_issues, delta=delta_text)
                
                st.markdown("---")
                st.markdown("#### üîç Chi ti·∫øt t·ª´ng C·ªôt")
                
                # Display each column analysis
                for analysis in analyses:
                    col_name = analysis['name']
                    
                    with st.container():
                        st.markdown(f"""
                        <div class="column-analysis-card">
                            <h4>üìå {col_name}</h4>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        col_left, col_right = st.columns([2, 3])
                        
                        with col_left:
                            # Basic statistics
                            st.markdown(f"**üè∑Ô∏è Lo·∫°i:** `{analysis['type']}`")
                            st.markdown(f"**üìä Ki·ªÉu d·ªØ li·ªáu:** `{analysis['dtype']}`")
                            st.markdown(f"**üß© √ù nghƒ©a:** {analysis['semantic']}")
                            st.markdown(f"**üî¢ Duy nh·∫•t:** `{analysis['unique']:,}`")
                            st.markdown(f"**‚ùå Thi·∫øu:** `{analysis['missing_pct']:.2f}%`")
                            
                            # Type-specific information
                            if analysis['type'] == 'Numeric' and 'mean' in analysis:
                                st.markdown(f"**üìà Trung b√¨nh:** `{analysis['mean']:.2f}`")
                                st.markdown(f"**üìä Trung v·ªã:** `{analysis['median']:.2f}`")
                                st.markdown(f"**üìè ƒê·ªô l·ªách chu·∫©n:** `{analysis['std']:.2f}`")
                                st.markdown(f"**‚ö†Ô∏è Ngo·∫°i l·ªá:** `{analysis['outliers']}`")
                                if 'skewness' in analysis:
                                    st.markdown(f"**‚ÜóÔ∏è ƒê·ªô l·ªách:** `{analysis['skewness']:.2f}`")
                            
                            elif analysis['type'] == 'Category' and 'value_counts' in analysis:
                                st.markdown("**üèÜ Top gi√° tr·ªã:**")
                                for val, count in list(analysis['value_counts'].items())[:3]:
                                    st.markdown(f"  - `{val}`: {count}")
                            
                            elif analysis['type'] == 'Text' and 'avg_length' in analysis:
                                st.markdown(f"**üìù ƒê·ªô d√†i TB:** `{analysis['avg_length']:.1f}`")
                                st.markdown(f"**üìè ƒê·ªô d√†i t·ªëi ƒëa:** `{analysis['max_length']}`")
                            
                            # Generate and display insight
                            insight = generate_insight(analysis)
                            if "‚úÖ" in insight:
                                badge_class = "insight-badge"
                            elif "‚ö†Ô∏è" in insight:
                                badge_class = "warning-badge"
                            else:
                                badge_class = "error-badge"
                            
                            st.markdown(f'<span class="{badge_class}">{insight}</span>', 
                                      unsafe_allow_html=True)
                        
                        with col_right:
                            # Distribution plot
                            if analysis['type'] != 'Error':
                                try:
                                    plot_distribution(col_name, df[col_name])
                                except Exception as e:
                                    st.error(f"L·ªói v·∫Ω bi·ªÉu ƒë·ªì cho {col_name}: {e}")
                            else:
                                st.error(f"L·ªói ph√¢n t√≠ch c·ªôt: {analysis.get('error', 'Unknown error')}")
                        
                        st.markdown("---")
        
        else:
            # No analysis available - show getting started message
            st.markdown("---")
            st.markdown("### üöÄ B·∫Øt ƒë·∫ßu Ph√¢n t√≠ch")
            st.info("""
            üëÜ **Ch∆∞a c√≥ d·ªØ li·ªáu ph√¢n t√≠ch n√†o.** 
            
            Nh·∫•n n√∫t **"üöÄ Ch·∫°y Ph√¢n t√≠ch M·ªõi"** ƒë·ªÉ b·∫Øt ƒë·∫ßu ph√¢n t√≠ch chi ti·∫øt t·ª´ng c·ªôt v·ªõi AI.
            
            Qu√° tr√¨nh n√†y s·∫Ω:
            - üîç Ph√¢n t√≠ch th·ªëng k√™ t·ª´ng c·ªôt
            - üß† S·ª≠ d·ª•ng AI ƒë·ªÉ hi·ªÉu √Ω nghƒ©a d·ªØ li·ªáu  
            - üíæ L∆∞u k·∫øt qu·∫£ ƒë·ªÉ s·ª≠ d·ª•ng l·∫°i
            - üìä T·∫°o bi·ªÉu ƒë·ªì ph√¢n ph·ªëi
            """)
            
            # Show quick preview of what will be analyzed
            st.markdown("#### üìã Xem tr∆∞·ªõc C·ªôt s·∫Ω ƒë∆∞·ª£c Ph√¢n t√≠ch")
            preview_data = []
            for col in df.columns[:10]:  # Show first 10 columns
                col_type = "S·ªë" if pd.api.types.is_numeric_dtype(df[col]) else "VƒÉn b·∫£n"
                missing_pct = df[col].isnull().mean() * 100
                unique_count = df[col].nunique()
                
                preview_data.append({
                    "C·ªôt": col,
                    "Lo·∫°i": col_type,
                    "Duy nh·∫•t": unique_count,
                    "Thi·∫øu (%)": f"{missing_pct:.1f}%"
                })
            
            preview_df = pd.DataFrame(preview_data)
            st.dataframe(preview_df, use_container_width=True)
            
            if len(df.columns) > 10:
                st.caption(f"... v√† {len(df.columns) - 10} c·ªôt kh√°c n·ªØa")

    with tab2:
        st.markdown("### üßº L√†m s·∫°ch D·ªØ li·ªáu Th√¥ng minh")
        
        # Get column statistics
        if not hasattr(st.session_state, 'col_analyses'):
            st.info("üîÑ Vui l√≤ng ch·∫°y ph√¢n t√≠ch trong tab 'T·ªïng quan Chi ti·∫øt' tr∆∞·ªõc.")
        else:
            col_stats = st.session_state.col_analyses
            
            # Display summary table
            summary_data = []
            for stat in col_stats:
                if 'error' not in stat:
                    summary_data.append({
                        'C·ªôt': stat['name'],
                        'Lo·∫°i': stat['type'],
                        'Ki·ªÉu d·ªØ li·ªáu': stat['dtype'],
                        '√ù nghƒ©a': stat['semantic'][:30] + "..." if len(stat['semantic']) > 30 else stat['semantic'],
                        'Duy nh·∫•t': stat['unique'],
                        'Thi·∫øu %': f"{stat['missing_pct']:.1f}%"
                    })
            
            summary_df = pd.DataFrame(summary_data)
            st.dataframe(summary_df, use_container_width=True)
            
            # Get user description for context
            user_desc = st.session_state.get('user_data_description', '')
            
            # Generate cleaning suggestions
            st.markdown("### ü§ñ ƒê·ªÅ xu·∫•t L√†m s·∫°ch AI")
            
            if st.button("üìã T·∫°o K·∫ø ho·∫°ch L√†m s·∫°ch", type="primary"):
                with st.spinner("ü§ñ AI ƒëang ph√¢n t√≠ch v√† t·∫°o k·∫ø ho·∫°ch..."):
                    base_plan = get_cleaning_suggestions(col_stats, user_desc)
                    st.session_state.base_cleaning_plan = base_plan
            
            # Display cleaning plan
            if hasattr(st.session_state, 'base_cleaning_plan'):
                st.markdown("#### üìã K·∫ø ho·∫°ch L√†m s·∫°ch")
                st.markdown(st.session_state.base_cleaning_plan)
                
                # Allow user customization
                if st.toggle("üõ†Ô∏è T√πy ch·ªânh K·∫ø ho·∫°ch"):
                    user_input = st.text_area(
                        "‚úçÔ∏è ƒêi·ªÅu ch·ªânh k·∫ø ho·∫°ch l√†m s·∫°ch:",
                        placeholder="V√≠ d·ª•: Kh√¥ng x√≥a c·ªôt ID, ƒëi·ªÅn gi√° tr·ªã thi·∫øu b·∫±ng 0 thay v√¨ trung v·ªã...",
                        height=100
                    )
                    
                    if user_input and st.button("üîÑ C·∫≠p nh·∫≠t K·∫ø ho·∫°ch"):
                        with st.spinner("üîÑ ƒêang c·∫≠p nh·∫≠t k·∫ø ho·∫°ch..."):
                            updated_plan = refine_cleaning_strategy(user_input, st.session_state.base_cleaning_plan)
                            st.session_state.base_cleaning_plan = updated_plan
                            st.success("‚úÖ ƒê√£ c·∫≠p nh·∫≠t k·∫ø ho·∫°ch!")
                            st.rerun()
                
                # Generate cleaning code
                st.markdown("#### üêç M√£ Python L√†m s·∫°ch")
                
                if st.button("üîß T·∫°o M√£ L√†m s·∫°ch"):
                    with st.spinner("üîß ƒêang t·∫°o m√£ Python..."):
                        code_raw = generate_cleaning_code_from_plan(st.session_state.base_cleaning_plan)
                        code_clean = extract_valid_code(code_raw)
                        st.session_state.cleaning_code = code_clean
                
                # Display and execute cleaning code
                if hasattr(st.session_state, 'cleaning_code'):
                    
                    # Show the code
                    with st.expander("üëÄ Xem M√£ L√†m s·∫°ch", expanded=True):
                        st.code(st.session_state.cleaning_code, language="python")
                    
                    # Execute cleaning
                    if st.button("üöÄ Th·ª±c thi L√†m s·∫°ch", type="primary"):
                        try:
                            with st.spinner("üîÑ ƒêang l√†m s·∫°ch d·ªØ li·ªáu..."):
                                # Generate safe cleaning code
                                st.info("üîß ƒêang t·∫°o m√£ l√†m s·∫°ch an to√†n...")
                                code_raw = generate_cleaning_code_from_plan(st.session_state.base_cleaning_plan)
                                code_clean = extract_valid_code(code_raw)
                                st.session_state.cleaning_code = code_clean
                                
                                # Execute safely
                                st.info("‚ö° ƒêang th·ª±c thi m√£ l√†m s·∫°ch...")
                                cleaned_df, success = safe_execute_cleaning_code(code_clean, df)
                                
                                if success:
                                    # Store cleaned data
                                    st.session_state.cleaned_df = cleaned_df
                                    st.session_state.raw_df = df
                                    
                                    st.success("‚úÖ L√†m s·∫°ch d·ªØ li·ªáu th√†nh c√¥ng!")
                                    
                                    # Show before/after comparison
                                    col1, col2 = st.columns(2)
                                    
                                    with col1:
                                        st.markdown("**üìä Tr∆∞·ªõc l√†m s·∫°ch:**")
                                        st.write(f"K√≠ch th∆∞·ªõc: {df.shape}")
                                        st.write(f"Gi√° tr·ªã thi·∫øu: {df.isnull().sum().sum()}")
                                        st.write(f"Ki·ªÉu d·ªØ li·ªáu: {df.dtypes.nunique()} lo·∫°i kh√°c nhau")
                                    
                                    with col2:
                                        st.markdown("**‚ú® Sau l√†m s·∫°ch:**")
                                        st.write(f"K√≠ch th∆∞·ªõc: {cleaned_df.shape}")
                                        st.write(f"Gi√° tr·ªã thi·∫øu: {cleaned_df.isnull().sum().sum()}")
                                        st.write(f"Ki·ªÉu d·ªØ li·ªáu: {cleaned_df.dtypes.nunique()} lo·∫°i kh√°c nhau")
                                        
                                        # Calculate improvement
                                        missing_reduction = df.isnull().sum().sum() - cleaned_df.isnull().sum().sum()
                                        if missing_reduction > 0:
                                            st.metric("Gi·∫£m thi·ªÉu missing", f"{missing_reduction:,}", delta="C·∫£i thi·ªán")
                                    
                                else:
                                    st.error("‚ùå L√†m s·∫°ch d·ªØ li·ªáu th·∫•t b·∫°i. Vui l√≤ng ki·ªÉm tra l·∫°i k·∫ø ho·∫°ch l√†m s·∫°ch.")
                                    
                        except Exception as e:
                            st.error(f"‚ùå L·ªói khi th·ª±c thi m√£ l√†m s·∫°ch: {str(e)}")
                            
                            # Show debugging info
                            with st.expander("üêõ Th√¥ng tin Debug"):
                                st.write("**L·ªói chi ti·∫øt:**", str(e))
                                if 'cleaning_code' in st.session_state:
                                    st.write("**M√£ ƒë√£ th·ª±c thi:**")
                                    st.code(st.session_state.cleaning_code, language="python")
                                
                                st.write("**DataFrame info:**")
                                st.write(f"Shape: {df.shape}")
                                st.write(f"Dtypes: {df.dtypes.to_dict()}")
                                st.write("**Sample data:**")
                                st.dataframe(df.head(3))
                
                # Show cleaned data preview
                if hasattr(st.session_state, 'cleaned_df'):
                    st.markdown("### ‚úÖ D·ªØ li·ªáu ƒê√£ l√†m s·∫°ch")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**üìä Tr∆∞·ªõc l√†m s·∫°ch:**")
                        st.write(f"K√≠ch th∆∞·ªõc: {df.shape}")
                        st.write(f"Gi√° tr·ªã thi·∫øu: {df.isnull().sum().sum()}")
                        st.dataframe(df.head(3), use_container_width=True)
                    
                    with col2:
                        st.markdown("**‚ú® Sau l√†m s·∫°ch:**")
                        st.write(f"K√≠ch th∆∞·ªõc: {st.session_state.cleaned_df.shape}")
                        st.write(f"Gi√° tr·ªã thi·∫øu: {st.session_state.cleaned_df.isnull().sum().sum()}")
                        st.dataframe(st.session_state.cleaned_df.head(3), use_container_width=True)
                    
                    # Download cleaned data
                    csv_data = st.session_state.cleaned_df.to_csv(index=False)
                    st.download_button(
                        label="üì• T·∫£i xu·ªëng D·ªØ li·ªáu ƒê√£ l√†m s·∫°ch",
                        data=csv_data,
                        file_name="cleaned_dataset.csv",
                        mime="text/csv",
                        key="download_cleaned"
                    )

    with tab3:
        st.markdown("### üìà Ph√¢n t√≠ch Ph√¢n ph·ªëi & Th·ªëng k√™")
        
        if hasattr(st.session_state, 'cleaned_df') and hasattr(st.session_state, 'raw_df'):
            show_skew_kurtosis(st.session_state.raw_df, st.session_state.cleaned_df)
        else:
            st.info("üîÑ Vui l√≤ng ch·∫°y l√†m s·∫°ch d·ªØ li·ªáu trong tab 'üßº L√†m s·∫°ch Th√¥ng minh' ƒë·ªÉ xem ph√¢n t√≠ch n√†y.")
            
            # Show basic distribution analysis for original data
            st.markdown("#### üìä Ph√¢n t√≠ch Ph√¢n ph·ªëi C∆° b·∫£n")
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                selected_col = st.selectbox("Ch·ªçn c·ªôt ƒë·ªÉ ph√¢n t√≠ch:", numeric_cols)
                
                if selected_col:
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # Basic statistics
                        stats = df[selected_col].describe()
                        st.markdown("**üìä Th·ªëng k√™ m√¥ t·∫£:**")
                        st.dataframe(stats)
                        
                        # Additional metrics
                        skewness = df[selected_col].skew()
                        kurtosis = df[selected_col].kurtosis()
                        
                        st.markdown(f"**‚ÜóÔ∏è ƒê·ªô l·ªách:** {skewness:.3f}")
                        st.markdown(f"**üìà ƒê·ªô nh·ªçn:** {kurtosis:.3f}")
                        
                        # Interpretation
                        if abs(skewness) < 0.5:
                            skew_interp = "G·∫ßn ƒë·ªëi x·ª©ng"
                        elif abs(skewness) < 1:
                            skew_interp = "H∆°i l·ªách"
                        else:
                            skew_interp = "L·ªách m·∫°nh"
                        
                        st.info(f"üîç **Ph√¢n t√≠ch:** Ph√¢n ph·ªëi {skew_interp}")
                    
                    with col2:
                        # Distribution plot
                        plot_distribution(selected_col, df[selected_col])

if __name__ == "__main__":
    main()