import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from src.utils import (get_all_datasets, get_dataset, 
                       save_dataset_analysis, get_dataset_analysis, 
                       delete_dataset_analysis, is_analysis_outdated)
from src.models.llms import load_llm
import json
import time

st.set_page_config(page_title="ğŸ“‚ Chi Tiáº¿t Bá»™ Dá»¯ Liá»‡u", layout="wide")
st.title("ğŸ“‚ Chi Tiáº¿t Bá»™ Dá»¯ Liá»‡u")

# Add custom CSS for better styling
st.markdown("""
<style>
    .data-description-box {
        background: linear-gradient(135deg, #667eea15 0%, #764ba215 100%);
        border: 2px solid #667eea30;
        padding: 1.5rem;
        border-radius: 12px;
        margin: 1rem 0;
    }
    .column-analysis-card {
        color: black;
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #667eea;
        margin: 0.5rem 0;
    }
    .insight-badge {
        background: linear-gradient(135deg, #28a745 0%, #20c997 100%);
        color: white;
        padding: 0.25rem 0.5rem;
        border-radius: 12px;
        font-size: 0.8rem;
        display: inline-block;
        margin: 0.25rem 0;
    }
    .warning-badge {
        background: linear-gradient(135deg, #ffc107 0%, #fd7e14 100%);
        color: white;
        padding: 0.25rem 0.5rem;
        border-radius: 12px;
        font-size: 0.8rem;
        display: inline-block;
        margin: 0.25rem 0;
    }
    .error-badge {
        background: linear-gradient(135deg, #dc3545 0%, #e55353 100%);
        color: white;
        padding: 0.25rem 0.5rem;
        border-radius: 12px;
        font-size: 0.8rem;
        display: inline-block;
        margin: 0.25rem 0;
    }
    .cache-info {
        background: #e3f2fd;
        border: 1px solid #2196f3;
        padding: 0.75rem;
        border-radius: 6px;
        margin: 0.5rem 0;
        font-size: 0.9rem;
    }
</style>
""", unsafe_allow_html=True)

llm = load_llm("gpt-3.5-turbo")

# ---------- Helper functions with enhanced error handling ----------
def safe_read_csv(file_path):
    """Safely read CSV with multiple encoding attempts"""
    for enc in ['utf-8', 'ISO-8859-1', 'utf-16', 'cp1252']:
        try:
            return pd.read_csv(file_path, encoding=enc)
        except UnicodeDecodeError:
            continue
        except Exception as e:
            st.error(f"Error reading file with {enc}: {str(e)}")
            continue
    raise UnicodeDecodeError("utf-8", b"", 0, 1, "Unable to decode file with common encodings.")

def extract_llm_content(response):
    """TrÃ­ch xuáº¥t ná»™i dung tá»« LLM response object"""
    try:
        # Náº¿u response cÃ³ thuá»™c tÃ­nh content
        if hasattr(response, 'content'):
            return response.content
        
        # Náº¿u response lÃ  string
        elif isinstance(response, str):
            return response
        
        # Náº¿u response cÃ³ thuá»™c tÃ­nh text
        elif hasattr(response, 'text'):
            return response.text
        
        # Náº¿u response cÃ³ thuá»™c tÃ­nh message vÃ  content
        elif hasattr(response, 'message') and hasattr(response.message, 'content'):
            return response.message.content
        
        # Fallback: convert to string
        else:
            return str(response)
            
    except Exception as e:
        st.warning(f"KhÃ´ng thá»ƒ trÃ­ch xuáº¥t ná»™i dung LLM: {str(e)}")
        return "KhÃ´ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c Ã½ nghÄ©a"

def analyze_column(col_name, series):
    """Enhanced column analysis with better error handling"""
    try:
        info = {
            'name': col_name, 
            'dtype': str(series.dtype), 
            'missing_pct': series.isna().mean() * 100, 
            'unique': series.nunique(),
            'total_count': len(series)
        }
        
        if pd.api.types.is_numeric_dtype(series):
            desc = series.describe()
            info.update({
                'min': desc['min'], 
                'max': desc['max'], 
                'mean': desc['mean'],
                'median': series.median(), 
                'std': desc['std'],
                'outliers': ((series < (desc['25%'] - 1.5*(desc['75%'] - desc['25%']))) | 
                           (series > (desc['75%'] + 1.5*(desc['75%'] - desc['25%'])))).sum(),
                'type': 'Numeric',
                'skewness': series.skew(),
                'kurtosis': series.kurtosis()
            })
        elif series.nunique() == 2:
            info['type'] = 'Boolean'
            info['value_counts'] = series.value_counts().to_dict()
        elif info['unique'] == len(series):
            info['type'] = 'ID'
        elif info['unique'] <= 20:
            info['type'] = 'Category'
            info['value_counts'] = series.value_counts().head(10).to_dict()
        else:
            info['type'] = 'Text'
            info['avg_length'] = series.astype(str).str.len().mean()
            info['max_length'] = series.astype(str).str.len().max()
        
        return info
    except Exception as e:
        return {
            'name': col_name,
            'dtype': 'Error',
            'missing_pct': 100,
            'unique': 0,
            'type': 'Error',
            'error': str(e)
        }

def guess_column_semantic_llm(col_name, sample_values=None):
    """Enhanced semantic analysis with sample values"""
    try:
        sample_text = ""
        if sample_values is not None and len(sample_values) > 0:
            # Convert sample values to string and take first few
            sample_str = [str(v) for v in sample_values[:5] if pd.notna(v)]
            if sample_str:
                sample_text = f" GiÃ¡ trá»‹ máº«u: {', '.join(sample_str)}"
        
        prompt = f"Loáº¡i ngá»¯ nghÄ©a cá»§a cá»™t '{col_name}'{sample_text} lÃ  gÃ¬? Tráº£ lá»i báº±ng 3-5 tá»« tiáº¿ng Viá»‡t mÃ´ táº£ Ã½ nghÄ©a (vÃ­ dá»¥: 'ID khÃ¡ch hÃ ng', 'NgÃ y sinh', 'TÃªn sáº£n pháº©m')."
        
        response = llm.invoke(prompt)
        
        # Sá»­ dá»¥ng hÃ m extract_llm_content Ä‘á»ƒ láº¥y ná»™i dung
        result = extract_llm_content(response)
        return result.strip()
        
    except Exception as e:
        return f"KhÃ´ng xÃ¡c Ä‘á»‹nh ({str(e)[:50]}...)"

@st.cache_data(show_spinner=False)
def get_cleaning_suggestions(col_stats, user_description=""):
    """Enhanced cleaning suggestions with user context"""
    try:
        cols_description = "\n".join([
            f"Cá»™t: {col['name']} | Loáº¡i: {col['dtype']} | Thiáº¿u: {col['missing_pct']:.2f}% | Duy nháº¥t: {col['unique']}" 
            for col in col_stats if 'error' not in col
        ])
        
        context_text = f"\nMÃ´ táº£ ngÆ°á»i dÃ¹ng: {user_description}" if user_description else ""
        
        prompt = f"""
Dá»±a trÃªn tÃ³m táº¯t sau vá» cÃ¡c cá»™t trong bá»™ dá»¯ liá»‡u:
{cols_description}{context_text}

HÃ£y Ä‘á» xuáº¥t káº¿ hoáº¡ch lÃ m sáº¡ch vá»›i cÃ¡c quy táº¯c sau:
- Chá»‰ xÃ³a cÃ¡c cá»™t náº¿u tá»· lá»‡ thiáº¿u > 70% hoáº·c toÃ n bá»™ lÃ  ID khÃ´ng cáº§n thiáº¿t.
- Äá»‘i vá»›i cÃ¡c cá»™t cÃ³ giÃ¡ trá»‹ thiáº¿u â‰¤ 70%:
    - Náº¿u lÃ  sá»‘: Ä‘iá»n báº±ng trung vá»‹ hoáº·c trung bÃ¬nh.
    - Náº¿u lÃ  phÃ¢n loáº¡i: Ä‘iá»n báº±ng mode hoáº·c 'Unknown'.
- Chá»‰ loáº¡i bá» ngoáº¡i lá»‡ tá»« cÃ¡c cá»™t sá»‘ cÃ³ outliers > 5% tá»•ng dá»¯ liá»‡u.
- Chuáº©n hÃ³a cÃ¡c cá»™t sá»‘ chá»‰ khi cáº§n thiáº¿t cho phÃ¢n tÃ­ch.
- Æ¯u tiÃªn giá»¯ nguyÃªn dá»¯ liá»‡u náº¿u cÃ³ thá»ƒ.
- Äá» xuáº¥t chuyá»ƒn Ä‘á»•i kiá»ƒu dá»¯ liá»‡u náº¿u phÃ¹ há»£p.

Tráº£ vá» káº¿ hoáº¡ch dÆ°á»›i dáº¡ng danh sÃ¡ch cÃ³ cáº¥u trÃºc rÃµ rÃ ng vá»›i lÃ½ do.
"""
        response = llm.invoke(prompt)
        return extract_llm_content(response)
    except Exception as e:
        return f"Lá»—i táº¡o Ä‘á» xuáº¥t lÃ m sáº¡ch: {str(e)}"

@st.cache_data(show_spinner=False)
def refine_cleaning_strategy(user_input, _base_plan):
    """Refine cleaning strategy based on user input"""
    try:
        base_plan_text = extract_llm_content(_base_plan)
        
        prompt = f"""
Káº¿ hoáº¡ch lÃ m sáº¡ch hiá»‡n táº¡i:
{base_plan_text}

NgÆ°á»i dÃ¹ng muá»‘n Ä‘iá»u chá»‰nh: {user_input}

Cáº­p nháº­t káº¿ hoáº¡ch lÃ m sáº¡ch phÃ¹ há»£p vá»›i yÃªu cáº§u cá»§a ngÆ°á»i dÃ¹ng. Giá»¯ nguyÃªn cÃ¡c pháº§n tá»‘t vÃ  chá»‰ thay Ä‘á»•i theo yÃªu cáº§u.
"""
        response = llm.invoke(prompt)
        return extract_llm_content(response)
    except Exception as e:
        return f"Lá»—i cáº­p nháº­t káº¿ hoáº¡ch: {str(e)}"

@st.cache_data(show_spinner=False)
def generate_cleaning_code_from_plan(_plan):
    """Enhanced code generation with better error handling"""
    try:
        plan_text = extract_llm_content(_plan)
        
        prompt = f"""
Chuyá»ƒn Ä‘á»•i káº¿ hoáº¡ch lÃ m sáº¡ch sau thÃ nh mÃ£ Python há»£p lá»‡ sá»­ dá»¥ng pandas.
Chá»‰ tráº£ vá» mÃ£ Python cÃ³ thá»ƒ thá»±c thi trá»±c tiáº¿p.
Giáº£ Ä‘á»‹nh dataframe Ä‘Æ°á»£c Ä‘áº·t tÃªn lÃ  `df`.

Quan trá»ng:
- LuÃ´n kiá»ƒm tra dtype trÆ°á»›c khi Ã¡p dá»¥ng .str methods
- Xá»­ lÃ½ lá»—i vá»›i try-except cho tá»«ng bÆ°á»›c
- ThÃªm print statements Ä‘á»ƒ theo dÃµi quÃ¡ trÃ¬nh
- Äáº£m báº£o mÃ£ hoáº¡t Ä‘á»™ng vá»›i dá»¯ liá»‡u thá»±c

Káº¿ hoáº¡ch LÃ m sáº¡ch:
{plan_text}

Tráº£ vá» mÃ£ Python Ä‘áº§y Ä‘á»§:
"""
        response = llm.invoke(prompt)
        return extract_llm_content(response)
    except Exception as e:
        return f"# Lá»—i táº¡o mÃ£: {str(e)}\nprint('KhÃ´ng thá»ƒ táº¡o mÃ£ lÃ m sáº¡ch')"

def generate_insight(info):
    """Generate insights for column analysis"""
    try:
        if 'error' in info:
            return f"âŒ Lá»—i phÃ¢n tÃ­ch: {info['error']}"
        
        if info['type'] == 'ID':
            return "ğŸ”¹ ÄÃ¢y lÃ  cá»™t Ä‘á»‹nh danh duy nháº¥t."
        
        if info['missing_pct'] > 50:
            return f"âš ï¸ {info['missing_pct']:.1f}% giÃ¡ trá»‹ thiáº¿u - cáº§n xem xÃ©t loáº¡i bá»."
        elif info['missing_pct'] > 10:
            return f"âš ï¸ {info['missing_pct']:.1f}% giÃ¡ trá»‹ thiáº¿u - cáº§n Ä‘iá»n bá»• sung."
        
        if info['type'] == 'Numeric':
            if 'std' in info and info['std'] < 1e-3:
                return "âš ï¸ Äá»™ biáº¿n thiÃªn ráº¥t tháº¥p - cÃ³ thá»ƒ lÃ  háº±ng sá»‘."
            if 'outliers' in info and info['outliers'] > 0:
                outlier_pct = (info['outliers'] / info['total_count']) * 100
                if outlier_pct > 5:
                    return f"âš ï¸ {info['outliers']} ngoáº¡i lá»‡ ({outlier_pct:.1f}%) - cáº§n kiá»ƒm tra."
        
        if info['unique'] < 5 and info['type'] == 'Category':
            return "â„¹ï¸ PhÃ¢n loáº¡i vá»›i Ã­t giÃ¡ trá»‹ - phÃ¹ há»£p cho grouping."
        
        if info['type'] == 'Text' and 'avg_length' in info:
            if info['avg_length'] > 100:
                return f"ğŸ“ VÄƒn báº£n dÃ i (TB: {info['avg_length']:.0f} kÃ½ tá»±) - cÃ³ thá»ƒ cáº§n xá»­ lÃ½ NLP."
        
        return "âœ… KhÃ´ng phÃ¡t hiá»‡n váº¥n Ä‘á» lá»›n."
    except Exception as e:
        return f"âŒ Lá»—i táº¡o insight: {str(e)}"

def plot_distribution(col_name, series):
    """Enhanced distribution plotting with error handling"""
    try:
        fig, ax = plt.subplots(figsize=(8, 5))
        
        if pd.api.types.is_numeric_dtype(series):
            # Numeric distribution
            clean_series = series.dropna()
            if len(clean_series) > 0:
                ax.hist(clean_series, bins=min(30, len(clean_series.unique())), 
                       color='#69b3a2', alpha=0.7, edgecolor='black')
                ax.axvline(clean_series.mean(), color='red', linestyle='--', 
                          label=f'Trung bÃ¬nh: {clean_series.mean():.2f}')
                ax.axvline(clean_series.median(), color='orange', linestyle='--', 
                          label=f'Trung vá»‹: {clean_series.median():.2f}')
                ax.legend()
                ax.set_xlabel(col_name)
                ax.set_ylabel('Táº§n suáº¥t')
        else:
            # Categorical distribution
            vc = series.fillna("NaN").value_counts().head(15)  # Show top 15
            if len(vc) > 0:
                bars = ax.bar(range(len(vc)), vc.values, color='#8c54ff', alpha=0.7)
                ax.set_xticks(range(len(vc)))
                ax.set_xticklabels([str(x) for x in vc.index], rotation=45, ha='right')
                ax.set_ylabel('Sá»‘ lÆ°á»£ng')
                
                # Add value labels on bars
                for bar, value in zip(bars, vc.values):
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(vc.values)*0.01,
                           str(value), ha='center', va='bottom', fontsize=9)
        
        ax.set_title(f"PhÃ¢n phá»‘i: {col_name}", fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        st.pyplot(fig)
        plt.close()
        
    except Exception as e:
        st.error(f"Lá»—i váº½ biá»ƒu Ä‘á»“ cho {col_name}: {str(e)}")

def perform_column_analysis(df, dataset_id):
    """Thá»±c hiá»‡n phÃ¢n tÃ­ch cá»™t vá»›i progress bar"""
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    col_analyses = []
    
    for i, col in enumerate(df.columns):
        status_text.text(f"Äang phÃ¢n tÃ­ch cá»™t: {col}")
        progress_bar.progress((i + 1) / len(df.columns))
        
        # Column analysis
        stats = analyze_column(col, df[col])
        
        # Get sample values for semantic analysis
        sample_vals = df[col].dropna().head(5).tolist()
        semantic = guess_column_semantic_llm(col, sample_vals)
        stats['semantic'] = semantic
        
        col_analyses.append(stats)
        
        time.sleep(0.1)  # Brief pause to show progress
    
    status_text.text("âœ… HoÃ n thÃ nh phÃ¢n tÃ­ch!")
    progress_bar.progress(1.0)
    
    # LÆ°u káº¿t quáº£ phÃ¢n tÃ­ch vÃ o database
    save_dataset_analysis(dataset_id, col_analyses)
    
    time.sleep(0.5)
    status_text.empty()
    progress_bar.empty()
    
    return col_analyses

def extract_valid_code(llm_response):
    """Extract valid Python code from LLM response"""
    try:
        # Try to extract code between ```python and ```
        match = re.search(r"```(?:python)?\n(.*?)```", llm_response.strip(), re.DOTALL)
        if match:
            return match.group(1)
        
        # If no code blocks, try to extract lines that look like Python code
        lines = llm_response.splitlines()
        code_lines = []
        for line in lines:
            stripped = line.strip()
            if stripped and not stripped.startswith("#") and not stripped.startswith("Káº¿ hoáº¡ch"):
                # Basic check if it looks like Python code
                if any(keyword in stripped for keyword in ['df[', 'df.', 'pd.', 'np.', '=', 'print(', 'try:', 'except:']):
                    code_lines.append(line)
        
        return "\n".join(code_lines) if code_lines else llm_response
    except Exception as e:
        return f"# Error extracting code: {str(e)}\n{llm_response}"

def fix_numeric_strings(df):
    """Enhanced numeric string fixing"""
    fixed_cols = []
    for col in df.select_dtypes(include='object').columns:
        try:
            if df[col].dropna().apply(lambda x: isinstance(x, str)).all():
                # Try to convert numeric strings
                original_type = df[col].dtype
                test_series = df[col].str.replace(',', '', regex=False)
                test_series = pd.to_numeric(test_series, errors='coerce')
                
                # If more than 80% can be converted, do the conversion
                valid_ratio = test_series.notna().sum() / len(test_series)
                if valid_ratio > 0.8:
                    df[col] = test_series
                    fixed_cols.append(col)
        except Exception as e:
            continue
    
    if fixed_cols:
        st.info(f"âœ… ÄÃ£ chuyá»ƒn Ä‘á»•i cÃ¡c cá»™t sá»‘: {', '.join(fixed_cols)}")
    
    return df

def show_skew_kurtosis(df, cleaned_df):
    """Enhanced skewness and kurtosis analysis"""
    try:
        raw_cols = df.select_dtypes(include='number').columns
        clean_cols = cleaned_df.select_dtypes(include='number').columns
        numeric_cols = list(set(raw_cols).intersection(set(clean_cols)))

        if not numeric_cols:
            st.info("KhÃ´ng cÃ³ cá»™t sá»‘ chung nÃ o kháº£ dá»¥ng cho bÃ¡o cÃ¡o Ä‘á»™ lá»‡ch/Ä‘á»™ nhá»n.")
            return

        # Create comprehensive report
        report = pd.DataFrame(index=numeric_cols)
        report['Äá»™ lá»‡ch (TrÆ°á»›c)'] = df[numeric_cols].skew()
        report['Äá»™ nhá»n (TrÆ°á»›c)'] = df[numeric_cols].kurtosis()
        report['Äá»™ lá»‡ch (Sau)'] = cleaned_df[numeric_cols].skew()
        report['Äá»™ nhá»n (Sau)'] = cleaned_df[numeric_cols].kurtosis()
        
        # Calculate improvements
        report['Cáº£i thiá»‡n Äá»™ lá»‡ch'] = abs(report['Äá»™ lá»‡ch (TrÆ°á»›c)']) - abs(report['Äá»™ lá»‡ch (Sau)'])
        report['Cáº£i thiá»‡n Äá»™ nhá»n'] = abs(report['Äá»™ nhá»n (TrÆ°á»›c)']) - abs(report['Äá»™ nhá»n (Sau)'])
        
        st.dataframe(report.round(3), use_container_width=True)

        # Visualization with subplots
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Skewness comparison
        x_pos = np.arange(len(numeric_cols))
        width = 0.35
        
        ax1.bar(x_pos - width/2, report['Äá»™ lá»‡ch (TrÆ°á»›c)'], width, 
               label='TrÆ°á»›c', alpha=0.8, color='#ff7f7f')
        ax1.bar(x_pos + width/2, report['Äá»™ lá»‡ch (Sau)'], width,
               label='Sau', alpha=0.8, color='#7fbf7f')
        
        ax1.set_xlabel('Äáº·c trÆ°ng')
        ax1.set_ylabel('Äá»™ lá»‡ch')
        ax1.set_title('So sÃ¡nh Äá»™ lá»‡ch TrÆ°á»›c vs Sau LÃ m sáº¡ch')
        ax1.set_xticks(x_pos)
        ax1.set_xticklabels(numeric_cols, rotation=45, ha='right')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        
        # Kurtosis comparison
        ax2.bar(x_pos - width/2, report['Äá»™ nhá»n (TrÆ°á»›c)'], width,
               label='TrÆ°á»›c', alpha=0.8, color='#ff7f7f')
        ax2.bar(x_pos + width/2, report['Äá»™ nhá»n (Sau)'], width,
               label='Sau', alpha=0.8, color='#7fbf7f')
        
        ax2.set_xlabel('Äáº·c trÆ°ng')
        ax2.set_ylabel('Äá»™ nhá»n')
        ax2.set_title('So sÃ¡nh Äá»™ nhá»n TrÆ°á»›c vs Sau LÃ m sáº¡ch')
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels(numeric_cols, rotation=45, ha='right')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)
        plt.close()

        # Generate AI insights
        try:
            interpretation_prompt = f"""
PhÃ¢n tÃ­ch bÃ¡o cÃ¡o Ä‘á»™ lá»‡ch vÃ  Ä‘á»™ nhá»n sau:

{report.to_string()}

HÃ£y Ä‘Æ°a ra nháº­n xÃ©t vá»:
1. Nhá»¯ng cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ trong phÃ¢n phá»‘i dá»¯ liá»‡u
2. CÃ¡c cá»™t cÃ²n cáº§n xá»­ lÃ½ thÃªm
3. TÃ¡c Ä‘á»™ng Ä‘áº¿n cháº¥t lÆ°á»£ng phÃ¢n tÃ­ch
4. Äá» xuáº¥t bÆ°á»›c tiáº¿p theo

Tráº£ lá»i báº±ng markdown vá»›i format Ä‘áº¹p vÃ  dá»… hiá»ƒu.
"""
            
            response = llm.invoke(interpretation_prompt)
            interpretation = extract_llm_content(response)
            st.markdown("### ğŸ¤– PhÃ¢n tÃ­ch AI")
            st.markdown(interpretation)
            
        except Exception as e:
            st.warning(f"KhÃ´ng thá»ƒ táº¡o phÃ¢n tÃ­ch AI: {str(e)}")

    except Exception as e:
        st.error(f"Lá»—i trong phÃ¢n tÃ­ch Ä‘á»™ lá»‡ch/Ä‘á»™ nhá»n: {str(e)}")

# Main application
def main():
    # Load datasets
    datasets = get_all_datasets()
    if not datasets:
        st.warning("KhÃ´ng tÃ¬m tháº¥y bá»™ dá»¯ liá»‡u nÃ o. Vui lÃ²ng táº£i lÃªn má»™t bá»™ dá»¯ liá»‡u trong Báº£ng Ä‘iá»u khiá»ƒn.")
        st.stop()

    # Dataset selection
    selected = st.selectbox("Chá»n bá»™ dá»¯ liá»‡u:", [f"{d[0]} - {d[1]}" for d in datasets])
    dataset_id = int(selected.split(" - ")[0])
    dataset = get_dataset(dataset_id)
    
    try:
        df = safe_read_csv(dataset[2])
    except Exception as e:
        st.error(f"âŒ KhÃ´ng thá»ƒ Ä‘á»c file: {str(e)}")
        st.stop()

    st.markdown(f"### Bá»™ dá»¯ liá»‡u: `{dataset[1]}` â€” {df.shape[0]:,} hÃ ng Ã— {df.shape[1]} cá»™t")

    # Add data description section
    st.markdown("### ğŸ“ MÃ´ táº£ Dá»¯ liá»‡u")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # User data description input
        user_description = st.text_area(
            "âœï¸ MÃ´ táº£ bá»™ dá»¯ liá»‡u cá»§a báº¡n:",
            height=100,
            placeholder="VÃ­ dá»¥: Dá»¯ liá»‡u bÃ¡n hÃ ng tá»« 2023-2024, bao gá»“m thÃ´ng tin khÃ¡ch hÃ ng, sáº£n pháº©m vÃ  doanh thu. ÄÆ°á»£c thu tháº­p tá»« há»‡ thá»‘ng POS...",
            help="MÃ´ táº£ chi tiáº¿t giÃºp AI hiá»ƒu rÃµ hÆ¡n vá» ngá»¯ cáº£nh vÃ  Ä‘Æ°a ra gá»£i Ã½ chÃ­nh xÃ¡c hÆ¡n"
        )
        
        if user_description:
            st.session_state.user_data_description = user_description
    
    with col2:
        st.markdown("**ğŸ’¡ Máº¹o viáº¿t mÃ´ táº£ tá»‘t:**")
        st.markdown("""
        - Nguá»“n gá»‘c dá»¯ liá»‡u
        - Má»¥c Ä‘Ã­ch thu tháº­p  
        - Khoáº£ng thá»i gian
        - Ã nghÄ©a cÃ¡c cá»™t chÃ­nh
        - ÄÆ¡n vá»‹ Ä‘o lÆ°á»ng
        - LÆ°u Ã½ Ä‘áº·c biá»‡t
        """)

    # Create tabs for different analysis sections
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š Tá»•ng quan Chi tiáº¿t", "ğŸ§¼ LÃ m sáº¡ch ThÃ´ng minh", "ğŸ“ˆ PhÃ¢n tÃ­ch PhÃ¢n phá»‘i"])

    with tab1:
        st.markdown("### ğŸ” PhÃ¢n tÃ­ch Chi tiáº¿t tá»«ng Cá»™t")
        
        # Kiá»ƒm tra xem Ä‘Ã£ cÃ³ phÃ¢n tÃ­ch cached khÃ´ng
        cached_analysis = get_dataset_analysis(dataset_id)
        
        # Hiá»ƒn thá»‹ thÃ´ng tin cache
        if cached_analysis:
            is_outdated = is_analysis_outdated(cached_analysis, dataset[4])  # dataset[4] lÃ  upload_time
            
            if is_outdated:
                st.markdown("""
                <div class="cache-info">
                    âš ï¸ <strong>PhÃ¢n tÃ­ch cÅ© Ä‘Æ°á»£c tÃ¬m tháº¥y</strong> - Dataset Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t sau láº§n phÃ¢n tÃ­ch cuá»‘i. 
                    NÃªn cháº¡y phÃ¢n tÃ­ch láº¡i Ä‘á»ƒ cÃ³ káº¿t quáº£ chÃ­nh xÃ¡c nháº¥t.
                </div>
                """, unsafe_allow_html=True)
            else:
                st.markdown(f"""
                <div class="cache-info">
                    âœ… <strong>PhÃ¢n tÃ­ch cÃ³ sáºµn</strong> - ÄÃ£ phÃ¢n tÃ­ch lÃºc {cached_analysis['updated_at']}. 
                    Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng káº¿t quáº£ nÃ y hoáº·c cháº¡y phÃ¢n tÃ­ch láº¡i.
                </div>
                """, unsafe_allow_html=True)
        
        # Buttons cho viá»‡c phÃ¢n tÃ­ch
        col_btn1, col_btn2, col_btn3 = st.columns(3)
        
        with col_btn1:
            if cached_analysis and not is_analysis_outdated(cached_analysis, dataset[4]):
                if st.button("ğŸ“‹ Sá»­ dá»¥ng PhÃ¢n tÃ­ch CÃ³ sáºµn", type="secondary"):
                    st.session_state.col_analyses = cached_analysis['analysis']
                    st.success("âœ… ÄÃ£ táº£i phÃ¢n tÃ­ch tá»« cache!")
                    st.rerun()
        
        with col_btn2:
            if st.button("ğŸš€ Cháº¡y PhÃ¢n tÃ­ch Má»›i", type="primary"):
                with st.spinner("ğŸ”„ Äang phÃ¢n tÃ­ch dá»¯ liá»‡u..."):
                    col_analyses = perform_column_analysis(df, dataset_id)
                    st.session_state.col_analyses = col_analyses
                    st.success("âœ… HoÃ n thÃ nh phÃ¢n tÃ­ch má»›i!")
                    st.rerun()
        
        with col_btn3:
            if cached_analysis:
                if st.button("ğŸ—‘ï¸ XÃ³a Cache", type="secondary"):
                    delete_dataset_analysis(dataset_id)
                    st.success("ğŸ—‘ï¸ ÄÃ£ xÃ³a cache phÃ¢n tÃ­ch!")
                    st.rerun()
        
        # Display analysis results
        if hasattr(st.session_state, 'col_analyses'):
            st.markdown("---")
            
            for analysis in st.session_state.col_analyses:
                col_name = analysis['name']
                
                with st.container():
                    st.markdown(f"""
                    <div class="column-analysis-card">
                        <h4>ğŸ“Œ {col_name}</h4>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    col_left, col_right = st.columns([2, 3])
                    
                    with col_left:
                        # Basic statistics
                        st.markdown(f"**ğŸ·ï¸ Loáº¡i:** `{analysis['type']}`")
                        st.markdown(f"**ğŸ“Š Kiá»ƒu dá»¯ liá»‡u:** `{analysis['dtype']}`")
                        st.markdown(f"**ğŸ§© Ã nghÄ©a:** {analysis['semantic']}")
                        st.markdown(f"**ğŸ”¢ Duy nháº¥t:** `{analysis['unique']:,}`")
                        st.markdown(f"**âŒ Thiáº¿u:** `{analysis['missing_pct']:.2f}%`")
                        
                        # Type-specific information
                        if analysis['type'] == 'Numeric' and 'mean' in analysis:
                            st.markdown(f"**ğŸ“ˆ Trung bÃ¬nh:** `{analysis['mean']:.2f}`")
                            st.markdown(f"**ğŸ“Š Trung vá»‹:** `{analysis['median']:.2f}`")
                            st.markdown(f"**ğŸ“ Äá»™ lá»‡ch chuáº©n:** `{analysis['std']:.2f}`")
                            st.markdown(f"**âš ï¸ Ngoáº¡i lá»‡:** `{analysis['outliers']}`")
                            if 'skewness' in analysis:
                                st.markdown(f"**â†—ï¸ Äá»™ lá»‡ch:** `{analysis['skewness']:.2f}`")
                        
                        elif analysis['type'] == 'Category' and 'value_counts' in analysis:
                            st.markdown("**ğŸ† Top giÃ¡ trá»‹:**")
                            for val, count in list(analysis['value_counts'].items())[:3]:
                                st.markdown(f"  - `{val}`: {count}")
                        
                        elif analysis['type'] == 'Text' and 'avg_length' in analysis:
                            st.markdown(f"**ğŸ“ Äá»™ dÃ i TB:** `{analysis['avg_length']:.1f}`")
                            st.markdown(f"**ğŸ“ Äá»™ dÃ i tá»‘i Ä‘a:** `{analysis['max_length']}`")
                        
                        # Generate and display insight
                        insight = generate_insight(analysis)
                        if "âœ…" in insight:
                            badge_class = "insight-badge"
                        elif "âš ï¸" in insight:
                            badge_class = "warning-badge"
                        else:
                            badge_class = "error-badge"
                        
                        st.markdown(f'<span class="{badge_class}">{insight}</span>', 
                                  unsafe_allow_html=True)
                    
                    with col_right:
                        # Distribution plot
                        if analysis['type'] != 'Error':
                            plot_distribution(col_name, df[col_name])
                        else:
                            st.error(f"Lá»—i phÃ¢n tÃ­ch cá»™t: {analysis.get('error', 'Unknown error')}")
                    
                    st.markdown("---")

    with tab2:
        st.markdown("### ğŸ§¼ LÃ m sáº¡ch Dá»¯ liá»‡u ThÃ´ng minh")
        
        # Get column statistics
        if not hasattr(st.session_state, 'col_analyses'):
            st.info("ğŸ”„ Vui lÃ²ng cháº¡y phÃ¢n tÃ­ch trong tab 'Tá»•ng quan Chi tiáº¿t' trÆ°á»›c.")
        else:
            col_stats = st.session_state.col_analyses
            
            # Display summary table
            summary_data = []
            for stat in col_stats:
                if 'error' not in stat:
                    summary_data.append({
                        'Cá»™t': stat['name'],
                        'Loáº¡i': stat['type'],
                        'Kiá»ƒu dá»¯ liá»‡u': stat['dtype'],
                        'Ã nghÄ©a': stat['semantic'][:30] + "..." if len(stat['semantic']) > 30 else stat['semantic'],
                        'Duy nháº¥t': stat['unique'],
                        'Thiáº¿u %': f"{stat['missing_pct']:.1f}%"
                    })
            
            summary_df = pd.DataFrame(summary_data)
            st.dataframe(summary_df, use_container_width=True)
            
            # Get user description for context
            user_desc = st.session_state.get('user_data_description', '')
            
            # Generate cleaning suggestions
            st.markdown("### ğŸ¤– Äá» xuáº¥t LÃ m sáº¡ch AI")
            
            if st.button("ğŸ“‹ Táº¡o Káº¿ hoáº¡ch LÃ m sáº¡ch", type="primary"):
                with st.spinner("ğŸ¤– AI Ä‘ang phÃ¢n tÃ­ch vÃ  táº¡o káº¿ hoáº¡ch..."):
                    base_plan = get_cleaning_suggestions(col_stats, user_desc)
                    st.session_state.base_cleaning_plan = base_plan
            
            # Display cleaning plan
            if hasattr(st.session_state, 'base_cleaning_plan'):
                st.markdown("#### ğŸ“‹ Káº¿ hoáº¡ch LÃ m sáº¡ch")
                st.markdown(st.session_state.base_cleaning_plan)
                
                # Allow user customization
                if st.toggle("ğŸ› ï¸ TÃ¹y chá»‰nh Káº¿ hoáº¡ch"):
                    user_input = st.text_area(
                        "âœï¸ Äiá»u chá»‰nh káº¿ hoáº¡ch lÃ m sáº¡ch:",
                        placeholder="VÃ­ dá»¥: KhÃ´ng xÃ³a cá»™t ID, Ä‘iá»n giÃ¡ trá»‹ thiáº¿u báº±ng 0 thay vÃ¬ trung vá»‹...",
                        height=100
                    )
                    
                    if user_input and st.button("ğŸ”„ Cáº­p nháº­t Káº¿ hoáº¡ch"):
                        with st.spinner("ğŸ”„ Äang cáº­p nháº­t káº¿ hoáº¡ch..."):
                            updated_plan = refine_cleaning_strategy(user_input, st.session_state.base_cleaning_plan)
                            st.session_state.base_cleaning_plan = updated_plan
                            st.success("âœ… ÄÃ£ cáº­p nháº­t káº¿ hoáº¡ch!")
                            st.rerun()
                
                # Generate cleaning code
                st.markdown("#### ğŸ MÃ£ Python LÃ m sáº¡ch")
                
                if st.button("ğŸ”§ Táº¡o MÃ£ LÃ m sáº¡ch"):
                    with st.spinner("ğŸ”§ Äang táº¡o mÃ£ Python..."):
                        code_raw = generate_cleaning_code_from_plan(st.session_state.base_cleaning_plan)
                        code_clean = extract_valid_code(code_raw)
                        st.session_state.cleaning_code = code_clean
                
                # Display and execute cleaning code
                if hasattr(st.session_state, 'cleaning_code'):
                    
                    # Show the code
                    with st.expander("ğŸ‘€ Xem MÃ£ LÃ m sáº¡ch", expanded=True):
                        st.code(st.session_state.cleaning_code, language="python")
                    
                    # Execute cleaning
                    if st.button("ğŸš€ Thá»±c thi LÃ m sáº¡ch", type="primary"):
                        try:
                            with st.spinner("ğŸ”„ Äang lÃ m sáº¡ch dá»¯ liá»‡u..."):
                                # Prepare execution environment
                                exec_globals = {
                                    'df': df.copy(), 
                                    'pd': pd, 
                                    'np': np, 
                                    'fix_numeric_strings': fix_numeric_strings,
                                    'print': st.write  # Redirect print to streamlit
                                }
                                
                                # Execute cleaning code
                                exec("df = fix_numeric_strings(df)\n" + st.session_state.cleaning_code, exec_globals)
                                cleaned_df = exec_globals['df']
                                
                                # Store cleaned data
                                st.session_state.cleaned_df = cleaned_df
                                st.session_state.raw_df = df
                                
                                st.success("âœ… LÃ m sáº¡ch dá»¯ liá»‡u thÃ nh cÃ´ng!")
                                
                        except Exception as e:
                            st.error(f"âŒ Lá»—i khi thá»±c thi mÃ£ lÃ m sáº¡ch: {str(e)}")
                            
                            # Show debugging info
                            with st.expander("ğŸ› ThÃ´ng tin Debug"):
                                st.write("**Lá»—i chi tiáº¿t:**", str(e))
                                st.write("**MÃ£ Ä‘Ã£ thá»±c thi:**")
                                st.code(st.session_state.cleaning_code, language="python")
                
                # Show cleaned data preview
                if hasattr(st.session_state, 'cleaned_df'):
                    st.markdown("### âœ… Dá»¯ liá»‡u ÄÃ£ lÃ m sáº¡ch")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**ğŸ“Š TrÆ°á»›c lÃ m sáº¡ch:**")
                        st.write(f"KÃ­ch thÆ°á»›c: {df.shape}")
                        st.write(f"GiÃ¡ trá»‹ thiáº¿u: {df.isnull().sum().sum()}")
                        st.dataframe(df.head(3), use_container_width=True)
                    
                    with col2:
                        st.markdown("**âœ¨ Sau lÃ m sáº¡ch:**")
                        st.write(f"KÃ­ch thÆ°á»›c: {st.session_state.cleaned_df.shape}")
                        st.write(f"GiÃ¡ trá»‹ thiáº¿u: {st.session_state.cleaned_df.isnull().sum().sum()}")
                        st.dataframe(st.session_state.cleaned_df.head(3), use_container_width=True)
                    
                    # Download cleaned data
                    csv_data = st.session_state.cleaned_df.to_csv(index=False)
                    st.download_button(
                        label="ğŸ“¥ Táº£i xuá»‘ng Dá»¯ liá»‡u ÄÃ£ lÃ m sáº¡ch",
                        data=csv_data,
                        file_name="cleaned_dataset.csv",
                        mime="text/csv",
                        key="download_cleaned"
                    )

    with tab3:
        st.markdown("### ğŸ“ˆ PhÃ¢n tÃ­ch PhÃ¢n phá»‘i & Thá»‘ng kÃª")
        
        if hasattr(st.session_state, 'cleaned_df') and hasattr(st.session_state, 'raw_df'):
            show_skew_kurtosis(st.session_state.raw_df, st.session_state.cleaned_df)
        else:
            st.info("ğŸ”„ Vui lÃ²ng cháº¡y lÃ m sáº¡ch dá»¯ liá»‡u trong tab 'ğŸ§¼ LÃ m sáº¡ch ThÃ´ng minh' Ä‘á»ƒ xem phÃ¢n tÃ­ch nÃ y.")
            
            # Show basic distribution analysis for original data
            st.markdown("#### ğŸ“Š PhÃ¢n tÃ­ch PhÃ¢n phá»‘i CÆ¡ báº£n")
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                selected_col = st.selectbox("Chá»n cá»™t Ä‘á»ƒ phÃ¢n tÃ­ch:", numeric_cols)
                
                if selected_col:
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # Basic statistics
                        stats = df[selected_col].describe()
                        st.markdown("**ğŸ“Š Thá»‘ng kÃª mÃ´ táº£:**")
                        st.dataframe(stats)
                        
                        # Additional metrics
                        skewness = df[selected_col].skew()
                        kurtosis = df[selected_col].kurtosis()
                        
                        st.markdown(f"**â†—ï¸ Äá»™ lá»‡ch:** {skewness:.3f}")
                        st.markdown(f"**ğŸ“ˆ Äá»™ nhá»n:** {kurtosis:.3f}")
                        
                        # Interpretation
                        if abs(skewness) < 0.5:
                            skew_interp = "Gáº§n Ä‘á»‘i xá»©ng"
                        elif abs(skewness) < 1:
                            skew_interp = "HÆ¡i lá»‡ch"
                        else:
                            skew_interp = "Lá»‡ch máº¡nh"
                        
                        st.info(f"ğŸ” **PhÃ¢n tÃ­ch:** PhÃ¢n phá»‘i {skew_interp}")
                    
                    with col2:
                        # Distribution plot
                        plot_distribution(selected_col, df[selected_col])

if __name__ == "__main__":
    main()