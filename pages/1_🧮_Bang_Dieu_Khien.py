import streamlit as st
import pandas as pd
import os
from datetime import datetime
from src.utils import init_db, add_dataset, get_all_datasets, delete_dataset, rename_dataset, safe_read_csv

# Import UI components
from src.components.ui_components import (
    render_professional_header, render_metric_cards, render_feature_card,
    render_insight_card, create_data_quality_indicator, render_interactive_data_explorer,
    create_ai_recommendation_panel, render_animated_loading, PROFESSIONAL_CSS
)

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np

st.set_page_config(page_title="ğŸ“‚ Báº£ng Ä‘iá»u khiá»ƒn ChuyÃªn nghiá»‡p", layout="wide", page_icon="ğŸ“Š")

# Apply professional styling
st.markdown(PROFESSIONAL_CSS, unsafe_allow_html=True)

# Professional header
render_professional_header(
    "Báº£ng Ä‘iá»u khiá»ƒn PhÃ¢n tÃ­ch Äa Bá»™ dá»¯ liá»‡u",
    "Táº£i lÃªn, quáº£n lÃ½ vÃ  khÃ¡m phÃ¡ má»‘i quan há»‡ giá»¯a dá»¯ liá»‡u cá»§a báº¡n vá»›i thÃ´ng tin chi tiáº¿t Ä‘Æ°á»£c há»— trá»£ bá»Ÿi AI",
    "ğŸ“Š"
)

init_db()
if not os.path.exists('data/uploads'):
    os.makedirs('data/uploads')

# Enhanced sidebar for dataset upload
with st.sidebar:
    st.markdown("""
    <div style="text-align: center; padding: 1rem 0; border-bottom: 1px solid #e1e5e9; margin-bottom: 1rem;">
        <h3 style="color: #667eea; margin: 0;">ğŸ“‚ Quáº£n lÃ½ Bá»™ dá»¯ liá»‡u</h3>
        <small style="color: #666;">Táº£i lÃªn & Tá»• chá»©c</small>
    </div>
    """, unsafe_allow_html=True)
    
    # Enhanced multi-file upload
    st.markdown("#### ğŸ“¤ Táº£i lÃªn Bá»™ dá»¯ liá»‡u")
    uploaded_files = st.file_uploader(
        "Chá»n cÃ¡c file CSV (há»— trá»£ nhiá»u file)", 
        type=["csv"], 
        accept_multiple_files=True,
        help="ğŸ’¡ Táº£i lÃªn nhiá»u bá»™ dá»¯ liá»‡u Ä‘á»ƒ khÃ¡m phÃ¡ má»‘i quan há»‡ chÃ©o dá»¯ liá»‡u"
    )
    
    # Upload progress and processing
    if uploaded_files:
        upload_progress = st.progress(0)
        upload_status = st.empty()
        
        for i, uploaded_file in enumerate(uploaded_files):
            if f"uploaded_{uploaded_file.name}" not in st.session_state:
                upload_status.text(f"Äang xá»­ lÃ½ {uploaded_file.name}...")
                upload_progress.progress((i + 1) / len(uploaded_files))
                
                now = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{now}_{uploaded_file.name}"
                file_path = os.path.join('data', 'uploads', filename)
                
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                df = safe_read_csv(file_path)
                rows, cols = df.shape
                upload_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                add_dataset(filename, file_path, rows, cols, upload_time)
                
                st.session_state[f"uploaded_{uploaded_file.name}"] = True
                st.success(f"âœ… {uploaded_file.name}")
        
        upload_status.text("âœ… Táº¥t cáº£ file Ä‘Ã£ Ä‘Æ°á»£c táº£i lÃªn thÃ nh cÃ´ng!")
        upload_progress.progress(1.0)
        st.rerun()

# Load datasets
datasets = get_all_datasets()

if datasets:
    # Enhanced dashboard metrics with animations
    st.markdown("### ğŸ“Š Tá»•ng quan Báº£ng Ä‘iá»u khiá»ƒn")
    
    total_datasets = len(datasets)
    total_rows = sum([d[2] for d in datasets])
    total_cols = sum([d[3] for d in datasets])
    avg_size = total_rows / total_datasets if total_datasets > 0 else 0
    
    # Calculate additional metrics
    largest_dataset = max(datasets, key=lambda x: x[2])
    newest_dataset = max(datasets, key=lambda x: datetime.strptime(x[4], "%Y-%m-%d %H:%M:%S"))
    
    # Professional metric cards
    metrics = [
        {"title": "Tá»•ng Bá»™ dá»¯ liá»‡u", "value": f"{total_datasets}", "delta": "+3 tuáº§n nÃ y"},
        {"title": "Tá»•ng Báº£n ghi", "value": f"{total_rows:,}", "delta": f"+{total_rows//10:,} gáº§n Ä‘Ã¢y"},
        {"title": "TrÆ°á»ng Dá»¯ liá»‡u", "value": f"{total_cols}", "delta": None},
        {"title": "KÃ­ch thÆ°á»›c TB Bá»™ dá»¯ liá»‡u", "value": f"{avg_size:,.0f}", "delta": None}
    ]
    
    render_metric_cards(metrics)
    
    # Enhanced analytics dashboard
    st.markdown("### ğŸ“ˆ Báº£ng Ä‘iá»u khiá»ƒn PhÃ¢n tÃ­ch")
    
    # Create comprehensive dashboard visualization
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('KÃ­ch thÆ°á»›c Bá»™ dá»¯ liá»‡u (Báº£n ghi)', 'DÃ²ng thá»i gian Táº£i lÃªn', 'PhÃ¢n phá»‘i Cá»™t', 'Äiá»ƒm Máº­t Ä‘á»™ Dá»¯ liá»‡u'),
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # Dataset sizes bar chart with enhanced styling
    dataset_names = [d[1][:25] + "..." if len(d[1]) > 25 else d[1] for d in datasets]
    dataset_sizes = [d[2] for d in datasets]
    
    colors = ['#667eea', '#764ba2', '#56CCF2', '#2F80ED', '#FF6B6B'] * (len(datasets) // 5 + 1)
    
    fig.add_trace(
        go.Bar(
            x=dataset_names, 
            y=dataset_sizes, 
            name="Báº£n ghi",
            marker=dict(color=colors[:len(datasets)], opacity=0.8),
            text=[f"{size:,}" for size in dataset_sizes],
            textposition="outside"
        ),
        row=1, col=1
    )
    
    # Upload timeline with trend
    upload_dates = [datetime.strptime(d[4], "%Y-%m-%d %H:%M:%S").date() for d in datasets]
    upload_counts = {}
    for date in upload_dates:
        upload_counts[date] = upload_counts.get(date, 0) + 1
    
    sorted_dates = sorted(upload_counts.keys())
    cumulative_counts = []
    total = 0
    for date in sorted_dates:
        total += upload_counts[date]
        cumulative_counts.append(total)
    
    fig.add_trace(
        go.Scatter(
            x=sorted_dates, 
            y=cumulative_counts,
            mode='lines+markers', 
            name="Táº£i lÃªn TÃ­ch lÅ©y",
            line=dict(color='#764ba2', width=3),
            marker=dict(size=8, color='#667eea')
        ),
        row=1, col=2
    )
    
    # Column distribution histogram
    column_counts = [d[3] for d in datasets]
    fig.add_trace(
        go.Histogram(
            x=column_counts, 
            name="Cá»™t",
            marker=dict(color='#56CCF2', opacity=0.8),
            nbinsx=10
        ),
        row=2, col=1
    )
    
    # Data density (records per column) scatter
    density_scores = [d[2]/d[3] if d[3] > 0 else 0 for d in datasets]
    fig.add_trace(
        go.Scatter(
            x=dataset_names, 
            y=density_scores, 
            mode='markers',
            name="Äiá»ƒm Máº­t Ä‘á»™",
            marker=dict(
                size=[min(50, max(10, size//1000)) for size in dataset_sizes],
                color=density_scores,
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(title="Äiá»ƒm Máº­t Ä‘á»™")
            ),
            text=[f"{d[1]}<br>Máº­t Ä‘á»™: {score:.1f}" for d, score in zip(datasets, density_scores)],
            hovertemplate="<b>%{text}</b><br>Báº£n ghi/Cá»™t: %{y:.1f}<extra></extra>"
        ),
        row=2, col=2
    )
    
    # Update layout with professional styling
    fig.update_layout(
        height=600,
        showlegend=False,
        title=dict(
            text="ğŸ“Š Tá»•ng quan PhÃ¢n tÃ­ch Bá»™ dá»¯ liá»‡u",
            x=0.5,
            font=dict(size=18, color='#2c3e50')
        ),
        font=dict(family="Inter, sans-serif", size=12),
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgba(0,0,0,0)'
    )
    
    # Update individual subplot styling
    for i in range(1, 3):
        for j in range(1, 3):
            fig.update_xaxes(
                gridcolor='#e1e5e9',
                gridwidth=1,
                zeroline=False,
                row=i, col=j
            )
            fig.update_yaxes(
                gridcolor='#e1e5e9', 
                gridwidth=1,
                zeroline=False,
                row=i, col=j
            )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Dataset insights with AI analysis
    st.markdown("### ğŸ¤– ThÃ´ng tin Ä‘Æ°á»£c Há»— trá»£ bá»Ÿi AI")
    
    # Generate insights about the dataset collection
    insights = []
    
    if total_datasets >= 3:
        insights.append("ğŸ¯ **Sáºµn sÃ ng PhÃ¢n tÃ­ch Äa Bá»™ dá»¯ liá»‡u**: Báº¡n cÃ³ Ä‘á»§ bá»™ dá»¯ liá»‡u cho phÃ¢n tÃ­ch chÃ©o toÃ n diá»‡n")
    
    if max(dataset_sizes) > 10000:
        insights.append(f"ğŸ“ˆ **PhÃ¡t hiá»‡n Bá»™ dá»¯ liá»‡u Lá»›n**: {largest_dataset[1]} cÃ³ {largest_dataset[2]:,} báº£n ghi - phÃ¹ há»£p cho phÃ¢n tÃ­ch sÃ¢u")
    
    if len(set(d[3] for d in datasets)) > 3:
        insights.append("ğŸ”— **Cáº¥u trÃºc Dá»¯ liá»‡u Äa dáº¡ng**: Sá»‘ lÆ°á»£ng cá»™t khÃ¡c nhau cho tháº¥y cÃ¡c loáº¡i dá»¯ liá»‡u khÃ¡c nhau - tá»‘t cho phÃ¢n tÃ­ch toÃ n diá»‡n")
    
    upload_recency = (datetime.now() - datetime.strptime(newest_dataset[4], "%Y-%m-%d %H:%M:%S")).days
    if upload_recency < 7:
        insights.append(f"âš¡ **Dá»¯ liá»‡u Má»›i**: Táº£i lÃªn má»›i nháº¥t ({newest_dataset[1]}) chá»‰ cÃ¡ch Ä‘Ã¢y {upload_recency} ngÃ y")
    
    # Display insights in cards
    if insights:
        for insight in insights:
            render_insight_card(insight)
    else:
        render_insight_card("ğŸ“Š **Báº¯t Ä‘áº§u**: Táº£i lÃªn thÃªm bá»™ dá»¯ liá»‡u Ä‘á»ƒ má»Ÿ khÃ³a thÃ´ng tin AI nÃ¢ng cao vÃ  phÃ¢n tÃ­ch chÃ©o dá»¯ liá»‡u!")
    
    # Multi-dataset relationship analysis
    st.markdown("### ğŸ”— PhÃ¢n tÃ­ch ChÃ©o Bá»™ dá»¯ liá»‡u")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        selected_datasets = st.multiselect(
            "ğŸ¯ Chá»n bá»™ dá»¯ liá»‡u Ä‘á»ƒ phÃ¢n tÃ­ch má»‘i quan há»‡:",
            options=[f"{d[0]} - {d[1]}" for d in datasets],
            help="Chá»n 2+ bá»™ dá»¯ liá»‡u Ä‘á»ƒ khÃ¡m phÃ¡ má»‘i quan há»‡ vÃ  mÃ´ hÃ¬nh áº©n",
            placeholder="Chá»n nhiá»u bá»™ dá»¯ liá»‡u..."
        )
    
    with col2:
        analysis_type = st.selectbox(
            "Loáº¡i PhÃ¢n tÃ­ch:",
            ["TÆ°Æ¡ng Ä‘á»“ng Cá»™t", "TÆ°Æ¡ng quan Thá»‘ng kÃª", "Má»‘i quan há»‡ Ngá»¯ nghÄ©a", "PhÃ¢n tÃ­ch SÃ¢u AI"]
        )
    
    if len(selected_datasets) >= 2:
        if st.button("ğŸš€ PhÃ¢n tÃ­ch Má»‘i quan há»‡", type="primary", use_container_width=True):
            with st.spinner("ğŸ¤– Äang phÃ¢n tÃ­ch má»‘i quan há»‡ chÃ©o bá»™ dá»¯ liá»‡u..."):
                render_animated_loading("Äang khÃ¡m phÃ¡ mÃ´ hÃ¬nh qua cÃ¡c bá»™ dá»¯ liá»‡u cá»§a báº¡n...")
                
                # Store selection for cross-analysis page
                st.session_state.cross_analysis_datasets = selected_datasets
                st.session_state.cross_analysis_type = analysis_type
                
                st.success("âœ… PhÃ¢n tÃ­ch sáºµn sÃ ng! Nháº¥p bÃªn dÆ°á»›i Ä‘á»ƒ xem káº¿t quáº£ chi tiáº¿t.")
                
                if st.button("ğŸ“Š Xem PhÃ¢n tÃ­ch Chi tiáº¿t", type="secondary"):
                    st.switch_page("pages/7_ğŸ”—_Phan_Tich_Cheo_Du_Lieu.py")
    
    # Enhanced dataset management with professional cards
    st.markdown("### ğŸ—‚ï¸ Quáº£n lÃ½ Bá»™ dá»¯ liá»‡u")
    
    # Filter and search options
    col1, col2, col3 = st.columns(3)
    
    with col1:
        search_term = st.text_input("ğŸ” TÃ¬m kiáº¿m bá»™ dá»¯ liá»‡u:", placeholder="Lá»c theo tÃªn...")
    
    with col2:
        size_filter = st.selectbox("ğŸ“ Lá»c kÃ­ch thÆ°á»›c:", ["Táº¥t cáº£", "Nhá» (<1K)", "Trung bÃ¬nh (1K-10K)", "Lá»›n (>10K)"])
    
    with col3:
        sort_by = st.selectbox("ğŸ“Š Sáº¯p xáº¿p theo:", ["TÃªn", "NgÃ y Táº£i lÃªn", "KÃ­ch thÆ°á»›c", "Cá»™t"])
    
    # Apply filters
    filtered_datasets = datasets
    
    if search_term:
        filtered_datasets = [d for d in filtered_datasets if search_term.lower() in d[1].lower()]
    
    if size_filter != "Táº¥t cáº£":
        if size_filter == "Nhá» (<1K)":
            filtered_datasets = [d for d in filtered_datasets if d[2] < 1000]
        elif size_filter == "Trung bÃ¬nh (1K-10K)":
            filtered_datasets = [d for d in filtered_datasets if 1000 <= d[2] <= 10000]
        elif size_filter == "Lá»›n (>10K)":
            filtered_datasets = [d for d in filtered_datasets if d[2] > 10000]
    
    # Sort datasets
    if sort_by == "TÃªn":
        filtered_datasets.sort(key=lambda x: x[1])
    elif sort_by == "NgÃ y Táº£i lÃªn":
        filtered_datasets.sort(key=lambda x: datetime.strptime(x[4], "%Y-%m-%d %H:%M:%S"), reverse=True)
    elif sort_by == "KÃ­ch thÆ°á»›c":
        filtered_datasets.sort(key=lambda x: x[2], reverse=True)
    elif sort_by == "Cá»™t":
        filtered_datasets.sort(key=lambda x: x[3], reverse=True)
    
    # Display filtered datasets with enhanced cards
    for dataset in filtered_datasets:
        id_, name, rows, cols, uploaded, status = dataset
        
        with st.expander(f"ğŸ“ {name}", expanded=False):
            # Load dataset for preview and analysis
            file_path = os.path.join("data", "uploads", name)
            
            try:
                preview_df = safe_read_csv(file_path)
                
                # Dataset overview section
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.markdown("#### ğŸ“Š Xem trÆ°á»›c Bá»™ dá»¯ liá»‡u")
                    st.dataframe(preview_df.head(5), use_container_width=True)
                    
                    # Quick statistics
                    numeric_cols = preview_df.select_dtypes(include=['number']).columns
                    categorical_cols = preview_df.select_dtypes(include=['object']).columns
                    missing_values = preview_df.isnull().sum().sum()
                    
                    # Data quality indicator
                    quality_score = create_data_quality_indicator(preview_df)
                    
                with col2:
                    st.markdown("#### ğŸ¯ Thá»‘ng kÃª Nhanh")
                    
                    # Mini metrics for this dataset
                    dataset_metrics = [
                        {"title": "Cá»™t Sá»‘", "value": str(len(numeric_cols))},
                        {"title": "Cá»™t VÄƒn báº£n", "value": str(len(categorical_cols))},
                        {"title": "Thiáº¿u", "value": str(missing_values)},
                        {"title": "Cháº¥t lÆ°á»£ng", "value": f"{quality_score:.0%}"}
                    ]
                    
                    render_metric_cards(dataset_metrics)
                    
                    # Action buttons
                    st.markdown("#### âš¡ HÃ nh Ä‘á»™ng Nhanh")
                    
                    action_col1, action_col2 = st.columns(2)
                    
                    with action_col1:
                        if st.button("ğŸ” PhÃ¢n tÃ­ch", key=f"analyze_{id_}", use_container_width=True):
                            st.session_state.selected_dataset_id = id_
                            st.switch_page("pages/3_ğŸ“‚_Chi_Tiet_Bo_Du_Lieu.py")
                        
                        if st.button("ğŸ’¬ TrÃ² chuyá»‡n", key=f"chat_{id_}", use_container_width=True):
                            st.session_state.selected_dataset_id = id_
                            st.switch_page("main.py")
                    
                    with action_col2:
                        if st.button("ğŸ“Š Biá»ƒu Ä‘á»“", key=f"chart_{id_}", use_container_width=True):
                            st.session_state.selected_dataset_id = id_
                            st.switch_page("pages/6_ğŸ“ˆ_Bieu_Do_Thong_Minh.py")
                        
                        if st.button("ğŸ“‹ BÃ¡o cÃ¡o", key=f"report_{id_}", use_container_width=True):
                            st.session_state.selected_dataset_id = id_
                            st.switch_page("pages/5_ğŸ“‹_Bao_Cao_EDA.py")
                
                # AI recommendations for this specific dataset
                st.markdown("#### ğŸ¤– Khuyáº¿n nghá»‹ AI")
                create_ai_recommendation_panel(preview_df)
                
                # Management options
                st.markdown("#### âš™ï¸ TÃ¹y chá»n Quáº£n lÃ½")
                
                mgmt_col1, mgmt_col2, mgmt_col3 = st.columns(3)
                
                with mgmt_col1:
                    new_name = st.text_input(
                        "Äá»•i tÃªn bá»™ dá»¯ liá»‡u:", 
                        value=name, 
                        key=f"rename_input_{id_}",
                        help="Äáº·t tÃªn mÃ´ táº£ cho bá»™ dá»¯ liá»‡u cá»§a báº¡n"
                    )
                    
                    if st.button("âœ… Äá»•i tÃªn", key=f"rename_btn_{id_}"):
                        rename_dataset(id_, new_name)
                        st.success("âœ… ÄÃ£ Ä‘á»•i tÃªn bá»™ dá»¯ liá»‡u!")
                        st.rerun()
                
                with mgmt_col2:
                    if st.button("ğŸ“¥ Táº£i xuá»‘ng", key=f"download_{id_}", help="Táº£i xuá»‘ng bá»™ dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½"):
                        # Create download functionality
                        csv_data = preview_df.to_csv(index=False)
                        st.download_button(
                            label="ğŸ“¥ Táº£i xuá»‘ng CSV",
                            data=csv_data,
                            file_name=f"{name.split('_', 1)[-1] if '_' in name else name}",
                            mime="text/csv",
                            key=f"download_btn_{id_}"
                        )
                
                with mgmt_col3:
                    if st.button("ğŸ—‘ï¸ XÃ³a", key=f"del_{id_}", type="secondary", help="XÃ³a vÄ©nh viá»…n bá»™ dá»¯ liá»‡u nÃ y"):
                        # Confirmation dialog
                        if st.checkbox(f"XÃ¡c nháº­n xÃ³a {name}", key=f"confirm_{id_}"):
                            delete_dataset(id_)
                            st.warning(f"ğŸ—‘ï¸ ÄÃ£ xÃ³a bá»™ dá»¯ liá»‡u: {name}")
                            st.rerun()
                
                # Interactive data explorer for this dataset
                if st.checkbox("ğŸ” Má»Ÿ KhÃ¡m phÃ¡ Dá»¯ liá»‡u", key=f"explorer_{id_}"):
                    render_interactive_data_explorer(preview_df)
                
            except Exception as e:
                st.error(f"âŒ KhÃ´ng thá»ƒ táº£i bá»™ dá»¯ liá»‡u: {e}")
                
                # Still show management options even if preview fails
                mgmt_col1, mgmt_col2 = st.columns(2)
                
                with mgmt_col1:
                    new_name = st.text_input("Äá»•i tÃªn:", value=name, key=f"rename_error_{id_}")
                    if st.button("âœ… Äá»•i tÃªn", key=f"rename_error_btn_{id_}"):
                        rename_dataset(id_, new_name)
                        st.rerun()
                
                with mgmt_col2:
                    if st.button("ğŸ—‘ï¸ XÃ³a", key=f"del_error_{id_}", type="secondary"):
                        delete_dataset(id_)
                        st.rerun()

else:
    # Welcome screen for new users
    st.markdown("### ğŸ‘‹ ChÃ o má»«ng Ä‘áº¿n vá»›i VizGenie-GPT ChuyÃªn nghiá»‡p!")
    
    # Feature showcase
    col1, col2, col3 = st.columns(3)
    
    with col1:
        render_feature_card(
            "ğŸ¤– PhÃ¢n tÃ­ch Ä‘Æ°á»£c Há»— trá»£ bá»Ÿi AI",
            "Äáº·t cÃ¢u há»i phá»©c táº¡p vá» dá»¯ liá»‡u cá»§a báº¡n báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  nháº­n Ä‘Æ°á»£c thÃ´ng tin thÃ´ng minh vá»›i trá»±c quan hÃ³a Ä‘áº¹p máº¯t.",
            "ğŸ¤–"
        )
    
    with col2:
        render_feature_card(
            "ğŸ”— KhÃ¡m phÃ¡ ChÃ©o Bá»™ dá»¯ liá»‡u",
            "Táº£i lÃªn nhiá»u bá»™ dá»¯ liá»‡u vÃ  khÃ¡m phÃ¡ má»‘i quan há»‡ áº©n vÃ  mÃ´ hÃ¬nh qua cÃ¡c nguá»“n dá»¯ liá»‡u cá»§a báº¡n.",
            "ğŸ”—"
        )
    
    with col3:
        render_feature_card(
            "ğŸ“Š Biá»ƒu Ä‘á»“ ChuyÃªn nghiá»‡p",
            "Táº¡o ra nhá»¯ng biá»ƒu Ä‘á»“ tuyá»‡t Ä‘áº¹p, sáºµn sÃ ng xuáº¥t báº£n vá»›i báº£ng mÃ u thÃ´ng minh vÃ  tÃ­nh nÄƒng tÆ°Æ¡ng tÃ¡c.",
            "ğŸ“Š"
        )
    
    # Getting started guide
    st.markdown("### ğŸš€ Báº¯t Ä‘áº§u")
    
    render_insight_card("""
    **ğŸ“‹ HÆ°á»›ng dáº«n Nhanh:**
    
    1. **ğŸ“¤ Táº£i lÃªn Dá»¯ liá»‡u**: Sá»­ dá»¥ng thanh bÃªn Ä‘á»ƒ táº£i lÃªn má»™t hoáº·c nhiá»u file CSV
    2. **ğŸ¤– Äáº·t CÃ¢u há»i**: TrÃ² chuyá»‡n vá»›i dá»¯ liá»‡u cá»§a báº¡n báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn
    3. **ğŸ“Š Táº¡o Trá»±c quan hÃ³a**: Táº¡o biá»ƒu Ä‘á»“ chuyÃªn nghiá»‡p tá»± Ä‘á»™ng
    4. **ğŸ”— TÃ¬m Má»‘i quan há»‡**: KhÃ¡m phÃ¡ mÃ´ hÃ¬nh qua nhiá»u bá»™ dá»¯ liá»‡u
    5. **ğŸ“„ Xuáº¥t BÃ¡o cÃ¡o**: Táº¡o bÃ¡o cÃ¡o PDF toÃ n diá»‡n cho cÃ¡c bÃªn liÃªn quan
    
    **ğŸ’¡ Máº¹o ChuyÃªn nghiá»‡p:**
    - Táº£i lÃªn cÃ¡c bá»™ dá»¯ liá»‡u liÃªn quan cÃ¹ng nhau Ä‘á»ƒ phÃ¢n tÃ­ch chÃ©o tá»‘t hÆ¡n
    - Sá»­ dá»¥ng tÃªn mÃ´ táº£ cho bá»™ dá»¯ liá»‡u cá»§a báº¡n
    - Äáº·t cÃ¢u há»i cá»¥ thá»ƒ Ä‘á»ƒ cÃ³ pháº£n há»“i AI tá»‘t hÆ¡n
    - Thá»­ cÃ¡c loáº¡i biá»ƒu Ä‘á»“ vÃ  báº£ng mÃ u khÃ¡c nhau
    """)
    
    # Sample data offer
    st.markdown("### ğŸ“š Thá»­ vá»›i Dá»¯ liá»‡u Máº«u")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ“Š Táº£i Dá»¯ liá»‡u BÃ¡n hÃ ng Máº«u", type="primary", use_container_width=True):
            # Create sample sales dataset
            np.random.seed(42)
            sample_sales = pd.DataFrame({
                'date': pd.date_range('2023-01-01', periods=365, freq='D'),
                'revenue': np.random.normal(10000, 2000, 365),
                'customers': np.random.poisson(50, 365),
                'region': np.random.choice(['North', 'South', 'East', 'West'], 365),
                'product_category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home'], 365)
            })
            
            # Save sample data
            sample_path = os.path.join('data', 'uploads', 'sample_sales_data.csv')
            sample_sales.to_csv(sample_path, index=False)
            
            # Add to database
            upload_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            add_dataset('sample_sales_data.csv', sample_path, len(sample_sales), len(sample_sales.columns), upload_time)
            
            st.success("âœ… ÄÃ£ táº£i dá»¯ liá»‡u bÃ¡n hÃ ng máº«u!")
            st.rerun()
    
    with col2:
        if st.button("ğŸ‘¥ Táº£i Dá»¯ liá»‡u KhÃ¡ch hÃ ng Máº«u", type="secondary", use_container_width=True):
            # Create sample customer dataset
            np.random.seed(24)
            sample_customers = pd.DataFrame({
                'customer_id': range(1, 501),
                'age': np.random.randint(18, 70, 500),
                'gender': np.random.choice(['Male', 'Female'], 500),
                'income': np.random.normal(50000, 15000, 500),
                'satisfaction_score': np.random.randint(1, 11, 500),
                'region': np.random.choice(['North', 'South', 'East', 'West'], 500)
            })
            
            # Save sample data
            sample_path = os.path.join('data', 'uploads', 'sample_customer_data.csv')
            sample_customers.to_csv(sample_path, index=False)
            
            # Add to database
            upload_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            add_dataset('sample_customer_data.csv', sample_path, len(sample_customers), len(sample_customers.columns), upload_time)
            
            st.success("âœ… ÄÃ£ táº£i dá»¯ liá»‡u khÃ¡ch hÃ ng máº«u!")
            st.rerun()

# Enhanced sidebar with navigation and tips
with st.sidebar:
    if datasets:
        st.markdown("---")
        st.markdown("### ğŸ¯ Thá»‘ng kÃª Nhanh")
        
        # Overall statistics
        total_size_mb = sum(os.path.getsize(os.path.join("data", "uploads", d[1])) for d in datasets if os.path.exists(os.path.join("data", "uploads", d[1]))) / (1024 * 1024)
        
        quick_stats = [
            {"title": "Bá»™ dá»¯ liá»‡u", "value": str(len(datasets)), "delta": None},
            {"title": "Tá»•ng KÃ­ch thÆ°á»›c", "value": f"{total_size_mb:.1f}MB", "delta": None},
            {"title": "Lá»›n nháº¥t", "value": f"{max(d[2] for d in datasets):,}", "delta": None}
        ]
        
        render_metric_cards(quick_stats)
    
    st.markdown("---")
    st.markdown("### ğŸ”— Äiá»u hÆ°á»›ng")
    
    nav_links = [
        ("ğŸ’¬ TrÃ² chuyá»‡n AI", "main.py"),
        ("ğŸ“Š Chi tiáº¿t Bá»™ dá»¯ liá»‡u", "pages/3_ğŸ“‚_Chi_Tiet_Bo_Du_Lieu.py"),
        ("ğŸ“ˆ Biá»ƒu Ä‘á»“ ThÃ´ng minh", "pages/6_ğŸ“ˆ_Bieu_Do_Thong_Minh.py"),
        ("ğŸ”— PhÃ¢n tÃ­ch ChÃ©o", "pages/7_ğŸ”—_Phan_Tich_Cheo_Du_Lieu.py"),
        ("ğŸ“‹ Lá»‹ch sá»­ Biá»ƒu Ä‘á»“", "pages/4_ğŸ“Š_Lich_Su_Bieu_Do.py"),
        ("ğŸ“„ BÃ¡o cÃ¡o EDA", "pages/5_ğŸ“‹_Bao_Cao_EDA.py"),
        ("ğŸ“– Vá» dá»± Ã¡n", "pages/ğŸ“–_Ve_Du_An.py")
    ]
    
    for label, page in nav_links:
        if st.button(label, key=f"nav_{label}", use_container_width=True):
            st.switch_page(page)
    
    st.markdown("---")
    st.markdown("### ğŸ’¡ Máº¹o & Thá»§ thuáº­t")
    
    tips = [
        "ğŸ¯ **PhÃ¢n tÃ­ch Tá»‘t hÆ¡n**: Táº£i lÃªn cÃ¡c bá»™ dá»¯ liá»‡u liÃªn quan cÃ¹ng nhau",
        "ğŸ¨ **Háº¥p dáº«n Trá»±c quan**: Thá»­ cÃ¡c báº£ng mÃ u khÃ¡c nhau trong biá»ƒu Ä‘á»“", 
        "ğŸ¤– **CÃ¢u há»i ThÃ´ng minh**: Cá»¥ thá»ƒ vá» nhá»¯ng gÃ¬ báº¡n muá»‘n khÃ¡m phÃ¡",
        "ğŸ“Š **PhÃ¢n tÃ­ch ChÃ©o**: TÃ¬m kiáº¿m mÃ´ hÃ¬nh qua cÃ¡c bá»™ dá»¯ liá»‡u",
        "ğŸ“‹ **LÆ°u CÃ´ng viá»‡c**: Sá»­ dá»¥ng lá»‹ch sá»­ biá»ƒu Ä‘á»“ vÃ  quáº£n lÃ½ phiÃªn"
    ]
    
    for tip in tips:
        st.markdown(f"- {tip}")

# Footer with system info
st.markdown("---")
col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("**ğŸ§  VizGenie-GPT ChuyÃªn nghiá»‡p**")
    st.caption("Ná»n táº£ng PhÃ¢n tÃ­ch Äa Bá»™ dá»¯ liá»‡u NÃ¢ng cao")

with col2:
    if datasets:
        st.markdown(f"**ğŸ“Š Tráº¡ng thÃ¡i Há»‡ thá»‘ng**")
        st.caption(f"{len(datasets)} bá»™ dá»¯ liá»‡u â€¢ {sum(d[2] for d in datasets):,} tá»•ng báº£n ghi")
    else:
        st.markdown("**ğŸš€ Sáºµn sÃ ng Báº¯t Ä‘áº§u**")
        st.caption("Táº£i lÃªn bá»™ dá»¯ liá»‡u Ä‘áº§u tiÃªn Ä‘á»ƒ báº¯t Ä‘áº§u")

with col3:
    st.markdown("**ğŸ‘¨â€ğŸ’» Delay Group**")
    st.caption("LÃ m cho phÃ¢n tÃ­ch dá»¯ liá»‡u cÃ³ thá»ƒ tiáº¿p cáº­n vá»›i má»i ngÆ°á»i")