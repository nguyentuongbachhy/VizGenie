import streamlit as st
import pandas as pd
import os
from datetime import datetime
from src.utils import init_db, add_dataset, get_all_datasets, delete_dataset, rename_dataset, safe_read_csv

# Import UI components
from src.components.ui_components import (
    render_professional_header, render_metric_cards, render_feature_card,
    render_insight_card, create_data_quality_indicator, render_interactive_data_explorer,
    create_ai_recommendation_panel, render_animated_loading, PROFESSIONAL_CSS
)

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
import time

st.set_page_config(page_title="üìÇ B·∫£ng ƒëi·ªÅu khi·ªÉn Chuy√™n nghi·ªáp", layout="wide", page_icon="üìä")

# Apply professional styling
st.markdown(PROFESSIONAL_CSS, unsafe_allow_html=True)

# Professional header
render_professional_header(
    "B·∫£ng ƒëi·ªÅu khi·ªÉn Ph√¢n t√≠ch ƒêa B·ªô d·ªØ li·ªáu",
    "T·∫£i l√™n, qu·∫£n l√Ω v√† kh√°m ph√° m·ªëi quan h·ªá gi·ªØa d·ªØ li·ªáu c·ªßa b·∫°n v·ªõi th√¥ng tin chi ti·∫øt ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi AI",
    "üìä"
)

init_db()
if not os.path.exists('data/uploads'):
    os.makedirs('data/uploads')

def show_loading_animation(text="ƒêang x·ª≠ l√Ω..."):
    """Show loading animation"""
    return st.markdown(f"""
    <div style="display: flex; align-items: center; justify-content: center; padding: 2rem; background: #f8f9fa; border-radius: 10px; margin: 1rem 0;">
        <div style="border: 4px solid #f3f3f3; border-top: 4px solid #667eea; border-radius: 50%; width: 40px; height: 40px; animation: spin 1s linear infinite; margin-right: 1rem;"></div>
        <span style="color: #667eea; font-weight: 500;">{text}</span>
    </div>
    <style>
        @keyframes spin {{
            0% {{ transform: rotate(0deg); }}
            100% {{ transform: rotate(360deg); }}
        }}
    </style>
    """, unsafe_allow_html=True)

def create_enhanced_analytics_dashboard(datasets):
    """Create comprehensive analytics dashboard with proper spacing and fallbacks"""
    try:
        # Create subplot with better spacing
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=[
                'üìä K√≠ch th∆∞·ªõc B·ªô d·ªØ li·ªáu (B·∫£n ghi)', 
                'üìÖ D√≤ng th·ªùi gian T·∫£i l√™n', 
                'üìã Ph√¢n ph·ªëi S·ªë c·ªôt', 
                'üíé ƒêi·ªÉm M·∫≠t ƒë·ªô D·ªØ li·ªáu'
            ],
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]],
            vertical_spacing=0.15,  # Increased spacing
            horizontal_spacing=0.12
        )
        
        # Prepare data with validation
        dataset_names = []
        dataset_sizes = []
        dataset_cols = []
        upload_dates = []
        
        for d in datasets:
            try:
                name = d[1][:20] + "..." if len(d[1]) > 20 else d[1]
                dataset_names.append(name)
                dataset_sizes.append(max(0, d[2]))  # Ensure non-negative
                dataset_cols.append(max(1, d[3]))   # Ensure at least 1
                upload_dates.append(datetime.strptime(d[4], "%Y-%m-%d %H:%M:%S").date())
            except Exception as e:
                continue  # Skip invalid entries
        
        if not dataset_names:
            # Fallback empty chart
            fig.add_annotation(text="Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ hi·ªÉn th·ªã", 
                             xref="paper", yref="paper", x=0.5, y=0.5,
                             showarrow=False, font=dict(size=16))
            return fig
        
        # Chart 1: Dataset sizes with better spacing
        colors = ['#667eea', '#764ba2', '#56CCF2', '#2F80ED', '#FF6B6B', '#FF8E53', '#4ECDC4', '#45B7D1'] * 10
        
        # Limit to top 10 datasets for better readability
        top_indices = sorted(range(len(dataset_sizes)), key=lambda i: dataset_sizes[i], reverse=True)[:10]
        top_names = [dataset_names[i] for i in top_indices]
        top_sizes = [dataset_sizes[i] for i in top_indices]
        
        fig.add_trace(
            go.Bar(
                x=top_names, 
                y=top_sizes, 
                name="B·∫£n ghi",
                marker=dict(
                    color=colors[:len(top_names)], 
                    opacity=0.8,
                    line=dict(color='rgba(0,0,0,0.1)', width=1)
                ),
                text=[f"{size:,}" for size in top_sizes],
                textposition="outside",
                textfont=dict(size=10),
                hovertemplate="<b>%{x}</b><br>B·∫£n ghi: %{y:,}<extra></extra>"
            ),
            row=1, col=1
        )
        
        # Update x-axis for better readability
        fig.update_xaxes(
            tickangle=-45, 
            tickfont=dict(size=9),
            row=1, col=1
        )
        
        # Chart 2: Upload timeline with trend
        upload_counts = {}
        for date in upload_dates:
            upload_counts[date] = upload_counts.get(date, 0) + 1
        
        if upload_counts:
            sorted_dates = sorted(upload_counts.keys())
            daily_counts = [upload_counts[date] for date in sorted_dates]
            
            # Calculate cumulative
            cumulative_counts = []
            total = 0
            for count in daily_counts:
                total += count
                cumulative_counts.append(total)
            
            fig.add_trace(
                go.Scatter(
                    x=sorted_dates, 
                    y=cumulative_counts,
                    mode='lines+markers', 
                    name="T√≠ch l≈©y",
                    line=dict(color='#764ba2', width=3, shape='spline'),
                    marker=dict(size=8, color='#667eea', symbol='circle'),
                    hovertemplate="<b>%{x}</b><br>T·ªïng c·ªông: %{y}<extra></extra>",
                    fill='tonexty' if len(sorted_dates) > 1 else None,
                    fillcolor='rgba(102, 126, 234, 0.1)'
                ),
                row=1, col=2
            )
        
        # Chart 3: Column distribution with better bins
        if dataset_cols:
            fig.add_trace(
                go.Histogram(
                    x=dataset_cols, 
                    name="S·ªë c·ªôt",
                    marker=dict(
                        color='#56CCF2', 
                        opacity=0.8,
                        line=dict(color='rgba(0,0,0,0.2)', width=1)
                    ),
                    nbinsx=min(10, max(5, len(set(dataset_cols)))),
                    hovertemplate="S·ªë c·ªôt: %{x}<br>S·ªë l∆∞·ª£ng: %{y}<extra></extra>"
                ),
                row=2, col=1
            )
        
        # Chart 4: Data density scatter with better visualization
        density_scores = []
        for i in range(len(dataset_sizes)):
            if dataset_cols[i] > 0:
                density_scores.append(dataset_sizes[i] / dataset_cols[i])
            else:
                density_scores.append(0)
        
        if density_scores:
            # Create size array for bubble chart
            bubble_sizes = [min(50, max(15, size/1000)) for size in dataset_sizes]
            
            fig.add_trace(
                go.Scatter(
                    x=list(range(len(dataset_names))), 
                    y=density_scores, 
                    mode='markers',
                    name="M·∫≠t ƒë·ªô",
                    marker=dict(
                        size=bubble_sizes,
                        color=density_scores,
                        colorscale='Viridis',
                        showscale=True,
                        colorbar=dict(
                            title="M·∫≠t ƒë·ªô<br>(b·∫£n ghi/c·ªôt)",
                            titleside="right",
                            tickmode="linear",
                            tick0=0,
                            dtick=max(1, max(density_scores)//5) if density_scores else 1
                        ),
                        line=dict(color='rgba(0,0,0,0.2)', width=1),
                        opacity=0.8
                    ),
                    text=[f"{name}<br>M·∫≠t ƒë·ªô: {score:.1f}<br>K√≠ch th∆∞·ªõc: {size:,}" 
                          for name, score, size in zip(dataset_names, density_scores, dataset_sizes)],
                    hovertemplate="<b>%{text}</b><extra></extra>",
                    customdata=dataset_names
                ),
                row=2, col=2
            )
            
            # Update x-axis to show dataset names
            fig.update_xaxes(
                tickvals=list(range(len(dataset_names))),
                ticktext=[name[:10] + "..." if len(name) > 10 else name for name in dataset_names],
                tickangle=-45,
                tickfont=dict(size=9),
                row=2, col=2
            )
        
        # Update layout with professional styling and better spacing
        fig.update_layout(
            height=700,  # Increased height
            showlegend=False,
            title=dict(
                text="üìä T·ªïng quan Ph√¢n t√≠ch B·ªô d·ªØ li·ªáu",
                x=0.5,
                font=dict(size=20, color='#2c3e50', family="Inter, sans-serif")
            ),
            font=dict(family="Inter, sans-serif", size=11),
            plot_bgcolor='rgba(248,249,250,0.8)',
            paper_bgcolor='rgba(0,0,0,0)',
            margin=dict(t=80, l=60, r=60, b=80)
        )
        
        # Update individual subplot styling with better spacing
        for i in range(1, 3):
            for j in range(1, 3):
                fig.update_xaxes(
                    gridcolor='rgba(225,229,233,0.8)',
                    gridwidth=1,
                    zeroline=False,
                    showline=True,
                    linecolor='rgba(225,229,233,0.8)',
                    row=i, col=j
                )
                fig.update_yaxes(
                    gridcolor='rgba(225,229,233,0.8)', 
                    gridwidth=1,
                    zeroline=False,
                    showline=True,
                    linecolor='rgba(225,229,233,0.8)',
                    row=i, col=j
                )
        
        return fig
        
    except Exception as e:
        # Fallback chart in case of any error
        fallback_fig = go.Figure()
        fallback_fig.add_annotation(
            text=f"L·ªói t·∫°o bi·ªÉu ƒë·ªì: {str(e)}<br>Vui l√≤ng th·ª≠ l·∫°i sau",
            xref="paper", yref="paper", x=0.5, y=0.5,
            showarrow=False, font=dict(size=14, color="red")
        )
        fallback_fig.update_layout(
            height=400,
            title="Dashboard Analytics",
            template="plotly_white"
        )
        return fallback_fig

def perform_ai_deep_analysis(datasets):
    """Perform AI deep analysis with loading indicators and error handling"""
    try:
        # Loading indicator
        loading_placeholder = st.empty()
        loading_placeholder.markdown(show_loading_animation("ü§ñ AI ƒëang ph√¢n t√≠ch s√¢u d·ªØ li·ªáu c·ªßa b·∫°n..."))
        
        # Simulate AI processing time
        time.sleep(2)
        
        # Analyze datasets
        total_records = sum([d[2] for d in datasets])
        total_fields = sum([d[3] for d in datasets])
        avg_size = total_records / len(datasets) if datasets else 0
        largest_dataset = max(datasets, key=lambda x: x[2]) if datasets else None
        
        # Generate insights
        insights = []
        
        if len(datasets) >= 3:
            insights.append({
                "icon": "üéØ",
                "title": "C∆° s·ªü D·ªØ li·ªáu Phong ph√∫",
                "description": f"B·∫°n c√≥ {len(datasets)} b·ªô d·ªØ li·ªáu v·ªõi t·ªïng c·ªông {total_records:,} b·∫£n ghi. ƒê√¢y l√† c∆° s·ªü tuy·ªát v·ªùi cho ph√¢n t√≠ch ƒëa chi·ªÅu v√† kh√°m ph√° m·ªëi quan h·ªá ch√©o.",
                "confidence": 0.9,
                "action": "Th·ª≠ ph√¢n t√≠ch ch√©o d·ªØ li·ªáu"
            })
        
        if largest_dataset and largest_dataset[2] > 10000:
            insights.append({
                "icon": "üìà",
                "title": "Ti·ªÅm nƒÉng Big Data",
                "description": f"B·ªô d·ªØ li·ªáu '{largest_dataset[1]}' c√≥ {largest_dataset[2]:,} b·∫£n ghi. K√≠ch th∆∞·ªõc n√†y r·∫•t ph√π h·ª£p cho machine learning v√† ph√¢n t√≠ch xu h∆∞·ªõng ph·ª©c t·∫°p.",
                "confidence": 0.85,
                "action": "√Åp d·ª•ng thu·∫≠t to√°n ML"
            })
        
        if total_fields > 50:
            insights.append({
                "icon": "üîó",
                "title": "D·ªØ li·ªáu ƒêa chi·ªÅu",
                "description": f"V·ªõi {total_fields} tr∆∞·ªùng d·ªØ li·ªáu t·ªïng c·ªông, b·∫°n c√≥ th·ªÉ th·ª±c hi·ªán ph√¢n t√≠ch t∆∞∆°ng quan s√¢u v√† ph√°t hi·ªán c√°c m·ªëi quan h·ªá ·∫©n gi·ªØa c√°c bi·∫øn.",
                "confidence": 0.8,
                "action": "T·∫°o ma tr·∫≠n t∆∞∆°ng quan"
            })
        
        # Data quality assessment
        quality_scores = []
        for dataset in datasets:
            try:
                df = safe_read_csv(dataset[2])
                missing_pct = (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100
                quality_score = max(0, 100 - missing_pct)
                quality_scores.append(quality_score)
            except:
                quality_scores.append(75)  # Default score if can't read
        
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 75
        
        if avg_quality > 85:
            insights.append({
                "icon": "‚úÖ",
                "title": "Ch·∫•t l∆∞·ª£ng D·ªØ li·ªáu Cao",
                "description": f"Ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu trung b√¨nh l√† {avg_quality:.1f}%. D·ªØ li·ªáu s·∫°ch n√†y s·∫µn s√†ng cho c√°c ph√¢n t√≠ch n√¢ng cao v√† m√¥ h√¨nh h√≥a.",
                "confidence": 0.9,
                "action": "B·∫Øt ƒë·∫ßu ph√¢n t√≠ch n√¢ng cao"
            })
        elif avg_quality < 60:
            insights.append({
                "icon": "‚ö†Ô∏è",
                "title": "C·∫ßn L√†m s·∫°ch D·ªØ li·ªáu",
                "description": f"Ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu trung b√¨nh ch·ªâ {avg_quality:.1f}%. N√™n l√†m s·∫°ch d·ªØ li·ªáu tr∆∞·ªõc khi ph√¢n t√≠ch ƒë·ªÉ c√≥ k·∫øt qu·∫£ ch√≠nh x√°c h∆°n.",
                "confidence": 0.85,
                "action": "ƒêi ƒë·∫øn Chi ti·∫øt B·ªô d·ªØ li·ªáu"
            })
        
        # Time-based analysis
        recent_uploads = [d for d in datasets 
                         if (datetime.now() - datetime.strptime(d[4], "%Y-%m-%d %H:%M:%S")).days < 7]
        
        if recent_uploads:
            insights.append({
                "icon": "‚ö°",
                "title": "D·ªØ li·ªáu M·ªõi",
                "description": f"{len(recent_uploads)} b·ªô d·ªØ li·ªáu ƒë∆∞·ª£c t·∫£i l√™n trong 7 ng√†y qua. D·ªØ li·ªáu m·ªõi th∆∞·ªùng ph·∫£n √°nh xu h∆∞·ªõng hi·ªán t·∫°i v√† c√≥ gi√° tr·ªã ph√¢n t√≠ch cao.",
                "confidence": 0.75,
                "action": "Ph√¢n t√≠ch xu h∆∞·ªõng m·ªõi nh·∫•t"
            })
        
        # Clear loading
        loading_placeholder.empty()
        
        return insights, avg_quality
        
    except Exception as e:
        st.error(f"‚ùå L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch AI: {str(e)}")
        return [], 75

# Enhanced sidebar for dataset upload
with st.sidebar:
    st.markdown("""
    <div style="text-align: center; padding: 1rem 0; border-bottom: 1px solid #e1e5e9; margin-bottom: 1rem;">
        <h3 style="color: #667eea; margin: 0;">üìÇ Qu·∫£n l√Ω B·ªô d·ªØ li·ªáu</h3>
        <small style="color: #666;">T·∫£i l√™n & T·ªï ch·ª©c</small>
    </div>
    """, unsafe_allow_html=True)
    
    # Enhanced multi-file upload with progress
    st.markdown("#### üì§ T·∫£i l√™n B·ªô d·ªØ li·ªáu")
    uploaded_files = st.file_uploader(
        "Ch·ªçn c√°c file CSV (h·ªó tr·ª£ nhi·ªÅu file)", 
        type=["csv"], 
        accept_multiple_files=True,
        help="üí° T·∫£i l√™n nhi·ªÅu b·ªô d·ªØ li·ªáu ƒë·ªÉ kh√°m ph√° m·ªëi quan h·ªá ch√©o d·ªØ li·ªáu"
    )
    
    # Upload processing with better feedback
    if uploaded_files:
        upload_progress = st.progress(0)
        upload_status = st.empty()
        success_count = 0
        
        for i, uploaded_file in enumerate(uploaded_files):
            cache_key = f"uploaded_{uploaded_file.name}_{uploaded_file.size}"
            
            if cache_key not in st.session_state:
                upload_status.text(f"üîÑ ƒêang x·ª≠ l√Ω {uploaded_file.name}...")
                
                try:
                    # Validate file
                    if uploaded_file.size > 50 * 1024 * 1024:  # 50MB limit
                        st.error(f"‚ùå File {uploaded_file.name} qu√° l·ªõn (>50MB)")
                        continue
                    
                    # Process file
                    now = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"{now}_{uploaded_file.name}"
                    file_path = os.path.join('data', 'uploads', filename)
                    
                    with open(file_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())
                    
                    # Read and validate CSV
                    df = safe_read_csv(file_path)
                    
                    if df.empty:
                        st.error(f"‚ùå File {uploaded_file.name} tr·ªëng")
                        os.remove(file_path)
                        continue
                    
                    rows, cols = df.shape
                    upload_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    
                    add_dataset(filename, file_path, rows, cols, upload_time)
                    
                    st.session_state[cache_key] = True
                    success_count += 1
                    
                    upload_status.success(f"‚úÖ {uploaded_file.name} ({rows:,} h√†ng, {cols} c·ªôt)")
                    
                except Exception as e:
                    st.error(f"‚ùå L·ªói x·ª≠ l√Ω {uploaded_file.name}: {str(e)}")
                    
            else:
                success_count += 1
            
            upload_progress.progress((i + 1) / len(uploaded_files))
        
        if success_count == len(uploaded_files):
            upload_status.success(f"üéâ ƒê√£ t·∫£i l√™n th√†nh c√¥ng {success_count}/{len(uploaded_files)} file!")
            time.sleep(1)
            st.rerun()

# Load datasets
datasets = get_all_datasets()

if datasets:
    # Enhanced dashboard metrics
    st.markdown("### üìä T·ªïng quan B·∫£ng ƒëi·ªÅu khi·ªÉn")
    
    # Calculate comprehensive metrics
    total_datasets = len(datasets)
    total_rows = sum([d[2] for d in datasets])
    total_cols = sum([d[3] for d in datasets])
    avg_size = total_rows / total_datasets if total_datasets > 0 else 0
    
    # Additional metrics
    largest_dataset = max(datasets, key=lambda x: x[2]) if datasets else None
    newest_dataset = max(datasets, key=lambda x: datetime.strptime(x[4], "%Y-%m-%d %H:%M:%S")) if datasets else None
    
    # Calculate storage size
    total_size_mb = 0
    for d in datasets:
        try:
            if os.path.exists(os.path.join("data", "uploads", d[1])):
                total_size_mb += os.path.getsize(os.path.join("data", "uploads", d[1])) / (1024 * 1024)
        except:
            continue
    
    # Professional metric cards with enhanced data
    metrics = [
        {"title": "T·ªïng B·ªô d·ªØ li·ªáu", "value": f"{total_datasets}", "delta": "+3 tu·∫ßn n√†y"},
        {"title": "T·ªïng B·∫£n ghi", "value": f"{total_rows:,}", "delta": f"+{total_rows//10:,} g·∫ßn ƒë√¢y"},
        {"title": "Tr∆∞·ªùng D·ªØ li·ªáu", "value": f"{total_cols}", "delta": None},
        {"title": "Dung l∆∞·ª£ng", "value": f"{total_size_mb:.1f}MB", "delta": None}
    ]
    
    render_metric_cards(metrics)
    
    # Enhanced analytics dashboard with loading and error handling
    st.markdown("### üìà B·∫£ng ƒëi·ªÅu khi·ªÉn Ph√¢n t√≠ch")
    
    dashboard_container = st.container()
    
    with dashboard_container:
        try:
            # Show loading for dashboard creation
            with st.spinner("üìä ƒêang t·∫°o dashboard ph√¢n t√≠ch..."):
                fig = create_enhanced_analytics_dashboard(datasets)
            
            # Display dashboard
            st.plotly_chart(fig, use_container_width=True, key="main_dashboard")
            
        except Exception as e:
            st.error(f"‚ùå Kh√¥ng th·ªÉ t·∫°o dashboard: {str(e)}")
            st.info("üí° Vui l√≤ng th·ª≠ t·∫£i l·∫°i trang ho·∫∑c ki·ªÉm tra d·ªØ li·ªáu")
    
    # AI Deep Analysis with enhanced loading
    st.markdown("### ü§ñ Ph√¢n t√≠ch S√¢u AI")
    
    if st.button("üöÄ B·∫Øt ƒë·∫ßu Ph√¢n t√≠ch AI", type="primary"):
        ai_insights, data_quality = perform_ai_deep_analysis(datasets)
        
        if ai_insights:
            st.markdown("#### üí° Insights ƒë∆∞·ª£c AI Ph√°t hi·ªán")
            
            # Display insights in an attractive grid
            cols = st.columns(2)
            for i, insight in enumerate(ai_insights):
                with cols[i % 2]:
                    confidence_color = "#28a745" if insight['confidence'] > 0.8 else "#ffc107" if insight['confidence'] > 0.6 else "#dc3545"
                    
                    st.markdown(f"""
                    <div style="
                        background: linear-gradient(135deg, {confidence_color}15 0%, {confidence_color}25 100%);
                        border: 1px solid {confidence_color}30;
                        padding: 1.5rem;
                        border-radius: 12px;
                        margin: 0.5rem 0;
                        position: relative;
                    ">
                        <div style="display: flex; align-items: center; margin-bottom: 0.5rem;">
                            <span style="font-size: 1.5rem; margin-right: 0.5rem;">{insight['icon']}</span>
                            <h4 style="margin: 0; color: #2c3e50;">{insight['title']}</h4>
                            <span style="
                                background: {confidence_color};
                                color: white;
                                padding: 0.2rem 0.5rem;
                                border-radius: 10px;
                                font-size: 0.7rem;
                                margin-left: auto;
                            ">{insight['confidence']:.0%}</span>
                        </div>
                        <p style="margin: 0.5rem 0; color: #495057;">{insight['description']}</p>
                        <small style="color: {confidence_color}; font-weight: 500;">üí° {insight['action']}</small>
                    </div>
                    """, unsafe_allow_html=True)
            
            # Overall data quality indicator
            st.markdown("#### üìä ƒê√°nh gi√° T·ªïng th·ªÉ")
            quality_color = "#28a745" if data_quality > 85 else "#ffc107" if data_quality > 60 else "#dc3545"
            quality_status = "Tuy·ªát v·ªùi" if data_quality > 85 else "Kh√° t·ªët" if data_quality > 60 else "C·∫ßn c·∫£i thi·ªán"
            
            st.markdown(f"""
            <div style="
                background: linear-gradient(135deg, {quality_color}15 0%, {quality_color}25 100%);
                border: 2px solid {quality_color};
                padding: 1.5rem;
                border-radius: 12px;
                text-align: center;
                margin: 1rem 0;
            ">
                <h3 style="margin: 0; color: {quality_color};">Ch·∫•t l∆∞·ª£ng D·ªØ li·ªáu: {quality_status}</h3>
                <div style="font-size: 2rem; font-weight: bold; color: {quality_color}; margin: 0.5rem 0;">
                    {data_quality:.1f}%
                </div>
                <p style="margin: 0; color: #495057;">
                    D·ª±a tr√™n ph√¢n t√≠ch t√≠nh to√†n v·∫πn, t√≠nh nh·∫•t qu√°n v√† ƒë·ªô ƒë·∫ßy ƒë·ªß c·ªßa d·ªØ li·ªáu
                </p>
            </div>
            """, unsafe_allow_html=True)
        
        else:
            st.info("ü§ñ Kh√¥ng th·ªÉ t·∫°o insights AI. Vui l√≤ng th·ª≠ l·∫°i sau.")
    
    # Dataset management section (existing code continues...)
    st.markdown("### üóÇÔ∏è Qu·∫£n l√Ω B·ªô d·ªØ li·ªáu")
    
    # Filter and search options
    col1, col2, col3 = st.columns(3)
    
    with col1:
        search_term = st.text_input("üîç T√¨m ki·∫øm b·ªô d·ªØ li·ªáu:", placeholder="L·ªçc theo t√™n...")
    
    with col2:
        size_filter = st.selectbox("üìè L·ªçc k√≠ch th∆∞·ªõc:", ["T·∫•t c·∫£", "Nh·ªè (<1K)", "Trung b√¨nh (1K-10K)", "L·ªõn (>10K)"])
    
    with col3:
        sort_by = st.selectbox("üìä S·∫Øp x·∫øp theo:", ["T√™n", "Ng√†y T·∫£i l√™n", "K√≠ch th∆∞·ªõc", "C·ªôt"])
    
    # Apply filters
    filtered_datasets = datasets
    
    if search_term:
        filtered_datasets = [d for d in filtered_datasets if search_term.lower() in d[1].lower()]
    
    if size_filter != "T·∫•t c·∫£":
        if size_filter == "Nh·ªè (<1K)":
            filtered_datasets = [d for d in filtered_datasets if d[2] < 1000]
        elif size_filter == "Trung b√¨nh (1K-10K)":
            filtered_datasets = [d for d in filtered_datasets if 1000 <= d[2] <= 10000]
        elif size_filter == "L·ªõn (>10K)":
            filtered_datasets = [d for d in filtered_datasets if d[2] > 10000]
    
    # Sort datasets
    if sort_by == "T√™n":
        filtered_datasets.sort(key=lambda x: x[1])
    elif sort_by == "Ng√†y T·∫£i l√™n":
        filtered_datasets.sort(key=lambda x: datetime.strptime(x[4], "%Y-%m-%d %H:%M:%S"), reverse=True)
    elif sort_by == "K√≠ch th∆∞·ªõc":
        filtered_datasets.sort(key=lambda x: x[2], reverse=True)
    elif sort_by == "C·ªôt":
        filtered_datasets.sort(key=lambda x: x[3], reverse=True)
    
    # Display datasets with enhanced management
    for dataset in filtered_datasets:
        id_, name, rows, cols, uploaded, status = dataset
        
        with st.expander(f"üìÅ {name}", expanded=False):
            try:
                file_path = os.path.join("data", "uploads", name)
                preview_df = safe_read_csv(file_path)
                
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.markdown("#### üìä Xem tr∆∞·ªõc B·ªô d·ªØ li·ªáu")
                    st.dataframe(preview_df.head(5), use_container_width=True)
                    
                    # Quick statistics
                    numeric_cols = preview_df.select_dtypes(include=['number']).columns
                    categorical_cols = preview_df.select_dtypes(include=['object']).columns
                    missing_values = preview_df.isnull().sum().sum()
                    
                    # Data quality for this dataset
                    quality_score = create_data_quality_indicator(preview_df)
                
                with col2:
                    st.markdown("#### üéØ Th·ªëng k√™ Nhanh")
                    
                    dataset_metrics = [
                        {"title": "C·ªôt S·ªë", "value": str(len(numeric_cols))},
                        {"title": "C·ªôt VƒÉn b·∫£n", "value": str(len(categorical_cols))},
                        {"title": "Thi·∫øu", "value": str(missing_values)},
                        {"title": "Ch·∫•t l∆∞·ª£ng", "value": f"{quality_score:.0%}"}
                    ]
                    
                    render_metric_cards(dataset_metrics)
                    
                    st.markdown("#### ‚ö° H√†nh ƒë·ªông Nhanh")
                    
                    action_col1, action_col2 = st.columns(2)
                    
                    with action_col1:
                        if st.button("üîç Ph√¢n t√≠ch", key=f"analyze_{id_}", use_container_width=True):
                            st.session_state.selected_dataset_id = id_
                            st.switch_page("pages/3_üìÇ_Chi_Tiet_Bo_Du_Lieu.py")
                        
                        if st.button("üí¨ Tr√≤ chuy·ªán", key=f"chat_{id_}", use_container_width=True):
                            st.session_state.selected_dataset_id = id_
                            st.switch_page("main.py")
                    
                    with action_col2:
                        if st.button("üìä Bi·ªÉu ƒë·ªì", key=f"chart_{id_}", use_container_width=True):
                            st.session_state.selected_dataset_id = id_
                            st.switch_page("pages/6_üìà_Bieu_Do_Thong_Minh.py")
                        
                        if st.button("üìã B√°o c√°o", key=f"report_{id_}", use_container_width=True):
                            st.session_state.selected_dataset_id = id_
                            st.switch_page("pages/5_üìã_Bao_Cao_EDA.py")
                
                # Management options
                st.markdown("#### ‚öôÔ∏è T√πy ch·ªçn Qu·∫£n l√Ω")
                
                mgmt_col1, mgmt_col2, mgmt_col3 = st.columns(3)
                
                with mgmt_col1:
                    new_name = st.text_input(
                        "ƒê·ªïi t√™n b·ªô d·ªØ li·ªáu:", 
                        value=name, 
                        key=f"rename_input_{id_}",
                        help="ƒê·∫∑t t√™n m√¥ t·∫£ cho b·ªô d·ªØ li·ªáu c·ªßa b·∫°n"
                    )
                    
                    if st.button("‚úÖ ƒê·ªïi t√™n", key=f"rename_btn_{id_}"):
                        try:
                            rename_dataset(id_, new_name)
                            st.success("‚úÖ ƒê√£ ƒë·ªïi t√™n b·ªô d·ªØ li·ªáu!")
                            time.sleep(1)
                            st.rerun()
                        except Exception as e:
                            st.error(f"‚ùå L·ªói khi ƒë·ªïi t√™n: {str(e)}")
                
                with mgmt_col2:
                    if st.button("üì• T·∫£i xu·ªëng", key=f"download_{id_}", help="T·∫£i xu·ªëng b·ªô d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω"):
                        try:
                            csv_data = preview_df.to_csv(index=False)
                            st.download_button(
                                label="üì• T·∫£i xu·ªëng CSV",
                                data=csv_data,
                                file_name=f"{name.split('_', 1)[-1] if '_' in name else name}",
                                mime="text/csv",
                                key=f"download_btn_{id_}"
                            )
                        except Exception as e:
                            st.error(f"‚ùå L·ªói khi t·∫°o file t·∫£i xu·ªëng: {str(e)}")
                
                with mgmt_col3:
                    if st.button("üóëÔ∏è X√≥a", key=f"del_{id_}", type="secondary", help="X√≥a vƒ©nh vi·ªÖn b·ªô d·ªØ li·ªáu n√†y"):
                        if st.checkbox(f"X√°c nh·∫≠n x√≥a {name}", key=f"confirm_{id_}"):
                            try:
                                delete_dataset(id_)
                                st.warning(f"üóëÔ∏è ƒê√£ x√≥a b·ªô d·ªØ li·ªáu: {name}")
                                time.sleep(1)
                                st.rerun()
                            except Exception as e:
                                st.error(f"‚ùå L·ªói khi x√≥a: {str(e)}")
                
            except Exception as e:
                st.error(f"‚ùå Kh√¥ng th·ªÉ t·∫£i b·ªô d·ªØ li·ªáu: {str(e)}")
                
                # Show basic management even if preview fails
                mgmt_col1, mgmt_col2 = st.columns(2)
                
                with mgmt_col1:
                    new_name = st.text_input("ƒê·ªïi t√™n:", value=name, key=f"rename_error_{id_}")
                    if st.button("‚úÖ ƒê·ªïi t√™n", key=f"rename_error_btn_{id_}"):
                        try:
                            rename_dataset(id_, new_name)
                            st.rerun()
                        except Exception as e:
                            st.error(f"‚ùå L·ªói: {str(e)}")
                
                with mgmt_col2:
                    if st.button("üóëÔ∏è X√≥a", key=f"del_error_{id_}", type="secondary"):
                        try:
                            delete_dataset(id_)
                            st.rerun()
                        except Exception as e:
                            st.error(f"‚ùå L·ªói: {str(e)}")

else:
    # Welcome screen for new users
    st.markdown("### üëã Ch√†o m·ª´ng ƒë·∫øn v·ªõi VizGenie-GPT Chuy√™n nghi·ªáp!")
    
    # Feature showcase
    col1, col2, col3 = st.columns(3)
    
    with col1:
        render_feature_card(
            "ü§ñ Ph√¢n t√≠ch ƒë∆∞·ª£c H·ªó tr·ª£ b·ªüi AI",
            "ƒê·∫∑t c√¢u h·ªèi ph·ª©c t·∫°p v·ªÅ d·ªØ li·ªáu c·ªßa b·∫°n b·∫±ng ng√¥n ng·ªØ t·ª± nhi√™n v√† nh·∫≠n ƒë∆∞·ª£c th√¥ng tin th√¥ng minh v·ªõi tr·ª±c quan h√≥a ƒë·∫πp m·∫Øt.",
            "ü§ñ"
        )
    
    with col2:
        render_feature_card(
            "üîó Kh√°m ph√° Ch√©o B·ªô d·ªØ li·ªáu",
            "T·∫£i l√™n nhi·ªÅu b·ªô d·ªØ li·ªáu v√† kh√°m ph√° m·ªëi quan h·ªá ·∫©n v√† m√¥ h√¨nh qua c√°c ngu·ªìn d·ªØ li·ªáu c·ªßa b·∫°n.",
            "üîó"
        )
    
    with col3:
        render_feature_card(
            "üìä Bi·ªÉu ƒë·ªì Chuy√™n nghi·ªáp",
            "T·∫°o ra nh·ªØng bi·ªÉu ƒë·ªì tuy·ªát ƒë·∫πp, s·∫µn s√†ng xu·∫•t b·∫£n v·ªõi b·∫£ng m√†u th√¥ng minh v√† t√≠nh nƒÉng t∆∞∆°ng t√°c.",
            "üìä"
        )
    
    # Getting started guide
    st.markdown("### üöÄ B·∫Øt ƒë·∫ßu")
    
    render_insight_card("""
    **üìã H∆∞·ªõng d·∫´n Nhanh:**
    
    1. **üì§ T·∫£i l√™n D·ªØ li·ªáu**: S·ª≠ d·ª•ng thanh b√™n ƒë·ªÉ t·∫£i l√™n m·ªôt ho·∫∑c nhi·ªÅu file CSV
    2. **ü§ñ ƒê·∫∑t C√¢u h·ªèi**: Tr√≤ chuy·ªán v·ªõi d·ªØ li·ªáu c·ªßa b·∫°n b·∫±ng ng√¥n ng·ªØ t·ª± nhi√™n
    3. **üìä T·∫°o Tr·ª±c quan h√≥a**: T·∫°o bi·ªÉu ƒë·ªì chuy√™n nghi·ªáp t·ª± ƒë·ªông
    4. **üîó T√¨m M·ªëi quan h·ªá**: Kh√°m ph√° m√¥ h√¨nh qua nhi·ªÅu b·ªô d·ªØ li·ªáu
    5. **üìÑ Xu·∫•t B√°o c√°o**: T·∫°o b√°o c√°o PDF to√†n di·ªán cho c√°c b√™n li√™n quan
    
    **üí° M·∫πo Chuy√™n nghi·ªáp:**
    - T·∫£i l√™n c√°c b·ªô d·ªØ li·ªáu li√™n quan c√πng nhau ƒë·ªÉ ph√¢n t√≠ch ch√©o t·ªët h∆°n
    - S·ª≠ d·ª•ng t√™n m√¥ t·∫£ cho b·ªô d·ªØ li·ªáu c·ªßa b·∫°n
    - ƒê·∫∑t c√¢u h·ªèi c·ª• th·ªÉ ƒë·ªÉ c√≥ ph·∫£n h·ªìi AI t·ªët h∆°n
    - Th·ª≠ c√°c lo·∫°i bi·ªÉu ƒë·ªì v√† b·∫£ng m√†u kh√°c nhau
    """)
    
    # Sample data offer
    st.markdown("### üìö Th·ª≠ v·ªõi D·ªØ li·ªáu M·∫´u")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üìä T·∫£i D·ªØ li·ªáu B√°n h√†ng M·∫´u", type="primary", use_container_width=True):
            # Create sample sales dataset
            np.random.seed(42)
            sample_sales = pd.DataFrame({
                'date': pd.date_range('2023-01-01', periods=365, freq='D'),
                'revenue': np.random.normal(10000, 2000, 365),
                'customers': np.random.poisson(50, 365),
                'region': np.random.choice(['North', 'South', 'East', 'West'], 365),
                'product_category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home'], 365)
            })
            
            # Save sample data
            sample_path = os.path.join('data', 'uploads', 'sample_sales_data.csv')
            sample_sales.to_csv(sample_path, index=False)
            
            # Add to database
            upload_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            add_dataset('sample_sales_data.csv', sample_path, len(sample_sales), len(sample_sales.columns), upload_time)
            
            st.success("‚úÖ ƒê√£ t·∫£i d·ªØ li·ªáu b√°n h√†ng m·∫´u!")
            st.rerun()
    
    with col2:
        if st.button("üë• T·∫£i D·ªØ li·ªáu Kh√°ch h√†ng M·∫´u", type="secondary", use_container_width=True):
            # Create sample customer dataset
            np.random.seed(24)
            sample_customers = pd.DataFrame({
                'customer_id': range(1, 501),
                'age': np.random.randint(18, 70, 500),
                'gender': np.random.choice(['Male', 'Female'], 500),
                'income': np.random.normal(50000, 15000, 500),
                'satisfaction_score': np.random.randint(1, 11, 500),
                'region': np.random.choice(['North', 'South', 'East', 'West'], 500)
            })
            
            # Save sample data
            sample_path = os.path.join('data', 'uploads', 'sample_customer_data.csv')
            sample_customers.to_csv(sample_path, index=False)
            
            # Add to database
            upload_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            add_dataset('sample_customer_data.csv', sample_path, len(sample_customers), len(sample_customers.columns), upload_time)
            
            st.success("‚úÖ ƒê√£ t·∫£i d·ªØ li·ªáu kh√°ch h√†ng m·∫´u!")
            st.rerun()

# Enhanced sidebar with navigation and tips
with st.sidebar:
    if datasets:
        st.markdown("---")
        st.markdown("### üéØ Th·ªëng k√™ Nhanh")
        
        # Overall statistics
        total_size_mb = sum(os.path.getsize(os.path.join("data", "uploads", d[1])) for d in datasets if os.path.exists(os.path.join("data", "uploads", d[1]))) / (1024 * 1024)
        
        quick_stats = [
            {"title": "B·ªô d·ªØ li·ªáu", "value": str(len(datasets)), "delta": None},
            {"title": "T·ªïng K√≠ch th∆∞·ªõc", "value": f"{total_size_mb:.1f}MB", "delta": None},
            {"title": "L·ªõn nh·∫•t", "value": f"{max(d[2] for d in datasets):,}", "delta": None}
        ]
        
        render_metric_cards(quick_stats)
    
    st.markdown("---")
    st.markdown("### üîó ƒêi·ªÅu h∆∞·ªõng")
    
    nav_links = [
        ("üí¨ Tr√≤ chuy·ªán AI", "main.py"),
        ("üìä Chi ti·∫øt B·ªô d·ªØ li·ªáu", "pages/3_üìÇ_Chi_Tiet_Bo_Du_Lieu.py"),
        ("üìà Bi·ªÉu ƒë·ªì Th√¥ng minh", "pages/6_üìà_Bieu_Do_Thong_Minh.py"),
        ("üîó Ph√¢n t√≠ch Ch√©o", "pages/7_üîó_Phan_Tich_Cheo_Du_Lieu.py"),
        ("üìã L·ªãch s·ª≠ Bi·ªÉu ƒë·ªì", "pages/4_üìä_Lich_Su_Bieu_Do.py"),
        ("üìÑ B√°o c√°o EDA", "pages/5_üìã_Bao_Cao_EDA.py"),
        ("üìñ V·ªÅ d·ª± √°n", "pages/üìñ_Ve_Du_An.py")
    ]
    
    for label, page in nav_links:
        if st.button(label, key=f"nav_{label}", use_container_width=True):
            st.switch_page(page)
    
    st.markdown("---")
    st.markdown("### üí° M·∫πo & Th·ªß thu·∫≠t")
    
    tips = [
        "üéØ **Ph√¢n t√≠ch T·ªët h∆°n**: T·∫£i l√™n c√°c b·ªô d·ªØ li·ªáu li√™n quan c√πng nhau",
        "üé® **H·∫•p d·∫´n Tr·ª±c quan**: Th·ª≠ c√°c b·∫£ng m√†u kh√°c nhau trong bi·ªÉu ƒë·ªì", 
        "ü§ñ **C√¢u h·ªèi Th√¥ng minh**: C·ª• th·ªÉ v·ªÅ nh·ªØng g√¨ b·∫°n mu·ªën kh√°m ph√°",
        "üìä **Ph√¢n t√≠ch Ch√©o**: T√¨m ki·∫øm m√¥ h√¨nh qua c√°c b·ªô d·ªØ li·ªáu",
        "üìã **L∆∞u C√¥ng vi·ªác**: S·ª≠ d·ª•ng l·ªãch s·ª≠ bi·ªÉu ƒë·ªì v√† qu·∫£n l√Ω phi√™n"
    ]
    
    for tip in tips:
        st.markdown(f"- {tip}")

# Footer with system info
st.markdown("---")
col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("**üß† VizGenie-GPT Chuy√™n nghi·ªáp**")
    st.caption("N·ªÅn t·∫£ng Ph√¢n t√≠ch ƒêa B·ªô d·ªØ li·ªáu N√¢ng cao")

with col2:
    if datasets:
        st.markdown(f"**üìä Tr·∫°ng th√°i H·ªá th·ªëng**")
        st.caption(f"{len(datasets)} b·ªô d·ªØ li·ªáu ‚Ä¢ {sum(d[2] for d in datasets):,} t·ªïng b·∫£n ghi")
    else:
        st.markdown("**üöÄ S·∫µn s√†ng B·∫Øt ƒë·∫ßu**")
        st.caption("T·∫£i l√™n b·ªô d·ªØ li·ªáu ƒë·∫ßu ti√™n ƒë·ªÉ b·∫Øt ƒë·∫ßu")

with col3:
    st.markdown("**üë®‚Äçüíª Delay Group**")
    st.caption("L√†m cho ph√¢n t√≠ch d·ªØ li·ªáu c√≥ th·ªÉ ti·∫øp c·∫≠n v·ªõi m·ªçi ng∆∞·ªùi")