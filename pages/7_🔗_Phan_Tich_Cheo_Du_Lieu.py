import streamlit as st
import pandas as pd
import numpy as np
from src.utils import get_all_datasets, get_dataset, safe_read_csv
from src.models.llms import load_llm
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy.stats import pearsonr, spearmanr
import warnings
warnings.filterwarnings('ignore')

st.set_page_config(page_title="üîó Ph√¢n T√≠ch Ch√©o D·ªØ Li·ªáu", layout="wide")

# Enhanced styling
st.markdown("""
<style>
    .analysis-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        color: white;
        margin-bottom: 2rem;
        text-align: center;
    }
    .insight-card {
        background: #f8f9fa;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 4px solid #28a745;
        margin-bottom: 1rem;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    .correlation-card {
        background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%);
        padding: 1rem;
        border-radius: 8px;
        color: white;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

st.markdown('<div class="analysis-header"><h1>üîó Ph√¢n T√≠ch M·ªëi Quan H·ªá Ch√©o B·ªô D·ªØ Li·ªáu</h1><p>Kh√°m ph√° c√°c m·∫´u v√† m·ªëi quan h·ªá ·∫©n qua nhi·ªÅu b·ªô d·ªØ li·ªáu</p></div>', unsafe_allow_html=True)

llm = load_llm("gpt-3.5-turbo")

# T·∫£i datasets c√≥ s·∫µn
datasets = get_all_datasets()
if not datasets:
    st.warning("Vui l√≤ng t·∫£i l√™n datasets tr∆∞·ªõc.")
    st.stop()

# Giao di·ªán ch·ªçn dataset
st.subheader("üìÇ Ch·ªçn B·ªô D·ªØ Li·ªáu ƒë·ªÉ Ph√¢n T√≠ch")
col1, col2 = st.columns(2)

with col1:
    dataset1_options = {f"{d[0]} - {d[1]}": d[0] for d in datasets}
    dataset1_selection = st.selectbox("B·ªô D·ªØ Li·ªáu Ch√≠nh:", list(dataset1_options.keys()))
    dataset1_id = dataset1_options[dataset1_selection]

with col2:
    dataset2_options = {f"{d[0]} - {d[1]}": d[0] for d in datasets if d[0] != dataset1_id}
    if dataset2_options:
        dataset2_selection = st.selectbox("B·ªô D·ªØ Li·ªáu Ph·ª•:", list(dataset2_options.keys()))
        dataset2_id = dataset2_options[dataset2_selection]
    else:
        st.warning("C·∫ßn √≠t nh·∫•t 2 b·ªô d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch ch√©o")
        st.stop()

# T·∫£i datasets
dataset1 = get_dataset(dataset1_id)
dataset2 = get_dataset(dataset2_id)
df1 = safe_read_csv(dataset1[2])
df2 = safe_read_csv(dataset2[2])

st.success(f"‚úÖ ƒê√£ t·∫£i: **{dataset1[1]}** ({df1.shape[0]} h√†ng) v√† **{dataset2[1]}** ({df2.shape[0]} h√†ng)")

# T√πy ch·ªçn ph√¢n t√≠ch
st.subheader("üéØ Lo·∫°i Ph√¢n T√≠ch")
analysis_type = st.radio(
    "Ch·ªçn ph∆∞∆°ng ph√°p ph√¢n t√≠ch:",
    ["T∆∞∆°ng ƒê·ªìng C·ªôt", "T∆∞∆°ng Quan Th·ªëng K√™", "M·ªëi Quan H·ªá Ng·ªØ Nghƒ©a", "Ph√¢n T√≠ch T·ªïng H·ª£p"],
    horizontal=True
)

def find_similar_columns(df1, df2, similarity_threshold=0.7):
    """T√¨m c√°c c·ªôt c√≥ t√™n ho·∫∑c ki·ªÉu d·ªØ li·ªáu t∆∞∆°ng t·ª±"""
    similar_pairs = []
    
    for col1 in df1.columns:
        for col2 in df2.columns:
            # T∆∞∆°ng ƒë·ªìng t√™n
            name_sim = len(set(col1.lower().split()) & set(col2.lower().split())) / max(len(set(col1.lower().split())), len(set(col2.lower().split())))
            
            # T∆∞∆°ng ƒë·ªìng ki·ªÉu d·ªØ li·ªáu
            type_sim = 1.0 if df1[col1].dtype == df2[col2].dtype else 0.5
            
            # T∆∞∆°ng ƒë·ªìng t·ªïng h·ª£p
            combined_sim = (name_sim + type_sim) / 2
            
            if combined_sim >= similarity_threshold:
                similar_pairs.append({
                    'col1': col1,
                    'col2': col2,
                    'similarity': combined_sim,
                    'type1': str(df1[col1].dtype),
                    'type2': str(df2[col2].dtype)
                })
    
    return sorted(similar_pairs, key=lambda x: x['similarity'], reverse=True)

def calculate_cross_correlations(df1, df2):
    """T√≠nh t∆∞∆°ng quan gi·ªØa c√°c c·ªôt s·ªë qua c√°c b·ªô d·ªØ li·ªáu"""
    num_cols1 = df1.select_dtypes(include=[np.number]).columns
    num_cols2 = df2.select_dtypes(include=[np.number]).columns
    
    correlations = []
    
    for col1 in num_cols1:
        for col2 in num_cols2:
            try:
                # CƒÉn ch·ªânh ƒë·ªô d√†i cho t∆∞∆°ng quan
                min_len = min(len(df1[col1]), len(df2[col2]))
                
                # T√≠nh t∆∞∆°ng quan Pearson
                pearson_r, pearson_p = pearsonr(df1[col1][:min_len].fillna(0), df2[col2][:min_len].fillna(0))
                
                # T√≠nh t∆∞∆°ng quan Spearman
                spearman_r, spearman_p = spearmanr(df1[col1][:min_len].fillna(0), df2[col2][:min_len].fillna(0))
                
                correlations.append({
                    'col1': col1,
                    'col2': col2,
                    'pearson_r': pearson_r,
                    'pearson_p': pearson_p,
                    'spearman_r': spearman_r,
                    'spearman_p': spearman_p,
                    'significance': 'Cao' if min(pearson_p, spearman_p) < 0.01 else 'Trung B√¨nh' if min(pearson_p, spearman_p) < 0.05 else 'Th·∫•p'
                })
            except:
                continue
    
    return sorted(correlations, key=lambda x: abs(x['pearson_r']), reverse=True)

def generate_ai_insights(df1, df2, dataset1_name, dataset2_name, analysis_results):
    """T·∫°o insights AI v·ªÅ m·ªëi quan h·ªá"""
    prompt = f"""
    L√† m·ªôt nh√† khoa h·ªçc d·ªØ li·ªáu, h√£y ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa hai b·ªô d·ªØ li·ªáu:

    B·ªô D·ªØ Li·ªáu 1: {dataset1_name}
    - K√≠ch th∆∞·ªõc: {df1.shape}
    - C√°c c·ªôt: {list(df1.columns)[:10]}...
    - D·ªØ li·ªáu m·∫´u: {df1.head(2).to_dict()}

    B·ªô D·ªØ Li·ªáu 2: {dataset2_name}
    - K√≠ch th∆∞·ªõc: {df2.shape}
    - C√°c c·ªôt: {list(df2.columns)[:10]}...
    - D·ªØ li·ªáu m·∫´u: {df2.head(2).to_dict()}

    K·∫øt qu·∫£ Ph√¢n t√≠ch: {str(analysis_results)[:1000]}...

    Cung c·∫•p insights theo ƒë·ªãnh d·∫°ng n√†y:
    
    ## üîç M·ªëi Quan H·ªá Ch√≠nh ƒê∆∞·ª£c T√¨m Th·∫•y
    [Li·ªát k√™ 3-5 m·ªëi quan h·ªá quan tr·ªçng nh·∫•t]
    
    ## üìä √ù Nghƒ©a Kinh Doanh
    [Gi·∫£i th√≠ch √Ω nghƒ©a c·ªßa c√°c m·ªëi quan h·ªá n√†y trong b·ªëi c·∫£nh kinh doanh]
    
    ## üéØ H√†nh ƒê·ªông ƒê∆∞·ª£c ƒê·ªÅ Xu·∫•t
    [ƒê·ªÅ xu·∫•t c√°c h√†nh ƒë·ªông c·ª• th·ªÉ d·ª±a tr√™n ph√°t hi·ªán]
    
    ## ‚ö†Ô∏è H·∫°n Ch·∫ø & C√¢n Nh·∫Øc
    [ƒê·ªÅ c·∫≠p ƒë·∫øn b·∫•t k·ª≥ l∆∞u √Ω ho·∫∑c h·∫°n ch·∫ø n√†o]
    
    H√£y c·ª• th·ªÉ v√† c√≥ th·ªÉ h√†nh ƒë·ªông. T·∫≠p trung v√†o insights th·ª±c t·∫ø.
    """
    
    return llm.invoke(prompt)

# Th·ª±c hi·ªán ph√¢n t√≠ch d·ª±a tr√™n l·ª±a ch·ªçn
if st.button("üöÄ Ch·∫°y Ph√¢n T√≠ch", type="primary"):
    with st.spinner("ƒêang ph√¢n t√≠ch m·ªëi quan h·ªá..."):
        
        if analysis_type == "T∆∞∆°ng ƒê·ªìng C·ªôt":
            similar_cols = find_similar_columns(df1, df2)
            
            st.subheader("üìã C√°c C·ªôt T∆∞∆°ng T·ª± ƒê∆∞·ª£c T√¨m Th·∫•y")
            if similar_cols:
                for pair in similar_cols[:10]:  # Hi·ªÉn th·ªã top 10
                    st.markdown(f"""
                    <div class="insight-card">
                        <strong>{pair['col1']}</strong> ‚ÜîÔ∏è <strong>{pair['col2']}</strong><br>
                        T∆∞∆°ng ƒë·ªìng: {pair['similarity']:.2%} | Lo·∫°i: {pair['type1']} vs {pair['type2']}
                    </div>
                    """, unsafe_allow_html=True)
            else:
                st.info("Kh√¥ng t√¨m th·∫•y c·ªôt t∆∞∆°ng t·ª± v·ªõi ng∆∞·ª°ng hi·ªán t·∫°i.")
        
        elif analysis_type == "T∆∞∆°ng Quan Th·ªëng K√™":
            correlations = calculate_cross_correlations(df1, df2)
            
            st.subheader("üìà T∆∞∆°ng Quan Ch√©o B·ªô D·ªØ Li·ªáu")
            if correlations:
                # T·∫°o tr·ª±c quan h√≥a ma tr·∫≠n t∆∞∆°ng quan
                correlation_data = []
                for corr in correlations[:20]:  # Top 20
                    correlation_data.append({
                        'M·ªëi Quan H·ªá': f"{corr['col1']} √ó {corr['col2']}",
                        'Pearson R': corr['pearson_r'],
                        'Spearman R': corr['spearman_r'],
                        '√ù Nghƒ©a': corr['significance']
                    })
                
                corr_df = pd.DataFrame(correlation_data)
                
                # Bi·ªÉu ƒë·ªì plotly t∆∞∆°ng t√°c
                fig = px.bar(corr_df, x='M·ªëi Quan H·ªá', y='Pearson R', 
                           color='√ù Nghƒ©a', 
                           title="T∆∞∆°ng Quan Ch√©o B·ªô D·ªØ Li·ªáu",
                           color_discrete_map={'Cao': '#e74c3c', 'Trung B√¨nh': '#f39c12', 'Th·∫•p': '#95a5a6'})
                fig.update_layout(xaxis_tickangle=-45)
                st.plotly_chart(fig, use_container_width=True)
                
                # Hi·ªÉn th·ªã b·∫£ng
                st.dataframe(corr_df, use_container_width=True)
            else:
                st.info("Kh√¥ng t√¨m th·∫•y t∆∞∆°ng quan ƒë√°ng k·ªÉ.")
        
        elif analysis_type == "M·ªëi Quan H·ªá Ng·ªØ Nghƒ©a":
            # Ph√¢n t√≠ch ng·ªØ nghƒ©a b·∫±ng AI
            prompt = f"""
            Ph√¢n t√≠ch hai b·ªô d·ªØ li·ªáu n√†y ƒë·ªÉ t√¨m m·ªëi quan h·ªá ng·ªØ nghƒ©a:
            
            C√°c c·ªôt B·ªô d·ªØ li·ªáu 1: {list(df1.columns)}
            C√°c c·ªôt B·ªô d·ªØ li·ªáu 2: {list(df2.columns)}
            
            T√¨m c√°c m·ªëi quan h·ªá ng·ªØ nghƒ©a ti·ªÅm nƒÉng nh∆∞:
            - K·∫øt n·ªëi ƒë·ªãa l√Ω (th√†nh ph·ªë, bang, qu·ªëc gia)
            - K·∫øt n·ªëi th·ªùi gian (ng√†y, th·ªùi gian, giai ƒëo·∫°n)
            - K·∫øt n·ªëi ph√¢n lo·∫°i (lo·∫°i, danh m·ª•c, l·ªõp)
            - K·∫øt n·ªëi ph√¢n c·∫•p (m·ªëi quan h·ªá cha-con)
            
            Tr·∫£ v·ªÅ danh s√°ch JSON c·ªßa c√°c m·ªëi quan h·ªá ti·ªÅm nƒÉng v·ªõi ƒëi·ªÉm tin c·∫≠y.
            """
            
            ai_relationships = llm.invoke(prompt)
            
            st.subheader("üß† M·ªëi Quan H·ªá Ng·ªØ Nghƒ©a ƒê∆∞·ª£c AI Ph√°t Hi·ªán")
            st.markdown(ai_relationships)
        
        elif analysis_type == "Ph√¢n T√≠ch T·ªïng H·ª£p":
            # Ch·∫°y t·∫•t c·∫£ ph√¢n t√≠ch
            similar_cols = find_similar_columns(df1, df2)
            correlations = calculate_cross_correlations(df1, df2)
            
            # T·∫°o insights AI t·ªïng h·ª£p
            all_results = {
                'similar_columns': similar_cols[:5],
                'correlations': correlations[:5]
            }
            
            ai_insights = generate_ai_insights(df1, df2, dataset1[1], dataset2[1], all_results)
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£ trong tabs
            tab1, tab2, tab3 = st.tabs(["üîç Insights AI", "üìã C·ªôt T∆∞∆°ng T·ª±", "üìà T∆∞∆°ng Quan"])
            
            with tab1:
                st.markdown(ai_insights)
            
            with tab2:
                if similar_cols:
                    for pair in similar_cols[:10]:
                        st.markdown(f"""
                        <div class="insight-card">
                            <strong>{pair['col1']}</strong> ‚ÜîÔ∏è <strong>{pair['col2']}</strong><br>
                            T∆∞∆°ng ƒë·ªìng: {pair['similarity']:.2%}
                        </div>
                        """, unsafe_allow_html=True)
            
            with tab3:
                if correlations:
                    correlation_data = []
                    for corr in correlations[:15]:
                        correlation_data.append({
                            'C·ªôt 1': corr['col1'],
                            'C·ªôt 2': corr['col2'],
                            'T∆∞∆°ng Quan': f"{corr['pearson_r']:.3f}",
                            'P-value': f"{corr['pearson_p']:.3f}",
                            '√ù Nghƒ©a': corr['significance']
                        })
                    
                    st.dataframe(pd.DataFrame(correlation_data), use_container_width=True)

# Giao di·ªán Truy v·∫•n N√¢ng cao
st.subheader("üí¨ ƒê·∫∑t C√¢u H·ªèi Qua C√°c B·ªô D·ªØ Li·ªáu")
query_placeholder = st.text_area(
    "ƒê·∫∑t c√¢u h·ªèi ph·ª©c t·∫°p tr·∫£i r·ªông c·∫£ hai b·ªô d·ªØ li·ªáu:",
    placeholder="V√≠ d·ª•:\n- C√≥ bao nhi√™u gi√°o vi√™n n·ªØ ·ªü tr∆∞·ªùng ti·ªÉu h·ªçc H√† N·ªôi?\n- T∆∞∆°ng quan t·ª∑ l·ªá b·ªè h·ªçc gi·ªØa c√°c v√πng nh∆∞ th·∫ø n√†o?\n- So s√°nh hi·ªáu su·∫•t h·ªçc sinh qua c√°c lo·∫°i tr∆∞·ªùng kh√°c nhau",
    height=100
)

if st.button("üéØ Tr·∫£ L·ªùi C√¢u H·ªèi") and query_placeholder:
    with st.spinner("ƒêang x·ª≠ l√Ω truy v·∫•n ph·ª©c t·∫°p..."):
        enhanced_prompt = f"""
        B·∫°n c√≥ quy·ªÅn truy c·∫≠p v√†o hai b·ªô d·ªØ li·ªáu:
        
        B·ªô D·ªØ Li·ªáu 1: {dataset1[1]}
        C√°c c·ªôt: {list(df1.columns)}
        M·∫´u: {df1.head(2).to_dict()}
        
        B·ªô D·ªØ Li·ªáu 2: {dataset2[1]}
        C√°c c·ªôt: {list(df2.columns)}
        M·∫´u: {df2.head(2).to_dict()}
        
        C√¢u H·ªèi Ng∆∞·ªùi D√πng: {query_placeholder}
        
        Cung c·∫•p c√¢u tr·∫£ l·ªùi to√†n di·ªán bao g·ªìm:
        1. X√°c ƒë·ªãnh b·ªô d·ªØ li·ªáu v√† c·ªôt n√†o li√™n quan
        2. Gi·∫£i th√≠ch b·∫•t k·ª≥ gi·∫£ ƒë·ªãnh n√†o ƒë∆∞·ª£c ƒë∆∞a ra
        3. Cung c·∫•p s·ªë li·ªáu/insights c·ª• th·ªÉ khi c√≥ th·ªÉ
        4. ƒê·ªÅ xu·∫•t ph√¢n t√≠ch ti·∫øp theo
        5. L∆∞u √Ω b·∫•t k·ª≥ h·∫°n ch·∫ø n√†o
        
        H√£y c·ª• th·ªÉ v√† d·ª±a tr√™n d·ªØ li·ªáu trong ph·∫£n h·ªìi c·ªßa b·∫°n.
        """
        
        response = llm.invoke(enhanced_prompt)
        
        st.markdown("### üéØ K·∫øt Qu·∫£ Ph√¢n T√≠ch")
        st.markdown(f"""
        <div class="insight-card">
            {response}
        </div>
        """, unsafe_allow_html=True)

# Xu·∫•t k·∫øt qu·∫£
if st.button("üì• Xu·∫•t B√°o C√°o Ph√¢n T√≠ch"):
    st.success("B√°o c√°o ph√¢n t√≠ch s·∫Ω ƒë∆∞·ª£c t·∫°o v√† t·∫£i xu·ªëng ·ªü ƒë√¢y")