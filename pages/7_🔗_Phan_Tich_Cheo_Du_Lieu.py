import streamlit as st
import pandas as pd
import numpy as np
from src.utils import get_all_datasets, get_dataset, safe_read_csv
from src.models.llms import load_llm
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy.stats import pearsonr, spearmanr
import warnings
import time
import json
import re
warnings.filterwarnings('ignore')

st.set_page_config(page_title="üîó Ph√¢n T√≠ch Ch√©o D·ªØ Li·ªáu", layout="wide")

# Enhanced styling
st.markdown("""
<style>
    .analysis-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        color: white;
        margin-bottom: 2rem;
        text-align: center;
    }
    .insight-card {
        background: #f8f9fa;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 4px solid #28a745;
        margin-bottom: 1rem;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    .correlation-card {
        background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%);
        padding: 1rem;
        border-radius: 8px;
        color: white;
        margin: 0.5rem 0;
    }
    .loading-spinner {
        display: flex;
        align-items: center;
        justify-content: center;
        padding: 2rem;
        background: #f8f9fa;
        border-radius: 10px;
        margin: 1rem 0;
    }
    .spinner {
        border: 4px solid #f3f3f3;
        border-top: 4px solid #667eea;
        border-radius: 50%;
        width: 40px;
        height: 40px;
        animation: spin 1s linear infinite;
        margin-right: 1rem;
    }
    @keyframes spin {
        0% { transform: rotate(0deg); }
        100% { transform: rotate(360deg); }
    }
    .analysis-result {
        background: linear-gradient(135deg, #56CCF215 0%, #2F80ED15 100%);
        border: 1px solid #56CCF230;
        padding: 1.5rem;
        border-radius: 12px;
        margin: 1rem 0;
    }
    .error-card {
        background: linear-gradient(135deg, #ff6b6b15 0%, #ee5a2415 100%);
        border: 1px solid #ff6b6b30;
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

st.markdown('<div class="analysis-header"><h1>üîó Ph√¢n T√≠ch M·ªëi Quan H·ªá Ch√©o B·ªô D·ªØ Li·ªáu</h1><p>Kh√°m ph√° c√°c m·∫´u v√† m·ªëi quan h·ªá ·∫©n qua nhi·ªÅu b·ªô d·ªØ li·ªáu v·ªõi AI n√¢ng cao</p></div>', unsafe_allow_html=True)

llm = load_llm("gpt-3.5-turbo")

def show_loading(text="ƒêang x·ª≠ l√Ω..."):
    """Show loading animation"""
    return st.markdown(f"""
    <div class="loading-spinner">
        <div class="spinner"></div>
        <span style="color: #667eea; font-weight: 500;">{text}</span>
    </div>
    """, unsafe_allow_html=True)

def safe_llm_invoke(prompt, max_retries=3):
    """Safely invoke LLM with retries and error handling"""
    for attempt in range(max_retries):
        try:
            response = llm.invoke(prompt)
            if isinstance(response, str):
                return response
            elif hasattr(response, 'content'):
                return response.content
            else:
                return str(response)
        except Exception as e:
            if attempt == max_retries - 1:
                return f"L·ªói LLM: {str(e)}"
            time.sleep(1)
    return "Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn AI"

def extract_json_from_response(response):
    """Extract JSON from LLM response"""
    try:
        # Try to find JSON in the response
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            return json.loads(json_str)
        
        # Try to parse the entire response as JSON
        return json.loads(response)
    except:
        # Return structured fallback
        return {
            "relationships": [
                {
                    "type": "text_analysis",
                    "description": response,
                    "confidence": 0.5
                }
            ]
        }

def find_similar_columns(df1, df2, similarity_threshold=0.6):
    """Find columns with similar names or data types"""
    try:
        similar_pairs = []
        
        for col1 in df1.columns:
            for col2 in df2.columns:
                try:
                    # Name similarity (case insensitive)
                    name1_words = set(col1.lower().replace('_', ' ').split())
                    name2_words = set(col2.lower().replace('_', ' ').split())
                    
                    if name1_words and name2_words:
                        name_sim = len(name1_words & name2_words) / max(len(name1_words), len(name2_words))
                    else:
                        name_sim = 0
                    
                    # Type similarity
                    type1 = str(df1[col1].dtype)
                    type2 = str(df2[col2].dtype)
                    type_sim = 1.0 if type1 == type2 else 0.3
                    
                    # Data pattern similarity for object columns
                    pattern_sim = 0
                    if df1[col1].dtype == 'object' and df2[col2].dtype == 'object':
                        sample1 = df1[col1].dropna().head(10).astype(str).tolist()
                        sample2 = df2[col2].dropna().head(10).astype(str).tolist()
                        
                        if sample1 and sample2:
                            # Check average length similarity
                            avg_len1 = np.mean([len(s) for s in sample1])
                            avg_len2 = np.mean([len(s) for s in sample2])
                            len_diff = abs(avg_len1 - avg_len2) / max(avg_len1, avg_len2, 1)
                            pattern_sim = max(0, 1 - len_diff)
                    
                    # Combined similarity
                    combined_sim = (name_sim * 0.5 + type_sim * 0.3 + pattern_sim * 0.2)
                    
                    if combined_sim >= similarity_threshold:
                        similar_pairs.append({
                            'col1': col1,
                            'col2': col2,
                            'similarity': combined_sim,
                            'name_sim': name_sim,
                            'type1': type1,
                            'type2': type2,
                            'pattern_sim': pattern_sim
                        })
                except Exception as e:
                    continue
        
        return sorted(similar_pairs, key=lambda x: x['similarity'], reverse=True)
    
    except Exception as e:
        st.error(f"L·ªói t√¨m c·ªôt t∆∞∆°ng t·ª±: {str(e)}")
        return []

def calculate_cross_correlations(df1, df2, max_pairs=50):
    """Calculate correlations between numeric columns across datasets"""
    try:
        num_cols1 = df1.select_dtypes(include=[np.number]).columns[:10]  # Limit for performance
        num_cols2 = df2.select_dtypes(include=[np.number]).columns[:10]
        
        if len(num_cols1) == 0 or len(num_cols2) == 0:
            return []
        
        correlations = []
        pair_count = 0
        
        for col1 in num_cols1:
            for col2 in num_cols2:
                if pair_count >= max_pairs:
                    break
                    
                try:
                    # Align data lengths
                    min_len = min(len(df1[col1]), len(df2[col2]))
                    data1 = df1[col1][:min_len].fillna(df1[col1].median())
                    data2 = df2[col2][:min_len].fillna(df2[col2].median())
                    
                    # Skip if not enough data
                    if len(data1) < 10 or data1.std() == 0 or data2.std() == 0:
                        continue
                    
                    # Calculate correlations
                    pearson_r, pearson_p = pearsonr(data1, data2)
                    spearman_r, spearman_p = spearmanr(data1, data2)
                    
                    # Skip very weak correlations
                    if abs(pearson_r) < 0.1 and abs(spearman_r) < 0.1:
                        continue
                    
                    correlations.append({
                        'col1': col1,
                        'col2': col2,
                        'pearson_r': pearson_r,
                        'pearson_p': pearson_p,
                        'spearman_r': spearman_r,
                        'spearman_p': spearman_p,
                        'significance': 'Cao' if min(pearson_p, spearman_p) < 0.01 else 'Trung B√¨nh' if min(pearson_p, spearman_p) < 0.05 else 'Th·∫•p',
                        'strength': 'M·∫°nh' if max(abs(pearson_r), abs(spearman_r)) > 0.7 else 'Trung B√¨nh' if max(abs(pearson_r), abs(spearman_r)) > 0.3 else 'Y·∫øu'
                    })
                    
                    pair_count += 1
                    
                except Exception as e:
                    continue
        
        return sorted(correlations, key=lambda x: max(abs(x['pearson_r']), abs(x['spearman_r'])), reverse=True)
    
    except Exception as e:
        st.error(f"L·ªói t√≠nh t∆∞∆°ng quan: {str(e)}")
        return []

def generate_ai_insights(df1, df2, dataset1_name, dataset2_name, analysis_results):
    """Generate comprehensive AI insights about relationships"""
    try:
        # Prepare data summary
        data_summary = f"""
        B·ªô D·ªØ Li·ªáu 1: {dataset1_name}
        - K√≠ch th∆∞·ªõc: {df1.shape}
        - C·ªôt s·ªë: {len(df1.select_dtypes(include=[np.number]).columns)}
        - C·ªôt vƒÉn b·∫£n: {len(df1.select_dtypes(include=['object']).columns)}
        - D·ªØ li·ªáu m·∫´u: {df1.head(2).to_dict() if not df1.empty else 'Tr·ªëng'}

        B·ªô D·ªØ Li·ªáu 2: {dataset2_name}
        - K√≠ch th∆∞·ªõc: {df2.shape}
        - C·ªôt s·ªë: {len(df2.select_dtypes(include=[np.number]).columns)}
        - C·ªôt vƒÉn b·∫£n: {len(df2.select_dtypes(include=['object']).columns)}
        - D·ªØ li·ªáu m·∫´u: {df2.head(2).to_dict() if not df2.empty else 'Tr·ªëng'}
        """
        
        results_summary = str(analysis_results)[:1000] + "..." if len(str(analysis_results)) > 1000 else str(analysis_results)
        
        prompt = f"""
        Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa hai b·ªô d·ªØ li·ªáu:

        {data_summary}

        K·∫øt qu·∫£ ph√¢n t√≠ch: {results_summary}

        H√£y ƒë∆∞a ra insights theo ƒë·ªãnh d·∫°ng markdown:
        
        ## üîç M·ªëi Quan H·ªá Ch√≠nh ƒê∆∞·ª£c T√¨m Th·∫•y
        [Li·ªát k√™ 3-5 m·ªëi quan h·ªá quan tr·ªçng nh·∫•t v·ªõi s·ªë li·ªáu c·ª• th·ªÉ]
        
        ## üìä √ù Nghƒ©a Kinh Doanh
        [Gi·∫£i th√≠ch √Ω nghƒ©a th·ª±c t·∫ø c·ªßa c√°c m·ªëi quan h·ªá n√†y]
        
        ## üéØ H√†nh ƒê·ªông ƒê∆∞·ª£c ƒê·ªÅ Xu·∫•t
        [3-4 h√†nh ƒë·ªông c·ª• th·ªÉ c√≥ th·ªÉ th·ª±c hi·ªán d·ª±a tr√™n ph√°t hi·ªán]
        
        ## ‚ö†Ô∏è H·∫°n Ch·∫ø & C√¢n Nh·∫Øc
        [Nh·ªØng l∆∞u √Ω quan tr·ªçng v·ªÅ ƒë·ªô tin c·∫≠y v√† gi·ªõi h·∫°n c·ªßa ph√¢n t√≠ch]
        
        H√£y c·ª• th·ªÉ, d·ª±a tr√™n d·ªØ li·ªáu th·ª±c v√† c√≥ th·ªÉ h√†nh ƒë·ªông.
        """
        
        return safe_llm_invoke(prompt)
    
    except Exception as e:
        return f"L·ªói t·∫°o insights AI: {str(e)}"

def perform_semantic_analysis(df1, df2):
    """Perform semantic relationship analysis using AI"""
    try:
        cols1_info = {col: df1[col].dtype for col in df1.columns[:10]}
        cols2_info = {col: df2[col].dtype for col in df2.columns[:10]}
        
        prompt = f"""
        Ph√¢n t√≠ch c√°c m·ªëi quan h·ªá ng·ªØ nghƒ©a ti·ªÅm nƒÉng gi·ªØa hai b·ªô d·ªØ li·ªáu:
        
        C·ªôt B·ªô d·ªØ li·ªáu 1: {cols1_info}
        C·ªôt B·ªô d·ªØ li·ªáu 2: {cols2_info}
        
        T√¨m c√°c m·ªëi quan h·ªá ng·ªØ nghƒ©a nh∆∞:
        - K·∫øt n·ªëi ƒë·ªãa l√Ω (th√†nh ph·ªë ‚Üî khu v·ª±c)
        - K·∫øt n·ªëi th·ªùi gian (ng√†y ‚Üî th√°ng ‚Üî nƒÉm)
        - K·∫øt n·ªëi ph√¢n lo·∫°i (lo·∫°i ‚Üî danh m·ª•c)
        - K·∫øt n·ªëi ƒë·ªãnh danh (ID kh√°ch h√†ng ‚Üî m√£ kh√°ch h√†ng)
        - K·∫øt n·ªëi ph√¢n c·∫•p (chi nh√°nh ‚Üî c√¥ng ty)
        
        Tr·∫£ v·ªÅ JSON format:
        {{
            "relationships": [
                {{
                    "col1": "t√™n_c·ªôt_1",
                    "col2": "t√™n_c·ªôt_2", 
                    "type": "lo·∫°i_quan_h·ªá",
                    "description": "m√¥_t·∫£_chi_ti·∫øt",
                    "confidence": 0.8
                }}
            ]
        }}
        """
        
        response = safe_llm_invoke(prompt)
        return extract_json_from_response(response)
    
    except Exception as e:
        return {
            "relationships": [
                {
                    "col1": "error",
                    "col2": "error",
                    "type": "error",
                    "description": f"L·ªói ph√¢n t√≠ch ng·ªØ nghƒ©a: {str(e)}",
                    "confidence": 0.0
                }
            ]
        }

def create_correlation_visualization(correlations):
    """Create enhanced correlation visualization"""
    try:
        if not correlations:
            fig = go.Figure()
            fig.add_annotation(text="Kh√¥ng c√≥ d·ªØ li·ªáu t∆∞∆°ng quan", 
                             x=0.5, y=0.5, showarrow=False)
            return fig
        
        # Prepare data for visualization
        df_corr = pd.DataFrame(correlations[:20])  # Top 20 correlations
        
        # Create subplot
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=[
                'T∆∞∆°ng quan Pearson',
                'T∆∞∆°ng quan Spearman', 
                'M·ª©c ƒë·ªô √ù nghƒ©a',
                'T·ªïng h·ª£p T∆∞∆°ng quan'
            ],
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        # Pearson correlation
        fig.add_trace(
            go.Bar(
                x=[f"{r['col1']} √ó {r['col2']}" for r in correlations[:10]],
                y=[r['pearson_r'] for r in correlations[:10]],
                name="Pearson",
                marker=dict(color=[r['pearson_r'] for r in correlations[:10]], 
                          colorscale='RdBu', cmin=-1, cmax=1),
                text=[f"{r['pearson_r']:.3f}" for r in correlations[:10]],
                textposition='outside'
            ),
            row=1, col=1
        )
        
        # Spearman correlation
        fig.add_trace(
            go.Bar(
                x=[f"{r['col1']} √ó {r['col2']}" for r in correlations[:10]],
                y=[r['spearman_r'] for r in correlations[:10]],
                name="Spearman",
                marker=dict(color=[r['spearman_r'] for r in correlations[:10]], 
                          colorscale='RdBu', cmin=-1, cmax=1),
                text=[f"{r['spearman_r']:.3f}" for r in correlations[:10]],
                textposition='outside'
            ),
            row=1, col=2
        )
        
        # Significance levels
        sig_counts = {}
        for corr in correlations:
            sig = corr['significance']
            sig_counts[sig] = sig_counts.get(sig, 0) + 1
        
        fig.add_trace(
            go.Pie(
                labels=list(sig_counts.keys()),
                values=list(sig_counts.values()),
                name="√ù nghƒ©a"
            ),
            row=2, col=1
        )
        
        # Strength distribution
        strength_counts = {}
        for corr in correlations:
            strength = corr['strength']
            strength_counts[strength] = strength_counts.get(strength, 0) + 1
        
        fig.add_trace(
            go.Pie(
                labels=list(strength_counts.keys()),
                values=list(strength_counts.values()),
                name="ƒê·ªô m·∫°nh"
            ),
            row=2, col=2
        )
        
        # Update layout
        fig.update_layout(
            height=800,
            title_text="Ph√¢n t√≠ch T∆∞∆°ng quan Ch√©o B·ªô d·ªØ li·ªáu",
            showlegend=False
        )
        
        # Update x-axes for better readability
        fig.update_xaxes(tickangle=45, row=1, col=1)
        fig.update_xaxes(tickangle=45, row=1, col=2)
        
        return fig
    
    except Exception as e:
        # Fallback simple chart
        fig = go.Figure()
        fig.add_annotation(text=f"L·ªói t·∫°o bi·ªÉu ƒë·ªì: {str(e)}", 
                         x=0.5, y=0.5, showarrow=False)
        return fig

# Load datasets
datasets = get_all_datasets()
if not datasets:
    st.warning("Vui l√≤ng t·∫£i l√™n datasets tr∆∞·ªõc.")
    st.stop()

# Dataset selection interface
st.subheader("üìÇ Ch·ªçn B·ªô D·ªØ Li·ªáu ƒë·ªÉ Ph√¢n T√≠ch")
col1, col2 = st.columns(2)

with col1:
    dataset1_options = {f"{d[0]} - {d[1]}": d[0] for d in datasets}
    dataset1_selection = st.selectbox("üóÇÔ∏è B·ªô D·ªØ Li·ªáu Ch√≠nh:", list(dataset1_options.keys()))
    dataset1_id = dataset1_options[dataset1_selection]

with col2:
    dataset2_options = {f"{d[0]} - {d[1]}": d[0] for d in datasets if d[0] != dataset1_id}
    if dataset2_options:
        dataset2_selection = st.selectbox("üìã B·ªô D·ªØ Li·ªáu Ph·ª•:", list(dataset2_options.keys()))
        dataset2_id = dataset2_options[dataset2_selection]
    else:
        st.warning("‚ö†Ô∏è C·∫ßn √≠t nh·∫•t 2 b·ªô d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch ch√©o")
        st.stop()

# Load datasets with error handling
try:
    dataset1 = get_dataset(dataset1_id)
    dataset2 = get_dataset(dataset2_id)
    df1 = safe_read_csv(dataset1[2])
    df2 = safe_read_csv(dataset2[2])
    
    if df1.empty or df2.empty:
        st.error("‚ùå M·ªôt trong c√°c b·ªô d·ªØ li·ªáu tr·ªëng. Vui l√≤ng ki·ªÉm tra l·∫°i.")
        st.stop()
    
    st.success(f"‚úÖ ƒê√£ t·∫£i: **{dataset1[1]}** ({df1.shape[0]:,} h√†ng, {df1.shape[1]} c·ªôt) v√† **{dataset2[1]}** ({df2.shape[0]:,} h√†ng, {df2.shape[1]} c·ªôt)")
    
except Exception as e:
    st.error(f"‚ùå L·ªói t·∫£i d·ªØ li·ªáu: {str(e)}")
    st.stop()

# Analysis type selection
st.subheader("üéØ Lo·∫°i Ph√¢n T√≠ch")
analysis_type = st.radio(
    "Ch·ªçn ph∆∞∆°ng ph√°p ph√¢n t√≠ch:",
    ["T∆∞∆°ng ƒê·ªìng C·ªôt", "T∆∞∆°ng Quan Th·ªëng K√™", "M·ªëi Quan H·ªá Ng·ªØ Nghƒ©a", "Ph√¢n T√≠ch T·ªïng H·ª£p"],
    horizontal=True,
    help="Ch·ªçn lo·∫°i ph√¢n t√≠ch ph√π h·ª£p v·ªõi m·ª•c ƒë√≠ch nghi√™n c·ª©u c·ªßa b·∫°n"
)

# Main analysis execution
if st.button("üöÄ Ch·∫°y Ph√¢n T√≠ch", type="primary"):
    # Show loading
    loading_placeholder = st.empty()
    
    try:
        if analysis_type == "T∆∞∆°ng ƒê·ªìng C·ªôt":
            with loading_placeholder:
                show_loading("üîç ƒêang t√¨m c√°c c·ªôt t∆∞∆°ng t·ª±...")
            
            time.sleep(1)
            similar_cols = find_similar_columns(df1, df2)
            loading_placeholder.empty()
            
            st.subheader("üìã C√°c C·ªôt T∆∞∆°ng T·ª± ƒê∆∞·ª£c T√¨m Th·∫•y")
            
            if similar_cols:
                # Display results in a nice format
                for i, pair in enumerate(similar_cols[:15]):  # Show top 15
                    confidence_color = "#28a745" if pair['similarity'] > 0.8 else "#ffc107" if pair['similarity'] > 0.6 else "#dc3545"
                    
                    st.markdown(f"""
                    <div class="analysis-result">
                        <div style="display: flex; justify-content: space-between; align-items: center;">
                            <div>
                                <h4 style="margin: 0; color: #2c3e50;">
                                    üìä {pair['col1']} ‚ÜîÔ∏è {pair['col2']}
                                </h4>
                                <p style="margin: 0.5rem 0; color: #495057;">
                                    <strong>Lo·∫°i:</strong> {pair['type1']} vs {pair['type2']}<br>
                                    <strong>T∆∞∆°ng ƒë·ªìng t√™n:</strong> {pair['name_sim']:.1%}<br>
                                    <strong>T∆∞∆°ng ƒë·ªìng m·∫´u:</strong> {pair['pattern_sim']:.1%}
                                </p>
                            </div>
                            <div style="text-align: center;">
                                <div style="
                                    background: {confidence_color};
                                    color: white;
                                    padding: 0.5rem 1rem;
                                    border-radius: 20px;
                                    font-weight: bold;
                                ">
                                    {pair['similarity']:.1%}
                                </div>
                                <small style="color: #666;">ƒê·ªô tin c·∫≠y</small>
                            </div>
                        </div>
                    </div>
                    """, unsafe_allow_html=True)
                
                # Summary statistics
                st.markdown("### üìä T√≥m t·∫Øt Ph√¢n t√≠ch")
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("T·ªïng c·∫∑p t∆∞∆°ng t·ª±", len(similar_cols))
                
                with col2:
                    high_conf = len([p for p in similar_cols if p['similarity'] > 0.8])
                    st.metric("ƒê·ªô tin c·∫≠y cao", high_conf)
                
                with col3:
                    avg_sim = np.mean([p['similarity'] for p in similar_cols])
                    st.metric("T∆∞∆°ng ƒë·ªìng TB", f"{avg_sim:.1%}")
                
            else:
                st.info("üîç Kh√¥ng t√¨m th·∫•y c·ªôt t∆∞∆°ng t·ª± v·ªõi ng∆∞·ª°ng hi·ªán t·∫°i. Th·ª≠ gi·∫£m ng∆∞·ª°ng t∆∞∆°ng ƒë·ªìng.")
        
        elif analysis_type == "T∆∞∆°ng Quan Th·ªëng K√™":
            with loading_placeholder:
                show_loading("üìä ƒêang t√≠nh to√°n t∆∞∆°ng quan ch√©o...")
            
            time.sleep(2)
            correlations = calculate_cross_correlations(df1, df2)
            loading_placeholder.empty()
            
            st.subheader("üìà T∆∞∆°ng Quan Ch√©o B·ªô D·ªØ Li·ªáu")
            
            if correlations:
                # Create interactive visualization
                fig = create_correlation_visualization(correlations)
                st.plotly_chart(fig, use_container_width=True)
                
                # Display top correlations table
                st.markdown("### üèÜ Top T∆∞∆°ng Quan")
                
                corr_data = []
                for corr in correlations[:20]:
                    corr_data.append({
                        'M·ªëi Quan H·ªá': f"{corr['col1']} √ó {corr['col2']}",
                        'Pearson': f"{corr['pearson_r']:.3f}",
                        'Spearman': f"{corr['spearman_r']:.3f}",
                        'P-value': f"{min(corr['pearson_p'], corr['spearman_p']):.3f}",
                        '√ù Nghƒ©a': corr['significance'],
                        'ƒê·ªô M·∫°nh': corr['strength']
                    })
                
                corr_df = pd.DataFrame(corr_data)
                st.dataframe(corr_df, use_container_width=True)
                
                # Export option
                csv_data = corr_df.to_csv(index=False)
                st.download_button(
                    "üì• T·∫£i xu·ªëng K·∫øt qu·∫£ T∆∞∆°ng quan",
                    csv_data,
                    file_name="cross_correlation_analysis.csv",
                    mime="text/csv"
                )
                
            else:
                st.info("üìä Kh√¥ng t√¨m th·∫•y t∆∞∆°ng quan ƒë√°ng k·ªÉ gi·ªØa c√°c c·ªôt s·ªë.")
        
        elif analysis_type == "M·ªëi Quan H·ªá Ng·ªØ Nghƒ©a":
            with loading_placeholder:
                show_loading("ü§ñ AI ƒëang ph√¢n t√≠ch m·ªëi quan h·ªá ng·ªØ nghƒ©a...")
            
            time.sleep(3)
            semantic_results = perform_semantic_analysis(df1, df2)
            loading_placeholder.empty()
            
            st.subheader("üß† M·ªëi Quan H·ªá Ng·ªØ Nghƒ©a ƒê∆∞·ª£c AI Ph√°t Hi·ªán")
            
            if semantic_results and 'relationships' in semantic_results:
                for rel in semantic_results['relationships']:
                    if rel['type'] != 'error':
                        confidence_color = "#28a745" if rel['confidence'] > 0.7 else "#ffc107" if rel['confidence'] > 0.4 else "#dc3545"
                        
                        st.markdown(f"""
                        <div class="analysis-result">
                            <h4 style="color: #2c3e50;">
                                üîó {rel['col1']} ‚ÜîÔ∏è {rel['col2']}
                            </h4>
                            <p><strong>Lo·∫°i quan h·ªá:</strong> {rel['type']}</p>
                            <p><strong>M√¥ t·∫£:</strong> {rel['description']}</p>
                            <div style="
                                background: {confidence_color};
                                color: white;
                                padding: 0.3rem 0.8rem;
                                border-radius: 15px;
                                display: inline-block;
                                font-size: 0.9rem;
                            ">
                                Tin c·∫≠y: {rel['confidence']:.1%}
                            </div>
                        </div>
                        """, unsafe_allow_html=True)
                    else:
                        st.markdown(f"""
                        <div class="error-card">
                            <p>{rel['description']}</p>
                        </div>
                        """, unsafe_allow_html=True)
            else:
                st.info("üîç Kh√¥ng ph√°t hi·ªán m·ªëi quan h·ªá ng·ªØ nghƒ©a r√µ r√†ng.")
        
        elif analysis_type == "Ph√¢n T√≠ch T·ªïng H·ª£p":
            with loading_placeholder:
                show_loading("üîÑ ƒêang th·ª±c hi·ªán ph√¢n t√≠ch t·ªïng h·ª£p...")
            
            # Run all analyses
            similar_cols = find_similar_columns(df1, df2)
            time.sleep(1)
            
            correlations = calculate_cross_correlations(df1, df2)
            time.sleep(1)
            
            semantic_results = perform_semantic_analysis(df1, df2)
            time.sleep(2)
            
            # Combine results for AI analysis
            all_results = {
                'similar_columns': similar_cols[:5],
                'correlations': correlations[:5],
                'semantic_relationships': semantic_results
            }
            
            ai_insights = generate_ai_insights(df1, df2, dataset1[1], dataset2[1], all_results)
            loading_placeholder.empty()
            
            # Display results in tabs
            tab1, tab2, tab3, tab4 = st.tabs(["üîç Insights AI", "üìã C·ªôt T∆∞∆°ng T·ª±", "üìà T∆∞∆°ng Quan", "üîó Ng·ªØ Nghƒ©a"])
            
            with tab1:
                st.markdown("### ü§ñ Ph√¢n T√≠ch T·ªïng H·ª£p AI")
                st.markdown(ai_insights)
                
                # Summary metrics
                st.markdown("### üìä T√≥m t·∫Øt K·∫øt qu·∫£")
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("C·ªôt t∆∞∆°ng t·ª±", len(similar_cols))
                
                with col2:
                    st.metric("T∆∞∆°ng quan m·∫°nh", len([c for c in correlations if max(abs(c['pearson_r']), abs(c['spearman_r'])) > 0.5]))
                
                with col3:
                    semantic_count = len(semantic_results.get('relationships', [])) if semantic_results else 0
                    st.metric("Quan h·ªá ng·ªØ nghƒ©a", semantic_count)
                
                with col4:
                    total_connections = len(similar_cols) + len(correlations) + semantic_count
                    st.metric("T·ªïng k·∫øt n·ªëi", total_connections)
            
            with tab2:
                if similar_cols:
                    for pair in similar_cols[:10]:
                        st.markdown(f"""
                        <div class="insight-card">
                            <strong>{pair['col1']}</strong> ‚ÜîÔ∏è <strong>{pair['col2']}</strong><br>
                            T∆∞∆°ng ƒë·ªìng: {pair['similarity']:.2%} | Lo·∫°i: {pair['type1']} vs {pair['type2']}
                        </div>
                        """, unsafe_allow_html=True)
                else:
                    st.info("Kh√¥ng t√¨m th·∫•y c·ªôt t∆∞∆°ng t·ª±")
            
            with tab3:
                if correlations:
                    corr_data = []
                    for corr in correlations[:15]:
                        corr_data.append({
                            'C·ªôt 1': corr['col1'],
                            'C·ªôt 2': corr['col2'],
                            'Pearson': f"{corr['pearson_r']:.3f}",
                            'Spearman': f"{corr['spearman_r']:.3f}",
                            '√ù Nghƒ©a': corr['significance']
                        })
                    
                    st.dataframe(pd.DataFrame(corr_data), use_container_width=True)
                else:
                    st.info("Kh√¥ng t√¨m th·∫•y t∆∞∆°ng quan ƒë√°ng k·ªÉ")
            
            with tab4:
                if semantic_results and 'relationships' in semantic_results:
                    for rel in semantic_results['relationships']:
                        if rel['type'] != 'error':
                            st.markdown(f"**{rel['col1']} ‚ÜîÔ∏è {rel['col2']}**")
                            st.write(f"Lo·∫°i: {rel['type']}")
                            st.write(f"M√¥ t·∫£: {rel['description']}")
                            st.write(f"Tin c·∫≠y: {rel['confidence']:.1%}")
                            st.markdown("---")
                else:
                    st.info("Kh√¥ng ph√°t hi·ªán m·ªëi quan h·ªá ng·ªØ nghƒ©a")
    
    except Exception as e:
        loading_placeholder.empty()
        st.error(f"‚ùå L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch: {str(e)}")
        
        # Show debug info
        with st.expander("üêõ Th√¥ng tin Debug"):
            st.write(f"**L·ªói:** {str(e)}")
            st.write(f"**Lo·∫°i ph√¢n t√≠ch:** {analysis_type}")
            st.write(f"**Dataset 1:** {dataset1[1]} - {df1.shape}")
            st.write(f"**Dataset 2:** {dataset2[1]} - {df2.shape}")

# Advanced query section
st.markdown("---")
st.subheader("üí¨ ƒê·∫∑t C√¢u H·ªèi Qua C√°c B·ªô D·ªØ Li·ªáu")

# Provide example questions
with st.expander("üí° C√¢u h·ªèi v√≠ d·ª•", expanded=False):
    example_questions = [
        "C√≥ m·ªëi quan h·ªá n√†o gi·ªØa doanh thu v√† s·ªë l∆∞·ª£ng kh√°ch h√†ng kh√¥ng?",
        "Xu h∆∞·ªõng theo th·ªùi gian gi·ªØa hai b·ªô d·ªØ li·ªáu c√≥ gi·ªëng nhau kh√¥ng?",
        "C√°c y·∫øu t·ªë n√†o ·∫£nh h∆∞·ªüng chung ƒë·∫øn c·∫£ hai b·ªô d·ªØ li·ªáu?",
        "C√≥ th·ªÉ d·ª± ƒëo√°n d·ªØ li·ªáu b·ªô 2 d·ª±a tr√™n b·ªô 1 kh√¥ng?",
        "Ph√¢n kh√∫c kh√°ch h√†ng n√†o xu·∫•t hi·ªán ·ªü c·∫£ hai ngu·ªìn d·ªØ li·ªáu?"
    ]
    
    for q in example_questions:
        if st.button(f"üìù {q}", key=f"example_{q[:20]}"):
            st.session_state.query_input = q

query_input = st.text_area(
    "ƒê·∫∑t c√¢u h·ªèi ph·ª©c t·∫°p tr·∫£i r·ªông c·∫£ hai b·ªô d·ªØ li·ªáu:",
    value=st.session_state.get('query_input', ''),
    placeholder="V√≠ d·ª•: T∆∞∆°ng quan gi·ªØa doanh thu v√† satisfaction score nh∆∞ th·∫ø n√†o?",
    height=100,
    help="ƒê·∫∑t c√¢u h·ªèi c·ª• th·ªÉ ƒë·ªÉ nh·∫≠n ƒë∆∞·ª£c ph√¢n t√≠ch chi ti·∫øt t·ª´ AI"
)

if st.button("üéØ Tr·∫£ L·ªùi C√¢u H·ªèi", type="secondary") and query_input:
    with st.spinner("ü§ñ AI ƒëang ph√¢n t√≠ch c√¢u h·ªèi c·ªßa b·∫°n..."):
        enhanced_prompt = f"""
        B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch d·ªØ li·ªáu. H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau d·ª±a tr√™n hai b·ªô d·ªØ li·ªáu:
        
        B·ªô D·ªØ Li·ªáu 1: {dataset1[1]}
        - K√≠ch th∆∞·ªõc: {df1.shape}
        - C√°c c·ªôt: {list(df1.columns)}
        - M·∫´u d·ªØ li·ªáu: {df1.head(2).to_dict()}
        
        B·ªô D·ªØ Li·ªáu 2: {dataset2[1]}
        - K√≠ch th∆∞·ªõc: {df2.shape}
        - C√°c c·ªôt: {list(df2.columns)}
        - M·∫´u d·ªØ li·ªáu: {df2.head(2).to_dict()}
        
        C√¢u H·ªèi: {query_input}
        
        H√£y ph√¢n t√≠ch v√† tr·∫£ l·ªùi m·ªôt c√°ch chi ti·∫øt, bao g·ªìm:
        1. X√°c ƒë·ªãnh c√°c c·ªôt v√† d·ªØ li·ªáu li√™n quan
        2. Ph∆∞∆°ng ph√°p ph√¢n t√≠ch ph√π h·ª£p
        3. K·∫øt qu·∫£ v√† insights c·ª• th·ªÉ
        4. Khuy·∫øn ngh·ªã h√†nh ƒë·ªông
        5. C√°c gi·ªõi h·∫°n c·ªßa ph√¢n t√≠ch
        
        S·ª≠ d·ª•ng markdown ƒë·ªÉ format c√¢u tr·∫£ l·ªùi m·ªôt c√°ch ƒë·∫πp m·∫Øt.
        """
        
        response = safe_llm_invoke(enhanced_prompt)
        
        st.markdown("### üéØ K·∫øt Qu·∫£ Ph√¢n T√≠ch")
        st.markdown(f"""
        <div class="analysis-result">
            {response}
        </div>
        """, unsafe_allow_html=True)

# Export and save options
st.markdown("---")
col1, col2, col3 = st.columns(3)

with col1:
    if st.button("üì• Xu·∫•t B√°o C√°o T√≥m T·∫Øt", use_container_width=True):
        st.info("üîÑ T√≠nh nƒÉng xu·∫•t b√°o c√°o ƒëang ƒë∆∞·ª£c ph√°t tri·ªÉn...")

with col2:
    if st.button("üíæ L∆∞u K·∫øt Qu·∫£ Ph√¢n T√≠ch", use_container_width=True):
        st.info("üîÑ T√≠nh nƒÉng l∆∞u k·∫øt qu·∫£ ƒëang ƒë∆∞·ª£c ph√°t tri·ªÉn...")

with col3:
    if st.button("üìä T·∫°o Dashboard T·ªïng H·ª£p", use_container_width=True):
        try:
            with st.spinner("üìä ƒêang t·∫°o dashboard..."):
                # Create a comprehensive dashboard
                dashboard_fig = make_subplots(
                    rows=2, cols=2,
                    subplot_titles=[
                        'So s√°nh K√≠ch th∆∞·ªõc D·ªØ li·ªáu',
                        'Ph√¢n ph·ªëi Ki·ªÉu D·ªØ li·ªáu',
                        'Ch·∫•t l∆∞·ª£ng D·ªØ li·ªáu',
                        'T·ªïng quan M·ªëi quan h·ªá'
                    ],
                    specs=[[{"type": "bar"}, {"type": "pie"}],
                           [{"type": "bar"}, {"type": "scatter"}]]
                )
                
                # Chart 1: Data size comparison
                dashboard_fig.add_trace(
                    go.Bar(
                        x=[dataset1[1][:20], dataset2[1][:20]],
                        y=[df1.shape[0], df2.shape[0]],
                        name="S·ªë h√†ng",
                        marker_color=['#667eea', '#764ba2']
                    ),
                    row=1, col=1
                )
                
                # Chart 2: Data type distribution
                type_dist1 = df1.dtypes.value_counts()
                dashboard_fig.add_trace(
                    go.Pie(
                        labels=[f"{dataset1[1][:10]}: {idx}" for idx in type_dist1.index],
                        values=type_dist1.values,
                        name="Ki·ªÉu d·ªØ li·ªáu"
                    ),
                    row=1, col=2
                )
                
                # Chart 3: Data quality comparison
                missing1 = (df1.isnull().sum().sum() / (df1.shape[0] * df1.shape[1])) * 100
                missing2 = (df2.isnull().sum().sum() / (df2.shape[0] * df2.shape[1])) * 100
                
                dashboard_fig.add_trace(
                    go.Bar(
                        x=[dataset1[1][:20], dataset2[1][:20]],
                        y=[100-missing1, 100-missing2],
                        name="Ch·∫•t l∆∞·ª£ng (%)",
                        marker_color=['#28a745', '#20c997']
                    ),
                    row=2, col=1
                )
                
                # Chart 4: Relationship summary (if analysis was run)
                if hasattr(st.session_state, 'analysis_results'):
                    # Use stored results
                    pass
                else:
                    # Simple overview
                    common_cols = len(set(df1.columns) & set(df2.columns))
                    total_cols = len(set(df1.columns) | set(df2.columns))
                    
                    dashboard_fig.add_trace(
                        go.Scatter(
                            x=[common_cols],
                            y=[total_cols],
                            mode='markers',
                            marker=dict(size=50, color='#ff6b6b'),
                            name="M·ªëi quan h·ªá",
                            text=[f"Chung: {common_cols}/{total_cols}"],
                            textposition="middle center"
                        ),
                        row=2, col=2
                    )
                
                dashboard_fig.update_layout(
                    height=800,
                    title_text=f"Dashboard Ph√¢n t√≠ch: {dataset1[1]} vs {dataset2[1]}",
                    showlegend=True
                )
                
                st.plotly_chart(dashboard_fig, use_container_width=True)
                st.success("‚úÖ Dashboard ƒë√£ ƒë∆∞·ª£c t·∫°o!")
                
        except Exception as e:
            st.error(f"‚ùå L·ªói t·∫°o dashboard: {str(e)}")

# Tips and best practices
st.markdown("---")
st.subheader("üí° M·∫πo Ph√¢n t√≠ch Ch√©o Hi·ªáu qu·∫£")

with st.expander("üìö H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng", expanded=False):
    st.markdown("""
    ### üéØ L·ª±a ch·ªçn Ph∆∞∆°ng ph√°p Ph√¢n t√≠ch
    
    **üîç T∆∞∆°ng ƒê·ªìng C·ªôt:**
    - S·ª≠ d·ª•ng khi: Mu·ªën t√¨m c√°c tr∆∞·ªùng d·ªØ li·ªáu t∆∞∆°ng t·ª± gi·ªØa hai b·ªô d·ªØ li·ªáu
    - Ph√π h·ª£p cho: Vi·ªác h·ª£p nh·∫•t d·ªØ li·ªáu, chu·∫©n h√≥a schema
    - V√≠ d·ª•: T√¨m "customer_id" v√† "khach_hang_id" c√≥ c√πng √Ω nghƒ©a
    
    **üìä T∆∞∆°ng Quan Th·ªëng K√™:**
    - S·ª≠ d·ª•ng khi: Mu·ªën t√¨m m·ªëi quan h·ªá s·ªë h·ªçc gi·ªØa c√°c bi·∫øn
    - Ph√π h·ª£p cho: Ph√¢n t√≠ch xu h∆∞·ªõng, d·ª± ƒëo√°n, m√¥ h√¨nh h√≥a
    - V√≠ d·ª•: M·ªëi quan h·ªá gi·ªØa doanh thu v√† chi ph√≠ marketing
    
    **üß† M·ªëi Quan H·ªá Ng·ªØ Nghƒ©a:**
    - S·ª≠ d·ª•ng khi: Mu·ªën hi·ªÉu √Ω nghƒ©a logic gi·ªØa c√°c tr∆∞·ªùng
    - Ph√π h·ª£p cho: Thi·∫øt k·∫ø data warehouse, integration
    - V√≠ d·ª•: M·ªëi quan h·ªá gi·ªØa "city" v√† "region"
    
    **üîÑ Ph√¢n T√≠ch T·ªïng H·ª£p:**
    - S·ª≠ d·ª•ng khi: C·∫ßn c√°i nh√¨n to√†n di·ªán v·ªÅ m·ªëi quan h·ªá
    - Ph√π h·ª£p cho: B√°o c√°o t·ªïng th·ªÉ, ra quy·∫øt ƒë·ªãnh strategice
    - V√≠ d·ª•: ƒê√°nh gi√° kh·∫£ nƒÉng t√≠ch h·ª£p to√†n b·ªô h·ªá th·ªëng
    
    ### üöÄ M·∫πo T·ªëi ∆∞u
    
    1. **Chu·∫©n b·ªã D·ªØ li·ªáu:**
       - ƒê·∫£m b·∫£o d·ªØ li·ªáu s·∫°ch v√† c√≥ c·∫•u tr√∫c
       - Th·ªëng nh·∫•t format ng√†y th√°ng, s·ªë li·ªáu
       - Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt
    
    2. **ƒê·∫∑t C√¢u h·ªèi ƒê√∫ng:**
       - C·ª• th·ªÉ v·ªÅ m·ª•c ti√™u ph√¢n t√≠ch
       - ƒê·ªÅ c·∫≠p ƒë·∫øn t√™n c·ªôt v√† b·ªëi c·∫£nh
       - Y√™u c·∫ßu insights c√≥ th·ªÉ h√†nh ƒë·ªông
    
    3. **Di·ªÖn gi·∫£i K·∫øt qu·∫£:**
       - Xem x√©t ƒë·ªô tin c·∫≠y v√† p-value
       - C√¢n nh·∫Øc k√≠ch th∆∞·ªõc m·∫´u
       - Ki·ªÉm tra t√≠nh h·ª£p l√Ω c·ªßa k·∫øt qu·∫£
    
    4. **H√†nh ƒë·ªông Ti·∫øp theo:**
       - L∆∞u c√°c ph√°t hi·ªán quan tr·ªçng
       - T·∫°o workflow cho vi·ªác c·∫≠p nh·∫≠t ƒë·ªãnh k·ª≥
       - Chia s·∫ª insights v·ªõi team
    """)

with st.expander("‚ö†Ô∏è L∆∞u √Ω Quan tr·ªçng", expanded=False):
    st.markdown("""
    ### üîî Nh·ªØng ƒëi·ªÅu C·∫ßn L∆∞u √Ω
    
    **üìä V·ªÅ T∆∞∆°ng quan:**
    - T∆∞∆°ng quan ‚â† Nh√¢n qu·∫£
    - Ki·ªÉm tra outliers c√≥ th·ªÉ ·∫£nh h∆∞·ªüng k·∫øt qu·∫£
    - P-value th·∫•p kh√¥ng c√≥ nghƒ©a l√† t∆∞∆°ng quan c√≥ √Ω nghƒ©a th·ª±c t·∫ø
    
    **üîç V·ªÅ Ph√¢n t√≠ch Ng·ªØ nghƒ©a:**
    - AI c√≥ th·ªÉ ƒë∆∞a ra g·ª£i √Ω sai
    - C·∫ßn ki·ªÉm tra l·∫°i v·ªõi hi·ªÉu bi·∫øt domain
    - C√°c m·ªëi quan h·ªá ph·ª©c t·∫°p c√≥ th·ªÉ b·ªã b·ªè qua
    
    **‚ö° V·ªÅ Hi·ªáu nƒÉng:**
    - B·ªô d·ªØ li·ªáu l·ªõn c√≥ th·ªÉ m·∫•t nhi·ªÅu th·ªùi gian
    - Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng c·ªôt ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô
    - Cache k·∫øt qu·∫£ cho vi·ªác ph√¢n t√≠ch l·∫∑p l·∫°i
    
    **üéØ V·ªÅ K·∫øt qu·∫£:**
    - Lu√¥n validate k·∫øt qu·∫£ v·ªõi business logic
    - Xem x√©t context v√† th·ªùi gian thu th·∫≠p d·ªØ li·ªáu
    - C·∫ßn c√≥ plan backup n·∫øu ph√¢n t√≠ch th·∫•t b·∫°i
    """)

# Footer
st.markdown("---")
col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("**üîó VizGenie-GPT Cross Analysis**")
    st.caption("Kh√°m ph√° m·ªëi quan h·ªá ·∫©n trong d·ªØ li·ªáu")

with col2:
    if hasattr(st.session_state, 'analysis_results'):
        st.markdown("**‚úÖ Tr·∫°ng th√°i**")
        st.caption("Ph√¢n t√≠ch ƒë√£ ho√†n th√†nh")
    else:
        st.markdown("**‚è≥ Tr·∫°ng th√°i**")
        st.caption("S·∫µn s√†ng ph√¢n t√≠ch")

with col3:
    st.markdown("**üí° M·∫πo**")
    st.caption("Th·ª≠ c√°c lo·∫°i ph√¢n t√≠ch kh√°c nhau ƒë·ªÉ c√≥ c√°i nh√¨n to√†n di·ªán!")